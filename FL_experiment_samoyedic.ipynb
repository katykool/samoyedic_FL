{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXoC7N7heDPH",
        "outputId": "f3461dc0-5e76-48a3-d996-bbc774af3a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers datasets accelerate wandb sacrebleu sentencepiece\n",
        "!pip install -q peft bitsandbytes pyyaml tqdm sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y9y5a9yccBb5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "from tqdm.auto import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Any, Optional, List\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8mqO0QieGdI",
        "outputId": "ee9753d4-7796-4661-b9fb-4471fb9c262b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfaoL7ofKrch"
      },
      "source": [
        "# –ë–∞–∑–æ–≤—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSfrcfpkeh5A"
      },
      "source": [
        "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "Nx23ALWoT5ac"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    \"\"\"–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞\"\"\"\n",
        "    def __init__(self, data, split=\"train\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏:\n",
        "                  - 'original': –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
        "                  - 'russian_translation': —Ä—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥\n",
        "                  - 'lang': —è–∑—ã–∫ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞\n",
        "            split: \"train\", \"val\", \"test\"\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.split = split\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return {\n",
        "            'input': item['original'],\n",
        "            'output': item['russian_translation'],\n",
        "            'language': item['lang']\n",
        "        }\n",
        "\n",
        "def load_your_dataset():\n",
        "    try:\n",
        "        print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å Hugging Face...\")\n",
        "        dataset = load_dataset(\"katykool/samoyed.ic\")\n",
        "\n",
        "        if 'train' in dataset:\n",
        "            data = dataset['train']\n",
        "        else:\n",
        "            data = dataset\n",
        "\n",
        "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\")\n",
        "\n",
        "        processed_data = []\n",
        "        skipped = 0\n",
        "\n",
        "        for i, item in enumerate(data):\n",
        "            try:\n",
        "                original = item.get('original', '')\n",
        "                russian = item.get('russian_translation', '')\n",
        "                lang = item.get('lang', '')\n",
        "\n",
        "                if (original and russian and lang and\n",
        "                    isinstance(original, str) and\n",
        "                    isinstance(russian, str) and\n",
        "                    isinstance(lang, str) and\n",
        "                    len(original.strip()) > 0 and\n",
        "                    len(russian.strip()) > 0):\n",
        "\n",
        "                    processed_data.append({\n",
        "                        'original': original.strip(),\n",
        "                        'russian_translation': russian.strip(),\n",
        "                        'lang': lang.strip().lower()\n",
        "                    })\n",
        "                else:\n",
        "                    skipped += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                skipped += 1\n",
        "\n",
        "\n",
        "        print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(processed_data)} –ø—Ä–∏–º–µ—Ä–æ–≤, –ø—Ä–æ–ø—É—â–µ–Ω–æ {skipped}\")\n",
        "\n",
        "        if len(processed_data) == 0:\n",
        "            print(\"–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\")\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}\")\n",
        "\n",
        "\n",
        "def create_language_clients(config, tokenizer):\n",
        "    \"\"\"\n",
        "    –°–æ–∑–¥–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ —è–∑—ã–∫–∞–º\n",
        "    \"\"\"\n",
        "    all_data = load_your_dataset()\n",
        "\n",
        "    if not all_data:\n",
        "        print(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö!\")\n",
        "\n",
        "    # –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —è–∑—ã–∫–∞–º\n",
        "    data_by_language = defaultdict(list)\n",
        "    for item in all_data:\n",
        "        lang = item.get('lang', 'unknown')\n",
        "        data_by_language[lang].append(item)\n",
        "\n",
        "    print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ —è–∑—ã–∫–∞–º:\")\n",
        "    for lang, items in data_by_language.items():\n",
        "        print(f\"  - {lang.upper()}: {len(items)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "\n",
        "    # —Å–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞\n",
        "    train_loaders = {}\n",
        "    val_loaders = {}\n",
        "    test_loaders = {}\n",
        "\n",
        "    # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–æ–≤, –µ—Å–ª–∏ –∏—Ö –±–æ–ª—å—à–µ —á–µ–º config.npeers\n",
        "    languages = list(data_by_language.keys())\n",
        "    if len(languages) > config.npeers:\n",
        "        languages = languages[:config.npeers]\n",
        "\n",
        "    for i, lang in enumerate(languages):\n",
        "        if i >= config.npeers:\n",
        "            break\n",
        "\n",
        "        lang_data = data_by_language[lang]\n",
        "        random.shuffle(lang_data)  # –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "\n",
        "        # —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val/test (70/15/15)\n",
        "        n_total = len(lang_data)\n",
        "        n_train = int(n_total * 0.7)\n",
        "        n_val = int(n_total * 0.15)\n",
        "\n",
        "        train_data = lang_data[:n_train]\n",
        "        val_data = lang_data[n_train:n_train + n_val]\n",
        "        test_data = lang_data[n_train + n_val:]\n",
        "\n",
        "        # –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö\n",
        "        # if len(train_data) > 0:\n",
        "        #     print(f\"   –ü—Ä–∏–º–µ—Ä: '{train_data[0]['original'][:50]}...' ‚Üí '{train_data[0]['russian_translation'][:50]}...'\")\n",
        "\n",
        "        # —Å–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
        "        train_dataset = TranslationDataset(train_data, split=\"train\")\n",
        "        val_dataset = TranslationDataset(val_data, split=\"val\")\n",
        "        test_dataset = TranslationDataset(test_data, split=\"test\")\n",
        "\n",
        "        # —Å–æ–∑–¥–∞–µ–º DataLoader—ã\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=config.batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=lambda x: x,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=config.batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=lambda x: x,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=config.batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=lambda x: x,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        train_loaders[i] = train_loader\n",
        "        val_loaders[i] = val_loader\n",
        "        test_loaders[i] = test_loader\n",
        "\n",
        "    # –æ–±—â–∏–π –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –∏–∑ –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤\n",
        "    all_val_data = []\n",
        "    for lang_data in data_by_language.values():\n",
        "        all_val_data.extend(lang_data[:min(5, len(lang_data))])  # –ø–æ 5 –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞\n",
        "\n",
        "    random.shuffle(all_val_data)\n",
        "    combined_val_dataset = TranslationDataset(all_val_data, split=\"val\")\n",
        "    combined_val_loader = DataLoader(\n",
        "        combined_val_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda x: x,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    return train_loaders, combined_val_loader, test_loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfLFckpAfEAJ"
      },
      "source": [
        "## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "Om1OvCSee8VI"
      },
      "outputs": [],
      "source": [
        "project_root = \"/content/meritopt\"\n",
        "os.makedirs(project_root, exist_ok=True)\n",
        "\n",
        "sys.path.append(project_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "jvIsmIWBe6dW"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    # –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "    seed: int = 42\n",
        "    epochs: int = 3\n",
        "    max_steps: int = 500\n",
        "    batch_size: int = 2\n",
        "    lr: float = 3e-5\n",
        "\n",
        "    # –º–æ–¥–µ–ª—å (–Ω–µ —Ç–∞, —á—Ç–æ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–µ -- –Ω–æ –∑–∞—Ç–æ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –ö–æ–ª–∞–±)\n",
        "    model_type: str = \"MarianMT\"\n",
        "    model_checkpoint: str = \"Helsinki-NLP/opus-mt-en-ru\"\n",
        "\n",
        "    # –¥–∞–Ω–Ω—ã–µ\n",
        "    max_seq_len: int = 32\n",
        "\n",
        "    # federate learning\n",
        "    fl: bool = True\n",
        "    npeers: int = 3  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–æ–≤ = —è–∑—ã–∫–æ–≤\n",
        "    mdlr_: float = 0.1\n",
        "    mdniters_: int = 2\n",
        "    fl_beta_1: float = 0.9\n",
        "\n",
        "    # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "    save_every: int = 100\n",
        "    save_strategy: str = \"steps\"\n",
        "    saving_path: str = \"/content/drive/MyDrive/meritopt_checkpoints\"\n",
        "\n",
        "    device: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def __post_init__(self):\n",
        "        import os\n",
        "        os.makedirs(self.saving_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb8AjtrUfetq"
      },
      "source": [
        "## –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä **Merit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "kHCihNruhAFv"
      },
      "outputs": [],
      "source": [
        "class MeritFedAdamFixed(torch.optim.Optimizer):\n",
        "    def __init__(self, params, config):\n",
        "        defaults = dict(lr=config.lr, betas=(0.9, 0.98), eps=1e-9)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "        self.config = config\n",
        "        self.device = config.device\n",
        "        self.npeers = config.npeers\n",
        "        self.weights = torch.ones(config.npeers, device=self.device) / config.npeers\n",
        "        self.grads_received = 0\n",
        "        self.step_count = 0\n",
        "\n",
        "        # –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
        "        self.client_gradients = [None] * config.npeers\n",
        "\n",
        "        # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                state['grads'] = [torch.zeros_like(p, device=self.device) for _ in range(config.npeers)]\n",
        "                state['m'] = torch.zeros_like(p, device=self.device)\n",
        "                state['v'] = torch.zeros_like(p, device=self.device)\n",
        "\n",
        "    def save_client_gradients(self, peer_id, gradients):\n",
        "        \"\"\"–°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏\"\"\"\n",
        "        self.client_gradients[peer_id] = gradients\n",
        "\n",
        "    def register_all_gradients(self):\n",
        "        \"\"\"–†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≤—Å–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\"\"\"\n",
        "        print(f\"\\n[register_all_gradients] Step {self.step_count + 1}\")\n",
        "\n",
        "        for peer_id in range(self.npeers):\n",
        "            gradients = self.client_gradients[peer_id]\n",
        "\n",
        "            if gradients is None:\n",
        "                # –Ω—É–ª–µ–≤—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
        "                for group in self.param_groups:\n",
        "                    for p in group['params']:\n",
        "                        self.state[p]['grads'][peer_id].zero_()\n",
        "                print(f\"  Client {peer_id}: zero gradients\")\n",
        "                self.grads_received += 1\n",
        "                continue\n",
        "\n",
        "            # –∫–æ–ø–∏—Ä—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
        "            grad_idx = 0\n",
        "            total_norm = 0\n",
        "            param_count = 0\n",
        "\n",
        "            for group in self.param_groups:\n",
        "                for p in group['params']:\n",
        "                    if grad_idx < len(gradients) and gradients[grad_idx] is not None:\n",
        "                        grad = gradients[grad_idx]\n",
        "                        self.state[p]['grads'][peer_id].copy_(grad)\n",
        "\n",
        "                        # –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ—Ä–º—É\n",
        "                        grad_norm = torch.norm(grad, p=2).item()\n",
        "                        total_norm += grad_norm\n",
        "                        param_count += 1\n",
        "\n",
        "                    grad_idx += 1\n",
        "\n",
        "            if param_count > 0:\n",
        "                print(f\"  Client {peer_id}: total grad norm = {total_norm:.6f} ({param_count} params)\")\n",
        "            else:\n",
        "                print(f\"  Client {peer_id}: NO gradients saved\")\n",
        "\n",
        "            self.grads_received += 1\n",
        "\n",
        "        # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ\n",
        "        self.client_gradients = [None] * self.npeers\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"–®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\"\"\"\n",
        "        if self.grads_received < self.npeers:\n",
        "            print(f\"WARNING: Not enough gradients ({self.grads_received}/{self.npeers})\")\n",
        "            return\n",
        "\n",
        "        self.step_count += 1\n",
        "        print(f\"\\n=== Optimizer Step {self.step_count} ===\")\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "\n",
        "                # –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å –≤–µ—Å–∞–º–∏\n",
        "                agg_grad = torch.zeros_like(p, device=self.device)\n",
        "                for i in range(self.npeers):\n",
        "                    agg_grad += state['grads'][i] * self.weights[i]\n",
        "\n",
        "                # adam update\n",
        "                beta1, beta2 = group['betas']\n",
        "                m, v = state['m'], state['v']\n",
        "\n",
        "                m = beta1 * m + (1 - beta1) * agg_grad\n",
        "                v = beta2 * v + (1 - beta2) * agg_grad.pow(2)\n",
        "\n",
        "                m_hat = m / (1 - beta1 ** self.step_count)\n",
        "                v_hat = v / (1 - beta2 ** self.step_count)\n",
        "\n",
        "                update = -group['lr'] * m_hat / (v_hat.sqrt() + group['eps'])\n",
        "                p.data.add_(update)\n",
        "\n",
        "                # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–º–µ–Ω—Ç—ã\n",
        "                state['m'].copy_(m)\n",
        "                state['v'].copy_(v)\n",
        "\n",
        "        # —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫\n",
        "        self.grads_received = 0\n",
        "\n",
        "        # –æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "        self._update_client_weights()\n",
        "\n",
        "    def _update_client_weights(self):\n",
        "        \"\"\"–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ—Ä–º—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\"\"\"\n",
        "        print(f\"\\n=== Updating Client Weights (Step {self.step_count}) ===\")\n",
        "\n",
        "        # —Å–æ–±–∏—Ä–∞–µ–º –Ω–æ—Ä–º—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
        "        grad_norms = torch.zeros(self.npeers, device=self.device)\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p_idx, p in enumerate(group['params'][:3]):  # –ø–µ—Ä–≤—ã–µ 3 –ø–∞—Ä–∞–º–µ—Ç—Ä–∞\n",
        "                state = self.state[p]\n",
        "                for i in range(self.npeers):\n",
        "                    grad_norm = torch.norm(state['grads'][i], p=2).item()\n",
        "                    grad_norms[i] += grad_norm\n",
        "\n",
        "        print(f\"Gradient norms: {grad_norms.tolist()}\")\n",
        "\n",
        "        # –µ—Å–ª–∏ –≤—Å–µ –Ω—É–ª–∏ - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–µ –≤–µ—Å–∞\n",
        "        if torch.all(grad_norms == 0):\n",
        "            print(\"WARNING: All gradients are zero! Keeping previous weights.\")\n",
        "            print(f\"Current weights: {self.weights.tolist()}\")\n",
        "            return\n",
        "\n",
        "        # 3. –æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ (–º–µ–Ω—å—à–∞—è –Ω–æ—Ä–º–∞ = –±–æ–ª—å—à–∏–π –≤–µ—Å)\n",
        "        epsilon = 1e-8\n",
        "        inverse_norms = 1.0 / (grad_norms + epsilon)\n",
        "        target_weights = inverse_norms / inverse_norms.sum()\n",
        "\n",
        "        # 4. –ø–ª–∞–≤–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ\n",
        "        alpha = 0.3\n",
        "        self.weights = alpha * target_weights + (1 - alpha) * self.weights\n",
        "        self.weights = self.weights / self.weights.sum()\n",
        "\n",
        "        print(f\"Target weights: {target_weights.tolist()}\")\n",
        "        print(f\"Updated weights: {self.weights.tolist()}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"–æ—á–∏—â–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\"\"\"\n",
        "        super().zero_grad()\n",
        "        # –æ—á–∏—â–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                for i in range(self.npeers):\n",
        "                    state['grads'][i].zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKVN2SizhB8a"
      },
      "source": [
        "## –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —à—Ç—É–∫–∏"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q_jUF6chE_m"
      },
      "source": [
        "### –õ–æ–≥–≥–µ—Ä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "sSJ3VmIVhRx4"
      },
      "outputs": [],
      "source": [
        "class SimpleLogger:\n",
        "    def __init__(self):\n",
        "        self.step = 0\n",
        "        self.mode = \"train\"\n",
        "        self.metrics_history = []\n",
        "\n",
        "    def set_step(self, step, mode=\"train\"):\n",
        "        self.step = step\n",
        "        self.mode = mode\n",
        "\n",
        "    def get_step(self):\n",
        "        return self.step\n",
        "\n",
        "    def add_scalar(self, name, value):\n",
        "        # print(f\"Step {self.step} ({self.mode}): {name} = {value:.4f}\")\n",
        "        self.metrics_history.append({\n",
        "            'step': self.step,\n",
        "            'mode': self.mode,\n",
        "            'name': name,\n",
        "            'value': value\n",
        "        })\n",
        "\n",
        "    def add_dict(self, metrics_dict):\n",
        "        for k, v in metrics_dict.items():\n",
        "            self.add_scalar(k, v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5GBY4lihWhu"
      },
      "source": [
        "### –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "NAElNs1ahZUH"
      },
      "outputs": [],
      "source": [
        "def move_to_device(data, device):\n",
        "    \"\"\"–ü–µ—Ä–µ–º–µ—â–∞–µ—Ç –≤—Å–µ —Ç–µ–Ω–∑–æ—Ä—ã –≤ —Å–ª–æ–≤–∞—Ä–µ –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\"\"\"\n",
        "    if isinstance(data, dict):\n",
        "        return {k: move_to_device(v, device) for k, v in data.items()}\n",
        "    elif isinstance(data, torch.Tensor):\n",
        "        return data.to(device)\n",
        "    elif isinstance(data, list):\n",
        "        return [move_to_device(item, device) for item in data]\n",
        "    else:\n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MXMAdvQhoQ4"
      },
      "source": [
        "## Federate learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh8Q9Xoxh7ZR"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "tqQfqNn-iLN0"
      },
      "outputs": [],
      "source": [
        "def train_epoch_fl_simple_fixed(model, tokenizer, optimizer, train_loaders, val_loader, logger, config, epoch, val_every=50):\n",
        "    model.train()\n",
        "\n",
        "    loader_iters = {i: iter(loader) for i, loader in train_loaders.items()}\n",
        "    n_batches = min(len(loader) for loader in train_loaders.values())\n",
        "\n",
        "    pbar = tqdm(range(n_batches), desc=f\"FL Epoch {epoch+1}\")\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx in pbar:\n",
        "        current_step = logger.get_step() + 1\n",
        "        logger.set_step(current_step, \"train\")\n",
        "\n",
        "        batch_loss = 0\n",
        "\n",
        "        # —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
        "        optimizer.client_gradients = [None] * config.npeers\n",
        "\n",
        "        # –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞\n",
        "        for client_id in range(config.npeers):\n",
        "            print(f\"\\n[Processing] Client {client_id}, batch {batch_idx}\")\n",
        "\n",
        "            try:\n",
        "                batch = next(loader_iters[client_id])\n",
        "            except StopIteration:\n",
        "                loader_iters[client_id] = iter(train_loaders[client_id])\n",
        "                batch = next(loader_iters[client_id])\n",
        "\n",
        "            valid_items = []\n",
        "            for item in batch:\n",
        "                input_text = item.get('input', '')\n",
        "                output_text = item.get('output', '')\n",
        "                if input_text and output_text:\n",
        "                    valid_items.append(item)\n",
        "\n",
        "            if not valid_items:\n",
        "                print(f\"  No valid items for client {client_id}\")\n",
        "                optimizer.save_client_gradients(client_id, None)\n",
        "                continue\n",
        "\n",
        "            inputs = [item['input'] for item in valid_items]\n",
        "            outputs = [item['output'] for item in valid_items]\n",
        "\n",
        "            try:\n",
        "                model_inputs = tokenizer(\n",
        "                    inputs,\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=config.max_seq_len,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).to(config.device)\n",
        "\n",
        "                labels = tokenizer(\n",
        "                    outputs,\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=config.max_seq_len,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).to(config.device)\n",
        "\n",
        "                # forward pass\n",
        "                outputs = model(\n",
        "                    input_ids=model_inputs['input_ids'],\n",
        "                    attention_mask=model_inputs['attention_mask'],\n",
        "                    labels=labels['input_ids']\n",
        "                )\n",
        "\n",
        "                loss = outputs.loss\n",
        "                batch_loss += loss.item()\n",
        "                print(f\"  Loss for client {client_id}: {loss.item():.4f}\")\n",
        "\n",
        "                # –æ–±–Ω—É–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
        "                model.zero_grad()\n",
        "\n",
        "                # backward pass\n",
        "                loss.backward()\n",
        "\n",
        "                # –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ç–µ–∫—É—â–µ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞\n",
        "                client_gradients = []\n",
        "                for group in optimizer.param_groups:\n",
        "                    for p in group['params']:\n",
        "                        if p.grad is not None:\n",
        "                            # –ö–ª–æ–Ω–∏—Ä—É–µ–º –∏ –¥–µ—Ç–∞—á–∏–º\n",
        "                            client_gradients.append(p.grad.clone().detach())\n",
        "                        else:\n",
        "                            client_gradients.append(None)\n",
        "\n",
        "                print(f\"  Collected {len([g for g in client_gradients if g is not None])} gradients\")\n",
        "\n",
        "                # –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
        "                optimizer.save_client_gradients(client_id, client_gradients)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing client {client_id}: {e}\")\n",
        "                optimizer.save_client_gradients(client_id, None)\n",
        "                continue\n",
        "\n",
        "        # –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
        "        optimizer.register_all_gradients()\n",
        "\n",
        "        # —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n",
        "        if batch_loss > 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            avg_loss = batch_loss / config.npeers\n",
        "            total_loss += avg_loss\n",
        "            logger.add_scalar(\"loss\", avg_loss)\n",
        "\n",
        "            # –ª–æ–≥–∏—Ä—É–µ–º –≤–µ—Å–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "            for i, w in enumerate(optimizer.weights):\n",
        "                logger.add_scalar(f\"weight_client_{i}\", w.item())\n",
        "\n",
        "            pbar.set_postfix({\"loss\": avg_loss})\n",
        "\n",
        "            # –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
        "            if current_step % val_every == 0:\n",
        "                print(f\"\\n[Validation] Step {current_step}\")\n",
        "                model.eval()\n",
        "\n",
        "                val_loss_total = 0\n",
        "                val_batches = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    val_iter = iter(val_loader)\n",
        "                    for _ in range(min(5, len(val_loader))):\n",
        "                        try:\n",
        "                            val_batch = next(val_iter)\n",
        "                        except StopIteration:\n",
        "                            break\n",
        "\n",
        "                        val_inputs = [item.get('input', '') for item in val_batch if 'input' in item]\n",
        "                        val_outputs = [item.get('output', '') for item in val_batch if 'output' in item]\n",
        "\n",
        "                        if val_inputs and val_outputs:\n",
        "                            val_model_inputs = tokenizer(\n",
        "                                val_inputs,\n",
        "                                padding=True,\n",
        "                                truncation=True,\n",
        "                                max_length=config.max_seq_len,\n",
        "                                return_tensors=\"pt\"\n",
        "                            ).to(config.device)\n",
        "\n",
        "                            val_labels = tokenizer(\n",
        "                                val_outputs,\n",
        "                                padding=True,\n",
        "                                truncation=True,\n",
        "                                max_length=config.max_seq_len,\n",
        "                                return_tensors=\"pt\"\n",
        "                            ).to(config.device)\n",
        "\n",
        "                            val_outputs = model(\n",
        "                                input_ids=val_model_inputs['input_ids'],\n",
        "                                attention_mask=val_model_inputs['attention_mask'],\n",
        "                                labels=val_labels['input_ids']\n",
        "                            )\n",
        "\n",
        "                            val_loss_total += val_outputs.loss.item()\n",
        "                            val_batches += 1\n",
        "\n",
        "                if val_batches > 0:\n",
        "                    avg_val_loss = val_loss_total / val_batches\n",
        "                    logger.set_step(current_step, \"val\")\n",
        "                    logger.add_scalar(\"val_loss\", avg_val_loss)\n",
        "                    logger.set_step(current_step, \"train\")\n",
        "                    print(f\"  Val Loss = {avg_val_loss:.4f} ({val_batches} batches)\")\n",
        "\n",
        "                model.train()\n",
        "\n",
        "            if current_step % config.save_every == 0:\n",
        "                save_model_simple(model, tokenizer, config, iteration=current_step)\n",
        "\n",
        "            if current_step >= config.max_steps:\n",
        "                print(f\"\\nReached max steps ({config.max_steps}), stopping...\")\n",
        "                break\n",
        "\n",
        "    return total_loss / n_batches if n_batches > 0 else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb0ATVJSiL_e"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "wu4smBqQiSMn"
      },
      "outputs": [],
      "source": [
        "def validate_simple(model, tokenizer, val_loader, logger, config, max_batches=None):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    val_iter = iter(val_loader)\n",
        "\n",
        "    if max_batches is None:\n",
        "        max_batches = min(20, len(val_loader))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx in range(max_batches):\n",
        "            try:\n",
        "                batch = next(val_iter)\n",
        "            except StopIteration:\n",
        "                val_iter = iter(val_loader)\n",
        "                batch = next(val_iter)\n",
        "\n",
        "            valid_items = []\n",
        "            for item in batch:\n",
        "                input_text = item.get('input', '')\n",
        "                output_text = item.get('output', '')\n",
        "                if input_text and output_text:\n",
        "                    valid_items.append(item)\n",
        "\n",
        "            if not valid_items:\n",
        "                continue\n",
        "\n",
        "            inputs = [item['input'] for item in valid_items]\n",
        "            outputs = [item['output'] for item in valid_items]\n",
        "\n",
        "            try:\n",
        "                model_inputs = tokenizer(\n",
        "                    inputs,\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=config.max_seq_len,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).to(config.device)\n",
        "\n",
        "                labels = tokenizer(\n",
        "                    outputs,\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=config.max_seq_len,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).to(config.device)\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=model_inputs['input_ids'],\n",
        "                    attention_mask=model_inputs['attention_mask'],\n",
        "                    labels=labels['input_ids']\n",
        "                )\n",
        "\n",
        "                loss = outputs.loss\n",
        "                total_loss += loss.item()\n",
        "                total_batches += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Validation error in batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –º–æ–¥–µ–ª—å –≤ –∏—Å—Ö–æ–¥–Ω—ã–π —Ä–µ–∂–∏–º\n",
        "    avg_loss = total_loss / total_batches if total_batches > 0 else 0\n",
        "    return avg_loss, total_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ti17kkziSvy"
      },
      "source": [
        "### Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "ZscW82HMiyPP"
      },
      "outputs": [],
      "source": [
        "def save_model_simple(model, tokenizer, config, epoch=None, iteration=None):\n",
        "\n",
        "    if epoch is not None:\n",
        "        save_path = f\"{config.saving_path}/epoch_{epoch}\"\n",
        "    elif iteration is not None:\n",
        "        save_path = f\"{config.saving_path}/step_{iteration}\"\n",
        "    else:\n",
        "        save_path = f\"{config.saving_path}/final\"\n",
        "\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å\n",
        "    model.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "\n",
        "    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –µ—Å–ª–∏ –µ—Å—Ç—å\n",
        "    if hasattr(config, 'optimizer') and hasattr(config.optimizer, 'weights'):\n",
        "        torch.save(\n",
        "            config.optimizer.weights.cpu(),\n",
        "            f\"{save_path}/fl_weights.pt\"\n",
        "        )\n",
        "\n",
        "    print(f\"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7gHSokdjen5"
      },
      "source": [
        "### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "6AesWolk_XC3"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "XUmBxG6PjiuI"
      },
      "outputs": [],
      "source": [
        "def test_model(model, tokenizer, config, test_loaders=None):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if test_loaders is None:\n",
        "        print('TestLoaders –ø—É—Å—Ç—ã–µ!')\n",
        "\n",
        "    else:\n",
        "        # —Ç–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞\n",
        "        total_samples = 0\n",
        "        total_correct = 0\n",
        "\n",
        "        for client_id, test_loader in test_loaders.items():\n",
        "            print(f\"\\nüë• –ö–ª–∏–µ–Ω—Ç {client_id}:\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            client_samples = 0\n",
        "            client_correct = 0\n",
        "\n",
        "            # –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "            max_test_samples = min(2, len(test_loader.dataset))\n",
        "            test_dataset = test_loader.dataset\n",
        "\n",
        "            for i in range(max_test_samples):\n",
        "                try:\n",
        "                    sample = test_dataset[i]\n",
        "                    original_text = sample['input']\n",
        "                    reference_translation = sample['output']\n",
        "                    lang = sample['language']\n",
        "\n",
        "                    # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π\n",
        "                    if not original_text or not reference_translation:\n",
        "                        continue\n",
        "\n",
        "                    inputs = tokenizer(\n",
        "                        original_text,\n",
        "                        return_tensors=\"pt\",\n",
        "                        padding=True,\n",
        "                        truncation=True,\n",
        "                        max_length=config.max_seq_len\n",
        "                    ).to(config.device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model.generate(**inputs, max_length=config.max_seq_len)\n",
        "\n",
        "                    predicted_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "                    print(f\"–Ø–∑—ã–∫: {lang}\")\n",
        "                    print(f\"–û—Ä–∏–≥–∏–Ω–∞–ª: {original_text[:80]}...\")\n",
        "                    print(f\"–≠—Ç–∞–ª–æ–Ω: {reference_translation[:80]}...\")\n",
        "                    print(f\"–ú–æ–¥–µ–ª—å: {predicted_translation[:80]}...\")\n",
        "                    print(\"-\" * 40)\n",
        "\n",
        "                    client_samples += 1\n",
        "                    total_samples += 1\n",
        "\n",
        "                    # –ø—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ (—Ö–æ—Ä–æ—à–æ –±—ã –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ BLEU)\n",
        "                    ref_words = set(reference_translation.lower().split()[:10])\n",
        "                    pred_words = set(predicted_translation.lower().split()[:10])\n",
        "                    overlap = len(ref_words.intersection(pred_words)) / max(len(ref_words), 1)\n",
        "\n",
        "                    if overlap > 0.3:  # –ø–æ—Ä–æ–≥ –¥–ª—è \"–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ\" –ø–µ—Ä–µ–≤–æ–¥–∞\n",
        "                        client_correct += 1\n",
        "                        total_correct += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø—Ä–∏–º–µ—Ä–∞ {i}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if client_samples > 0:\n",
        "                accuracy = client_correct / client_samples\n",
        "                print(f\"–ö–ª–∏–µ–Ω—Ç {client_id}: —Ç–æ—á–Ω–æ—Å—Ç—å = {accuracy:.2%} ({client_correct}/{client_samples})\")\n",
        "\n",
        "        if total_samples > 0:\n",
        "            overall_accuracy = total_correct / total_samples\n",
        "            print(f\"\\n–û–ë–©–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {overall_accuracy:.2%} ({total_correct}/{total_samples})\")\n",
        "        else:\n",
        "            print(\"–ù–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏\")\n",
        "\n",
        "\n",
        "def visualize_results(logger, smooth_window=10):\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import numpy as np\n",
        "\n",
        "        def smooth_curve_centered(data, window_size=5):\n",
        "            \"\"\"–¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ\"\"\"\n",
        "            if len(data) < window_size or window_size < 2:\n",
        "                return data.copy()\n",
        "\n",
        "            smoothed = np.zeros_like(data, dtype=float)\n",
        "            half = window_size // 2\n",
        "\n",
        "            for i in range(len(data)):\n",
        "                start = max(0, i - half)\n",
        "                end = min(len(data), i + half + 1)\n",
        "                smoothed[i] = np.mean(data[start:end])\n",
        "\n",
        "            return smoothed\n",
        "\n",
        "        # —Å–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        client_weights = {i: [] for i in range(3)}\n",
        "\n",
        "        # —à–∞–≥–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏\n",
        "        train_steps = []\n",
        "        val_steps = []\n",
        "        weight_steps = {i: [] for i in range(3)}\n",
        "\n",
        "        for metric in logger.metrics_history:\n",
        "            if metric['name'] == 'loss' and metric['mode'] == 'train':\n",
        "                train_losses.append(metric['value'])\n",
        "                train_steps.append(metric['step'])\n",
        "            elif metric['name'] == 'val_loss':\n",
        "                val_losses.append(metric['value'])\n",
        "                val_steps.append(metric['step'])\n",
        "            elif 'weight_client' in metric['name']:\n",
        "                try:\n",
        "                    client_id = int(metric['name'].split('_')[-1])\n",
        "                    client_weights[client_id].append(metric['value'])\n",
        "                    weight_steps[client_id].append(metric['step'])\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        # –≥—Ä–∞—Ñ–∏–∫ loss\n",
        "        plt.figure(figsize=(16, 8))\n",
        "\n",
        "        if train_losses:\n",
        "            plt.subplot(2, 2, 1)\n",
        "\n",
        "            # plt.scatter(train_steps, train_losses, alpha=0.3, s=15, color='blue',\n",
        "            #            label=f'Raw Train Loss ({len(train_losses)} points)')\n",
        "\n",
        "            # —Å–≥–ª–∞–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ train\n",
        "            if len(train_losses) >= smooth_window:\n",
        "                smoothed_train = smooth_curve_centered(train_losses, smooth_window)\n",
        "                plt.plot(train_steps, smoothed_train, 'b-', linewidth=3,\n",
        "                        label=f'Train Loss (smoothed, w={smooth_window})')\n",
        "            else:\n",
        "                plt.plot(train_steps, train_losses, 'b-', linewidth=2, label='Train Loss')\n",
        "\n",
        "            if val_losses and len(val_losses) > 0:\n",
        "                # plt.scatter(val_steps, val_losses, alpha=0.7, s=80, color='red',\n",
        "                #            marker='s', label=f'Raw Val Loss ({len(val_losses)} points)')\n",
        "\n",
        "                if len(val_losses) >= max(3, smooth_window//2):\n",
        "                    smoothed_val = smooth_curve_centered(val_losses, min(smooth_window, len(val_losses)))\n",
        "                    plt.plot(val_steps, smoothed_val, 'r--', linewidth=3,\n",
        "                            label=f'Val Loss (smoothed)', markersize=8)\n",
        "                else:\n",
        "                    # –µ—Å–ª–∏ –º–∞–ª–æ —Ç–æ—á–µ–∫, –ø—Ä–æ—Å—Ç–æ —Å–æ–µ–¥–∏–Ω—è–µ–º –ª–∏–Ω–∏–µ–π\n",
        "                    plt.plot(val_steps, val_losses, 'r--', linewidth=2,\n",
        "                            label='Val Loss', markersize=8)\n",
        "\n",
        "            plt.xlabel('Step')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Training Progress (Train vs Validation)')\n",
        "            plt.legend(loc='best')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            if train_losses:\n",
        "                y_max = max(max(train_losses), max(val_losses) if val_losses else 0) * 1.1\n",
        "                y_min = min(min(train_losses), min(val_losses) if val_losses else 0) * 0.9\n",
        "                plt.ylim(y_min, y_max)\n",
        "\n",
        "        # –≥—Ä–∞—Ñ–∏–∫ –≤–µ—Å–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "        plt.subplot(2, 2, 2)\n",
        "        has_weights = False\n",
        "\n",
        "        colors = ['blue', 'green', 'red']\n",
        "        labels = ['Client 0 (ENF)', 'Client 1 (NGA)', 'Client 2 (NEN)']\n",
        "\n",
        "        for client_id in range(3):\n",
        "            weights = client_weights.get(client_id, [])\n",
        "            steps = weight_steps.get(client_id, [])\n",
        "\n",
        "            if weights and len(weights) > 0:\n",
        "                has_weights = True\n",
        "\n",
        "                # –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "                # plt.scatter(steps, weights, alpha=0.3, s=20, color=colors[client_id])\n",
        "\n",
        "                # —Å–≥–ª–∞–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "                if len(weights) >= smooth_window:\n",
        "                    smoothed_weights = smooth_curve_centered(weights, smooth_window)\n",
        "                    plt.plot(steps, smoothed_weights, color=colors[client_id],\n",
        "                            linewidth=3, label=labels[client_id])\n",
        "                else:\n",
        "                    plt.plot(steps, weights, color=colors[client_id],\n",
        "                            linewidth=2, label=labels[client_id])\n",
        "\n",
        "        if has_weights:\n",
        "            plt.xlabel('Step')\n",
        "            plt.ylabel('Weight')\n",
        "            plt.title('Client Weights Evolution')\n",
        "            plt.legend(loc='best')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # –ª–∏–Ω–∏—è —Ä–∞–≤–Ω—ã—Ö –≤–µ—Å–æ–≤\n",
        "            plt.axhline(y=1/3, color='gray', linestyle=':', alpha=0.7,\n",
        "                       linewidth=2, label='Equal (1/3)')\n",
        "            plt.legend()\n",
        "\n",
        "            plt.ylim(-0.05, 1.05)\n",
        "\n",
        "        # –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Å—Ä–µ–¥–Ω–∏—Ö –≤–µ—Å–æ–≤\n",
        "        plt.subplot(2, 2, 3)\n",
        "\n",
        "        avg_weights = []\n",
        "        std_weights = []\n",
        "        client_labels = []\n",
        "\n",
        "        for client_id in range(3):\n",
        "            weights = client_weights.get(client_id, [])\n",
        "            if weights:\n",
        "                avg_weights.append(np.mean(weights))\n",
        "                std_weights.append(np.std(weights) if len(weights) > 1 else 0)\n",
        "                client_labels.append(f'Client {client_id}')\n",
        "\n",
        "        if avg_weights:\n",
        "            x_pos = np.arange(len(avg_weights))\n",
        "            bars = plt.bar(x_pos, avg_weights, yerr=std_weights,\n",
        "                          color=colors[:len(avg_weights)], alpha=0.7,\n",
        "                          capsize=5, error_kw={'elinewidth': 2})\n",
        "\n",
        "            plt.axhline(y=1/3, color='gray', linestyle='--', alpha=0.5, label='Equal')\n",
        "\n",
        "            for i, (bar, avg, std) in enumerate(zip(bars, avg_weights, std_weights)):\n",
        "                height = bar.get_height()\n",
        "                plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                        f'{avg:.3f} ¬± {std:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "            plt.xticks(x_pos, client_labels)\n",
        "            plt.ylabel('Average Weight')\n",
        "            plt.title('Average Client Weights with Std Dev')\n",
        "            plt.legend()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbwKBp8mi0ei"
      },
      "source": [
        "## –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "UAkqE-JGlZ-m"
      },
      "outputs": [],
      "source": [
        "def main_simple():\n",
        "    config = Config()\n",
        "    config.npeers = 3\n",
        "    config.epochs = 1\n",
        "    config.max_steps = 500\n",
        "    config.batch_size = 2\n",
        "    config.val_every = 50\n",
        "\n",
        "    logger = SimpleLogger()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.model_checkpoint)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(config.model_checkpoint)\n",
        "    model = model.to(config.device)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    train_loaders, val_loader, test_loaders = create_language_clients(config, tokenizer)\n",
        "\n",
        "    optimizer = MeritFedAdamFixed(model.parameters(), config)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Starting FIXED training (max_steps={config.max_steps})\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{config.epochs}\")\n",
        "\n",
        "        train_loss = train_epoch_fl_simple_fixed(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            optimizer=optimizer,\n",
        "            train_loaders=train_loaders,\n",
        "            val_loader=val_loader,\n",
        "            logger=logger,\n",
        "            config=config,\n",
        "            epoch=epoch,\n",
        "            val_every=config.val_every\n",
        "        )\n",
        "\n",
        "        # –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
        "        model.eval()\n",
        "        val_loss, val_batches = validate_simple(model, tokenizer, val_loader, logger, config)\n",
        "        model.train()\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} completed:\")\n",
        "        print(f\"   Train Loss = {train_loss:.4f}\")\n",
        "        print(f\"   Val Loss = {val_loss:.4f} ({val_batches} batches)\")\n",
        "\n",
        "        save_model_simple(model, tokenizer, config, epoch=epoch+1)\n",
        "\n",
        "        if logger.get_step() >= config.max_steps:\n",
        "            break\n",
        "\n",
        "    # –ë—ã—Å—Ç—Ä–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Quick test:\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model.eval()\n",
        "    for client_id in range(3):\n",
        "        if client_id in test_loaders:\n",
        "            loader = test_loaders[client_id]\n",
        "            dataset = loader.dataset\n",
        "            if len(dataset) > 0:\n",
        "                sample = dataset[0]\n",
        "                inputs = tokenizer(sample['input'], return_tensors=\"pt\").to(config.device)\n",
        "                with torch.no_grad():\n",
        "                    outputs = model.generate(**inputs, max_length=50)\n",
        "                translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                print(f\"Client {client_id}: {sample['input'][:50]}... -> {translation[:50]}...\")\n",
        "\n",
        "    visualize_results(logger)\n",
        "    return model, tokenizer, logger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1X1dcwSla4m"
      },
      "source": [
        "# –ó–∞–ø—É—Å–∫"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fa551c132feb44febc20ea803b8c9fde",
            "d14c346160914ddf91e072551748a4af",
            "360f59a848314bf1bcc052fa7ee862fb",
            "eb366f9f242e437db16a83dd785b8247",
            "add5848224864485a1539e425812b183",
            "09f27b49ad4b42ac8157d94e6a7a2e3a",
            "a62ec6941fa149f5a7283ec4ae03fa98",
            "817320e627fa41f9bcada3f2142890d7",
            "35b94d9c4ecf4c92b8f3be219b40d65d",
            "1b8a7b22ac7d461185ee0647485f4ff2",
            "385af288fc034ef2ade84cabf8ad7ad4"
          ]
        },
        "id": "5PgRu4arRpf-",
        "outputId": "9f9732dc-b55d-4300-8f77-619ab16df2ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å Hugging Face...\n",
            "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 97434 –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
            "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 96608 –ø—Ä–∏–º–µ—Ä–æ–≤, –ø—Ä–æ–ø—É—â–µ–Ω–æ 826\n",
            "\n",
            "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ —è–∑—ã–∫–∞–º:\n",
            "  - ENF: 51777 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "  - NGA: 34609 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "  - NEN: 10222 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "\n",
            "============================================================\n",
            "Starting FIXED training (max_steps=500)\n",
            "============================================================\n",
            "\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa551c132feb44febc20ea803b8c9fde",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FL Epoch 1:   0%|          | 0/3578 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Processing] Client 0, batch 0\n",
            "  Loss for client 0: 6.0191\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 0\n",
            "  Loss for client 1: 7.9001\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 0\n",
            "  Loss for client 2: 6.6126\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 1\n",
            "  Client 0: total grad norm = 261.948647 (253 params)\n",
            "  Client 1: total grad norm = 490.343046 (253 params)\n",
            "  Client 2: total grad norm = 267.993286 (253 params)\n",
            "\n",
            "=== Optimizer Step 1 ===\n",
            "\n",
            "=== Updating Client Weights (Step 1) ===\n",
            "Gradient norms: [19.52560043334961, 48.04645538330078, 19.035829544067383]\n",
            "Target weights: [0.4111641049385071, 0.16709299385547638, 0.42174291610717773]\n",
            "Updated weights: [0.35668256878852844, 0.28346124291419983, 0.3598562180995941]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 1\n",
            "  Loss for client 0: 5.5284\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 1\n",
            "  Loss for client 1: 5.1832\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 1\n",
            "  Loss for client 2: 5.2492\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 2\n",
            "  Client 0: total grad norm = 188.826138 (253 params)\n",
            "  Client 1: total grad norm = 186.223427 (253 params)\n",
            "  Client 2: total grad norm = 189.367108 (253 params)\n",
            "\n",
            "=== Optimizer Step 2 ===\n",
            "\n",
            "=== Updating Client Weights (Step 2) ===\n",
            "Gradient norms: [17.612607955932617, 16.262428283691406, 16.30615234375]\n",
            "Target weights: [0.3161410689353943, 0.3423885405063629, 0.3414704203605652]\n",
            "Updated weights: [0.34452012181282043, 0.3011394143104553, 0.35434049367904663]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 2\n",
            "  Loss for client 0: 4.4245\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 2\n",
            "  Loss for client 1: 4.3760\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 2\n",
            "  Loss for client 2: 4.4685\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 3\n",
            "  Client 0: total grad norm = 144.082054 (253 params)\n",
            "  Client 1: total grad norm = 159.869724 (253 params)\n",
            "  Client 2: total grad norm = 167.672254 (253 params)\n",
            "\n",
            "=== Optimizer Step 3 ===\n",
            "\n",
            "=== Updating Client Weights (Step 3) ===\n",
            "Gradient norms: [13.3594388961792, 17.489574432373047, 14.877772331237793]\n",
            "Target weights: [0.3756859600543976, 0.28696832060813904, 0.33734574913978577]\n",
            "Updated weights: [0.35386988520622253, 0.2968880832195282, 0.34924206137657166]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 3\n",
            "  Loss for client 0: 4.0489\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 3\n",
            "  Loss for client 1: 4.0928\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 3\n",
            "  Loss for client 2: 4.1284\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 4\n",
            "  Client 0: total grad norm = 112.698194 (253 params)\n",
            "  Client 1: total grad norm = 123.272445 (253 params)\n",
            "  Client 2: total grad norm = 107.159664 (253 params)\n",
            "\n",
            "=== Optimizer Step 4 ===\n",
            "\n",
            "=== Updating Client Weights (Step 4) ===\n",
            "Gradient norms: [12.401954650878906, 12.413128852844238, 13.615030288696289]\n",
            "Target weights: [0.3436424136161804, 0.34333309531211853, 0.31302449107170105]\n",
            "Updated weights: [0.35080164670944214, 0.3108215928077698, 0.3383767902851105]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 4\n",
            "  Loss for client 0: 4.3579\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 4\n",
            "  Loss for client 1: 4.3798\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 4\n",
            "  Loss for client 2: 4.1818\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 5\n",
            "  Client 0: total grad norm = 122.874593 (253 params)\n",
            "  Client 1: total grad norm = 210.150222 (253 params)\n",
            "  Client 2: total grad norm = 99.702665 (253 params)\n",
            "\n",
            "=== Optimizer Step 5 ===\n",
            "\n",
            "=== Updating Client Weights (Step 5) ===\n",
            "Gradient norms: [13.82482624053955, 20.56769371032715, 11.429197311401367]\n",
            "Target weights: [0.34700918197631836, 0.23324644565582275, 0.4197444021701813]\n",
            "Updated weights: [0.3496639132499695, 0.28754904866218567, 0.36278706789016724]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 5\n",
            "  Loss for client 0: 3.7683\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 5\n",
            "  Loss for client 1: 3.7260\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 5\n",
            "  Loss for client 2: 3.9237\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 6\n",
            "  Client 0: total grad norm = 118.903291 (253 params)\n",
            "  Client 1: total grad norm = 86.776702 (253 params)\n",
            "  Client 2: total grad norm = 103.064892 (253 params)\n",
            "\n",
            "=== Optimizer Step 6 ===\n",
            "\n",
            "=== Updating Client Weights (Step 6) ===\n",
            "Gradient norms: [12.754552841186523, 11.258694648742676, 12.748176574707031]\n",
            "Target weights: [0.31914594769477844, 0.3615484833717346, 0.31930556893348694]\n",
            "Updated weights: [0.34050852060317993, 0.30974888801574707, 0.3497426211833954]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 6\n",
            "  Loss for client 0: 3.1888\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 6\n",
            "  Loss for client 1: 3.9691\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 6\n",
            "  Loss for client 2: 3.4169\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 7\n",
            "  Client 0: total grad norm = 228.735826 (253 params)\n",
            "  Client 1: total grad norm = 75.640320 (253 params)\n",
            "  Client 2: total grad norm = 98.542078 (253 params)\n",
            "\n",
            "=== Optimizer Step 7 ===\n",
            "\n",
            "=== Updating Client Weights (Step 7) ===\n",
            "Gradient norms: [22.028640747070312, 10.180285453796387, 12.42745590209961]\n",
            "Target weights: [0.2025754749774933, 0.43834352493286133, 0.3590809106826782]\n",
            "Updated weights: [0.29912862181663513, 0.34832727909088135, 0.3525441288948059]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 7\n",
            "  Loss for client 0: 3.5377\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 7\n",
            "  Loss for client 1: 3.2052\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 7\n",
            "  Loss for client 2: 3.5351\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 8\n",
            "  Client 0: total grad norm = 102.126411 (253 params)\n",
            "  Client 1: total grad norm = 151.599921 (253 params)\n",
            "  Client 2: total grad norm = 118.145145 (253 params)\n",
            "\n",
            "=== Optimizer Step 8 ===\n",
            "\n",
            "=== Updating Client Weights (Step 8) ===\n",
            "Gradient norms: [12.011945724487305, 15.147071838378906, 11.857917785644531]\n",
            "Target weights: [0.35637786984443665, 0.28261512517929077, 0.36100703477859497]\n",
            "Updated weights: [0.31630340218544006, 0.32861363887786865, 0.35508298873901367]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 8\n",
            "  Loss for client 0: 3.3158\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 8\n",
            "  Loss for client 1: 3.2608\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 8\n",
            "  Loss for client 2: 2.9820\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 9\n",
            "  Client 0: total grad norm = 158.295465 (253 params)\n",
            "  Client 1: total grad norm = 105.871001 (253 params)\n",
            "  Client 2: total grad norm = 233.513130 (253 params)\n",
            "\n",
            "=== Optimizer Step 9 ===\n",
            "\n",
            "=== Updating Client Weights (Step 9) ===\n",
            "Gradient norms: [16.19260025024414, 11.232714653015137, 25.052555084228516]\n",
            "Target weights: [0.32384443283081055, 0.4668402671813965, 0.20931529998779297]\n",
            "Updated weights: [0.3185657262802124, 0.3700816035270691, 0.3113526701927185]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 9\n",
            "  Loss for client 0: 3.4899\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 9\n",
            "  Loss for client 1: 3.4538\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 9\n",
            "  Loss for client 2: 2.8561\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 10\n",
            "  Client 0: total grad norm = 83.370596 (253 params)\n",
            "  Client 1: total grad norm = 76.325406 (253 params)\n",
            "  Client 2: total grad norm = 92.021811 (253 params)\n",
            "\n",
            "=== Optimizer Step 10 ===\n",
            "\n",
            "=== Updating Client Weights (Step 10) ===\n",
            "Gradient norms: [10.202948570251465, 9.318391799926758, 11.254663467407227]\n",
            "Target weights: [0.3331691026687622, 0.3647954761981964, 0.3020354211330414]\n",
            "Updated weights: [0.3229467570781708, 0.36849576234817505, 0.30855751037597656]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 10\n",
            "  Loss for client 0: 2.8389\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 10\n",
            "  Loss for client 1: 2.8675\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 10\n",
            "  Loss for client 2: 3.6301\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 11\n",
            "  Client 0: total grad norm = 67.983249 (253 params)\n",
            "  Client 1: total grad norm = 59.953152 (253 params)\n",
            "  Client 2: total grad norm = 77.466292 (253 params)\n",
            "\n",
            "=== Optimizer Step 11 ===\n",
            "\n",
            "=== Updating Client Weights (Step 11) ===\n",
            "Gradient norms: [8.68542194366455, 7.308577537536621, 10.669960975646973]\n",
            "Target weights: [0.33306750655174255, 0.3958132565021515, 0.27111926674842834]\n",
            "Updated weights: [0.3259829878807068, 0.3766910135746002, 0.2973260283470154]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 11\n",
            "  Loss for client 0: 2.4427\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 11\n",
            "  Loss for client 1: 2.5414\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 11\n",
            "  Loss for client 2: 2.3487\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 12\n",
            "  Client 0: total grad norm = 61.165882 (253 params)\n",
            "  Client 1: total grad norm = 51.951585 (253 params)\n",
            "  Client 2: total grad norm = 57.423078 (253 params)\n",
            "\n",
            "=== Optimizer Step 12 ===\n",
            "\n",
            "=== Updating Client Weights (Step 12) ===\n",
            "Gradient norms: [7.2413763999938965, 7.08280611038208, 8.284652709960938]\n",
            "Target weights: [0.34524908661842346, 0.35297858715057373, 0.3017722964286804]\n",
            "Updated weights: [0.33176282048225403, 0.3695772886276245, 0.29865992069244385]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 12\n",
            "  Loss for client 0: 2.8058\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 12\n",
            "  Loss for client 1: 3.5664\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 12\n",
            "  Loss for client 2: 3.4346\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 13\n",
            "  Client 0: total grad norm = 73.280766 (253 params)\n",
            "  Client 1: total grad norm = 140.325054 (253 params)\n",
            "  Client 2: total grad norm = 78.272970 (253 params)\n",
            "\n",
            "=== Optimizer Step 13 ===\n",
            "\n",
            "=== Updating Client Weights (Step 13) ===\n",
            "Gradient norms: [9.844414710998535, 15.262775421142578, 11.223224639892578]\n",
            "Target weights: [0.39648839831352234, 0.25573307275772095, 0.3477784991264343]\n",
            "Updated weights: [0.3511804938316345, 0.33542400598526, 0.31339550018310547]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 13\n",
            "  Loss for client 0: 3.2318\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 13\n",
            "  Loss for client 1: 3.2426\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 13\n",
            "  Loss for client 2: 3.7606\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 14\n",
            "  Client 0: total grad norm = 88.237044 (253 params)\n",
            "  Client 1: total grad norm = 83.642205 (253 params)\n",
            "  Client 2: total grad norm = 74.932138 (253 params)\n",
            "\n",
            "=== Optimizer Step 14 ===\n",
            "\n",
            "=== Updating Client Weights (Step 14) ===\n",
            "Gradient norms: [12.076285362243652, 12.653291702270508, 12.507828712463379]\n",
            "Target weights: [0.34247785806655884, 0.3268604278564453, 0.33066174387931824]\n",
            "Updated weights: [0.34856972098350525, 0.3328549265861511, 0.318575382232666]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 14\n",
            "  Loss for client 0: 2.8536\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 14\n",
            "  Loss for client 1: 3.2185\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 14\n",
            "  Loss for client 2: 2.5745\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 15\n",
            "  Client 0: total grad norm = 89.422221 (253 params)\n",
            "  Client 1: total grad norm = 103.935115 (253 params)\n",
            "  Client 2: total grad norm = 80.124968 (253 params)\n",
            "\n",
            "=== Optimizer Step 15 ===\n",
            "\n",
            "=== Updating Client Weights (Step 15) ===\n",
            "Gradient norms: [12.395801544189453, 14.726574897766113, 12.257962226867676]\n",
            "Target weights: [0.3505113124847412, 0.2950359284877777, 0.35445278882980347]\n",
            "Updated weights: [0.34915220737457275, 0.3215092420578003, 0.32933861017227173]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 15\n",
            "  Loss for client 0: 1.8408\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 15\n",
            "  Loss for client 1: 2.9132\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 15\n",
            "  Loss for client 2: 3.1996\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 16\n",
            "  Client 0: total grad norm = 155.196971 (253 params)\n",
            "  Client 1: total grad norm = 68.667294 (253 params)\n",
            "  Client 2: total grad norm = 66.773535 (253 params)\n",
            "\n",
            "=== Optimizer Step 16 ===\n",
            "\n",
            "=== Updating Client Weights (Step 16) ===\n",
            "Gradient norms: [16.391881942749023, 11.570361137390137, 11.6104097366333]\n",
            "Target weights: [0.26119622588157654, 0.3700401186943054, 0.3687637150287628]\n",
            "Updated weights: [0.32276540994644165, 0.3360685110092163, 0.3411661386489868]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 16\n",
            "  Loss for client 0: 2.9701\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 16\n",
            "  Loss for client 1: 2.8838\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 16\n",
            "  Loss for client 2: 3.1077\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 17\n",
            "  Client 0: total grad norm = 68.446912 (253 params)\n",
            "  Client 1: total grad norm = 75.177169 (253 params)\n",
            "  Client 2: total grad norm = 59.815205 (253 params)\n",
            "\n",
            "=== Optimizer Step 17 ===\n",
            "\n",
            "=== Updating Client Weights (Step 17) ===\n",
            "Gradient norms: [9.851290702819824, 10.14027214050293, 9.689702033996582]\n",
            "Target weights: [0.3346520960330963, 0.32511505484580994, 0.34023287892341614]\n",
            "Updated weights: [0.32633140683174133, 0.33278244733810425, 0.3408861458301544]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 17\n",
            "  Loss for client 0: 2.6604\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 17\n",
            "  Loss for client 1: 3.2300\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 17\n",
            "  Loss for client 2: 3.4525\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 18\n",
            "  Client 0: total grad norm = 73.590304 (253 params)\n",
            "  Client 1: total grad norm = 70.414373 (253 params)\n",
            "  Client 2: total grad norm = 63.189440 (253 params)\n",
            "\n",
            "=== Optimizer Step 18 ===\n",
            "\n",
            "=== Updating Client Weights (Step 18) ===\n",
            "Gradient norms: [11.081897735595703, 14.575922966003418, 9.212450981140137]\n",
            "Target weights: [0.3374714255332947, 0.25657543540000916, 0.40595316886901855]\n",
            "Updated weights: [0.3296734094619751, 0.3099203407764435, 0.3604062497615814]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 18\n",
            "  Loss for client 0: 3.4802\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 18\n",
            "  Loss for client 1: 3.1678\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 18\n",
            "  Loss for client 2: 2.8198\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 19\n",
            "  Client 0: total grad norm = 82.505013 (253 params)\n",
            "  Client 1: total grad norm = 66.021404 (253 params)\n",
            "  Client 2: total grad norm = 67.741313 (253 params)\n",
            "\n",
            "=== Optimizer Step 19 ===\n",
            "\n",
            "=== Updating Client Weights (Step 19) ===\n",
            "Gradient norms: [15.049259185791016, 9.385491371154785, 11.713932037353516]\n",
            "Target weights: [0.25718894600868225, 0.41239219903945923, 0.33041882514953613]\n",
            "Updated weights: [0.30792805552482605, 0.340661883354187, 0.35141003131866455]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 19\n",
            "  Loss for client 0: 2.8468\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 19\n",
            "  Loss for client 1: 3.6646\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 19\n",
            "  Loss for client 2: 2.4010\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 20\n",
            "  Client 0: total grad norm = 61.376788 (253 params)\n",
            "  Client 1: total grad norm = 69.932636 (253 params)\n",
            "  Client 2: total grad norm = 60.232003 (253 params)\n",
            "\n",
            "=== Optimizer Step 20 ===\n",
            "\n",
            "=== Updating Client Weights (Step 20) ===\n",
            "Gradient norms: [10.303882598876953, 10.631836891174316, 8.085575103759766]\n",
            "Target weights: [0.3083082437515259, 0.2987980246543884, 0.3928937613964081]\n",
            "Updated weights: [0.30804210901260376, 0.328102707862854, 0.36385515332221985]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 20\n",
            "  Loss for client 0: 2.2903\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 20\n",
            "  Loss for client 1: 3.2207\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 20\n",
            "  Loss for client 2: 3.2663\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 21\n",
            "  Client 0: total grad norm = 53.232431 (253 params)\n",
            "  Client 1: total grad norm = 60.590168 (253 params)\n",
            "  Client 2: total grad norm = 72.722635 (253 params)\n",
            "\n",
            "=== Optimizer Step 21 ===\n",
            "\n",
            "=== Updating Client Weights (Step 21) ===\n",
            "Gradient norms: [8.681912422180176, 9.758540153503418, 11.123071670532227]\n",
            "Target weights: [0.37450307607650757, 0.3331853747367859, 0.29231157898902893]\n",
            "Updated weights: [0.3279803991317749, 0.32962751388549805, 0.34239208698272705]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 21\n",
            "  Loss for client 0: 2.2628\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 21\n",
            "  Loss for client 1: 2.6830\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 21\n",
            "  Loss for client 2: 2.8810\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 22\n",
            "  Client 0: total grad norm = 67.069991 (253 params)\n",
            "  Client 1: total grad norm = 74.513848 (253 params)\n",
            "  Client 2: total grad norm = 58.323345 (253 params)\n",
            "\n",
            "=== Optimizer Step 22 ===\n",
            "\n",
            "=== Updating Client Weights (Step 22) ===\n",
            "Gradient norms: [9.270052909851074, 11.806411743164062, 8.282727241516113]\n",
            "Target weights: [0.3443082869052887, 0.2703408896923065, 0.3853508532047272]\n",
            "Updated weights: [0.3328787684440613, 0.31184151768684387, 0.35527971386909485]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 22\n",
            "  Loss for client 0: 3.1399\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 22\n",
            "  Loss for client 1: 2.9729\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 22\n",
            "  Loss for client 2: 2.6728\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 23\n",
            "  Client 0: total grad norm = 71.140256 (253 params)\n",
            "  Client 1: total grad norm = 97.824000 (253 params)\n",
            "  Client 2: total grad norm = 61.281062 (253 params)\n",
            "\n",
            "=== Optimizer Step 23 ===\n",
            "\n",
            "=== Updating Client Weights (Step 23) ===\n",
            "Gradient norms: [12.015787124633789, 12.985554695129395, 9.088627815246582]\n",
            "Target weights: [0.3079397976398468, 0.28494271636009216, 0.40711748600006104]\n",
            "Updated weights: [0.3253970742225647, 0.30377188324928284, 0.37083104252815247]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 23\n",
            "  Loss for client 0: 2.4948\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 23\n",
            "  Loss for client 1: 2.7643\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 23\n",
            "  Loss for client 2: 3.0939\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 24\n",
            "  Client 0: total grad norm = 72.306796 (253 params)\n",
            "  Client 1: total grad norm = 62.229447 (253 params)\n",
            "  Client 2: total grad norm = 63.141594 (253 params)\n",
            "\n",
            "=== Optimizer Step 24 ===\n",
            "\n",
            "=== Updating Client Weights (Step 24) ===\n",
            "Gradient norms: [9.947629928588867, 9.440135955810547, 9.235194206237793]\n",
            "Target weights: [0.3193967044353485, 0.3365671932697296, 0.3440360724925995]\n",
            "Updated weights: [0.3235969543457031, 0.3136104941368103, 0.3627925515174866]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 24\n",
            "  Loss for client 0: 2.5331\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 24\n",
            "  Loss for client 1: 3.2246\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 24\n",
            "  Loss for client 2: 2.8306\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 25\n",
            "  Client 0: total grad norm = 60.140413 (253 params)\n",
            "  Client 1: total grad norm = 58.658097 (253 params)\n",
            "  Client 2: total grad norm = 53.067466 (253 params)\n",
            "\n",
            "=== Optimizer Step 25 ===\n",
            "\n",
            "=== Updating Client Weights (Step 25) ===\n",
            "Gradient norms: [9.732634544372559, 8.106759071350098, 7.360099792480469]\n",
            "Target weights: [0.2838563323020935, 0.3407859802246094, 0.3753576874732971]\n",
            "Updated weights: [0.3116747736930847, 0.32176315784454346, 0.3665620684623718]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 25\n",
            "  Loss for client 0: 1.7769\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 25\n",
            "  Loss for client 1: 2.6327\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 25\n",
            "  Loss for client 2: 2.8253\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 26\n",
            "  Client 0: total grad norm = 43.755869 (253 params)\n",
            "  Client 1: total grad norm = 61.846886 (253 params)\n",
            "  Client 2: total grad norm = 67.935793 (253 params)\n",
            "\n",
            "=== Optimizer Step 26 ===\n",
            "\n",
            "=== Updating Client Weights (Step 26) ===\n",
            "Gradient norms: [6.033435344696045, 10.4882173538208, 11.412951469421387]\n",
            "Target weights: [0.47530630230903625, 0.2734239399433136, 0.25126978754997253]\n",
            "Updated weights: [0.3607642352581024, 0.3072614073753357, 0.3319743871688843]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 26\n",
            "  Loss for client 0: 2.6066\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 26\n",
            "  Loss for client 1: 2.5401\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 26\n",
            "  Loss for client 2: 2.8506\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 27\n",
            "  Client 0: total grad norm = 65.809432 (253 params)\n",
            "  Client 1: total grad norm = 56.012560 (253 params)\n",
            "  Client 2: total grad norm = 62.650865 (253 params)\n",
            "\n",
            "=== Optimizer Step 27 ===\n",
            "\n",
            "=== Updating Client Weights (Step 27) ===\n",
            "Gradient norms: [9.38871955871582, 8.270644187927246, 8.655160903930664]\n",
            "Target weights: [0.3105648159980774, 0.3525488078594208, 0.33688637614250183]\n",
            "Updated weights: [0.34570440649986267, 0.32084763050079346, 0.33344799280166626]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 27\n",
            "  Loss for client 0: 2.8536\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 27\n",
            "  Loss for client 1: 2.8462\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 27\n",
            "  Loss for client 2: 2.6626\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 28\n",
            "  Client 0: total grad norm = 78.283834 (253 params)\n",
            "  Client 1: total grad norm = 77.357803 (253 params)\n",
            "  Client 2: total grad norm = 64.779038 (253 params)\n",
            "\n",
            "=== Optimizer Step 28 ===\n",
            "\n",
            "=== Updating Client Weights (Step 28) ===\n",
            "Gradient norms: [10.276625633239746, 8.74009895324707, 9.844579696655273]\n",
            "Target weights: [0.3105890154838562, 0.3651912212371826, 0.3242197632789612]\n",
            "Updated weights: [0.33516979217529297, 0.3341507017612457, 0.3306795358657837]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 28\n",
            "  Loss for client 0: 3.0756\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 28\n",
            "  Loss for client 1: 1.8349\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 28\n",
            "  Loss for client 2: 2.8200\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 29\n",
            "  Client 0: total grad norm = 73.121585 (253 params)\n",
            "  Client 1: total grad norm = 73.019105 (253 params)\n",
            "  Client 2: total grad norm = 74.266164 (253 params)\n",
            "\n",
            "=== Optimizer Step 29 ===\n",
            "\n",
            "=== Updating Client Weights (Step 29) ===\n",
            "Gradient norms: [11.012044906616211, 8.789047241210938, 11.439624786376953]\n",
            "Target weights: [0.3109886944293976, 0.3896464705467224, 0.29936483502388]\n",
            "Updated weights: [0.3279154598712921, 0.35079944133758545, 0.3212851285934448]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 29\n",
            "  Loss for client 0: 1.9807\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 29\n",
            "  Loss for client 1: 2.1710\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 29\n",
            "  Loss for client 2: 2.1901\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 30\n",
            "  Client 0: total grad norm = 72.082640 (253 params)\n",
            "  Client 1: total grad norm = 74.399549 (253 params)\n",
            "  Client 2: total grad norm = 63.670356 (253 params)\n",
            "\n",
            "=== Optimizer Step 30 ===\n",
            "\n",
            "=== Updating Client Weights (Step 30) ===\n",
            "Gradient norms: [8.974283218383789, 8.85312271118164, 7.5830607414245605]\n",
            "Target weights: [0.3127785325050354, 0.31705909967422485, 0.37016230821609497]\n",
            "Updated weights: [0.3233743906021118, 0.34067732095718384, 0.33594828844070435]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 30\n",
            "  Loss for client 0: 3.3988\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 30\n",
            "  Loss for client 1: 2.4656\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 30\n",
            "  Loss for client 2: 2.8491\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 31\n",
            "  Client 0: total grad norm = 70.934733 (253 params)\n",
            "  Client 1: total grad norm = 82.104069 (253 params)\n",
            "  Client 2: total grad norm = 92.182419 (253 params)\n",
            "\n",
            "=== Optimizer Step 31 ===\n",
            "\n",
            "=== Updating Client Weights (Step 31) ===\n",
            "Gradient norms: [9.608457565307617, 11.006303787231445, 9.588415145874023]\n",
            "Target weights: [0.3478156626224518, 0.3036416172981262, 0.3485426902770996]\n",
            "Updated weights: [0.33070677518844604, 0.3295665979385376, 0.33972659707069397]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 31\n",
            "  Loss for client 0: 2.6302\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 31\n",
            "  Loss for client 1: 2.4811\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 31\n",
            "  Loss for client 2: 2.5150\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 32\n",
            "  Client 0: total grad norm = 72.791227 (253 params)\n",
            "  Client 1: total grad norm = 66.753612 (253 params)\n",
            "  Client 2: total grad norm = 69.156649 (253 params)\n",
            "\n",
            "=== Optimizer Step 32 ===\n",
            "\n",
            "=== Updating Client Weights (Step 32) ===\n",
            "Gradient norms: [9.39001178741455, 7.716649532318115, 10.052909851074219]\n",
            "Target weights: [0.31736865639686584, 0.3861903250217438, 0.29644107818603516]\n",
            "Updated weights: [0.32670533657073975, 0.3465537130832672, 0.32674095034599304]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 32\n",
            "  Loss for client 0: 1.7343\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 32\n",
            "  Loss for client 1: 2.2538\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 32\n",
            "  Loss for client 2: 2.5957\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 33\n",
            "  Client 0: total grad norm = 86.724510 (253 params)\n",
            "  Client 1: total grad norm = 69.032275 (253 params)\n",
            "  Client 2: total grad norm = 72.025053 (253 params)\n",
            "\n",
            "=== Optimizer Step 33 ===\n",
            "\n",
            "=== Updating Client Weights (Step 33) ===\n",
            "Gradient norms: [11.459359169006348, 9.208698272705078, 9.904685020446777]\n",
            "Target weights: [0.29399922490119934, 0.3658543825149536, 0.34014636278152466]\n",
            "Updated weights: [0.3168935179710388, 0.35234391689300537, 0.3307625651359558]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 33\n",
            "  Loss for client 0: 2.8562\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 33\n",
            "  Loss for client 1: 2.2277\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 33\n",
            "  Loss for client 2: 1.9302\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 34\n",
            "  Client 0: total grad norm = 84.373636 (253 params)\n",
            "  Client 1: total grad norm = 75.675673 (253 params)\n",
            "  Client 2: total grad norm = 73.479082 (253 params)\n",
            "\n",
            "=== Optimizer Step 34 ===\n",
            "\n",
            "=== Updating Client Weights (Step 34) ===\n",
            "Gradient norms: [9.27844524383545, 8.054272651672363, 9.082883834838867]\n",
            "Target weights: [0.31510740518569946, 0.3630007207393646, 0.3218919038772583]\n",
            "Updated weights: [0.31635770201683044, 0.3555409610271454, 0.32810136675834656]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 34\n",
            "  Loss for client 0: 2.3290\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 34\n",
            "  Loss for client 1: 1.6928\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 34\n",
            "  Loss for client 2: 1.6800\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 35\n",
            "  Client 0: total grad norm = 83.722212 (253 params)\n",
            "  Client 1: total grad norm = 112.580150 (253 params)\n",
            "  Client 2: total grad norm = 103.127376 (253 params)\n",
            "\n",
            "=== Optimizer Step 35 ===\n",
            "\n",
            "=== Updating Client Weights (Step 35) ===\n",
            "Gradient norms: [9.586342811584473, 12.484359741210938, 12.461201667785645]\n",
            "Target weights: [0.39414095878601074, 0.3026483356952667, 0.3032107949256897]\n",
            "Updated weights: [0.3396926820278168, 0.33967316150665283, 0.3206341862678528]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 35\n",
            "  Loss for client 0: 2.2400\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 35\n",
            "  Loss for client 1: 2.6513\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 35\n",
            "  Loss for client 2: 2.9429\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 36\n",
            "  Client 0: total grad norm = 82.091696 (253 params)\n",
            "  Client 1: total grad norm = 80.965169 (253 params)\n",
            "  Client 2: total grad norm = 87.919025 (253 params)\n",
            "\n",
            "=== Optimizer Step 36 ===\n",
            "\n",
            "=== Updating Client Weights (Step 36) ===\n",
            "Gradient norms: [8.23300552368164, 9.770252227783203, 11.187111854553223]\n",
            "Target weights: [0.38780778646469116, 0.32679030299186707, 0.28540197014808655]\n",
            "Updated weights: [0.3541272282600403, 0.33580830693244934, 0.31006452441215515]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 36\n",
            "  Loss for client 0: 2.0485\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 36\n",
            "  Loss for client 1: 2.7849\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 36\n",
            "  Loss for client 2: 2.4547\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 37\n",
            "  Client 0: total grad norm = 73.097484 (253 params)\n",
            "  Client 1: total grad norm = 87.120447 (253 params)\n",
            "  Client 2: total grad norm = 83.848988 (253 params)\n",
            "\n",
            "=== Optimizer Step 37 ===\n",
            "\n",
            "=== Updating Client Weights (Step 37) ===\n",
            "Gradient norms: [8.56309700012207, 10.620684623718262, 8.61524772644043]\n",
            "Target weights: [0.3571157455444336, 0.28793027997016907, 0.3549540340900421]\n",
            "Updated weights: [0.3550237715244293, 0.32144486904144287, 0.3235313594341278]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 37\n",
            "  Loss for client 0: 2.1511\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 37\n",
            "  Loss for client 1: 2.1658\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 37\n",
            "  Loss for client 2: 2.9869\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 38\n",
            "  Client 0: total grad norm = 85.250772 (253 params)\n",
            "  Client 1: total grad norm = 120.005146 (253 params)\n",
            "  Client 2: total grad norm = 90.109560 (253 params)\n",
            "\n",
            "=== Optimizer Step 38 ===\n",
            "\n",
            "=== Updating Client Weights (Step 38) ===\n",
            "Gradient norms: [9.78911304473877, 12.393345832824707, 10.629640579223633]\n",
            "Target weights: [0.368895560503006, 0.2913789749145508, 0.339725524187088]\n",
            "Updated weights: [0.3591853082180023, 0.3124251067638397, 0.32838961482048035]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 38\n",
            "  Loss for client 0: 1.7625\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 38\n",
            "  Loss for client 1: 3.0767\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 38\n",
            "  Loss for client 2: 2.0197\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 39\n",
            "  Client 0: total grad norm = 67.240728 (253 params)\n",
            "  Client 1: total grad norm = 90.888338 (253 params)\n",
            "  Client 2: total grad norm = 96.669530 (253 params)\n",
            "\n",
            "=== Optimizer Step 39 ===\n",
            "\n",
            "=== Updating Client Weights (Step 39) ===\n",
            "Gradient norms: [7.326054096221924, 10.428668022155762, 10.149271965026855]\n",
            "Target weights: [0.41248637437820435, 0.2897683382034302, 0.2977452576160431]\n",
            "Updated weights: [0.3751756250858307, 0.30562809109687805, 0.31919631361961365]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 39\n",
            "  Loss for client 0: 1.9347\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 39\n",
            "  Loss for client 1: 2.3211\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 39\n",
            "  Loss for client 2: 2.3494\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 40\n",
            "  Client 0: total grad norm = 81.338981 (253 params)\n",
            "  Client 1: total grad norm = 73.416695 (253 params)\n",
            "  Client 2: total grad norm = 92.770525 (253 params)\n",
            "\n",
            "=== Optimizer Step 40 ===\n",
            "\n",
            "=== Updating Client Weights (Step 40) ===\n",
            "Gradient norms: [8.41468620300293, 7.210318088531494, 10.658238410949707]\n",
            "Target weights: [0.3382338285446167, 0.3947303593158722, 0.2670358121395111]\n",
            "Updated weights: [0.3640930652618408, 0.33235877752304077, 0.3035481572151184]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 40\n",
            "  Loss for client 0: 1.7587\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 40\n",
            "  Loss for client 1: 2.7672\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 40\n",
            "  Loss for client 2: 2.5654\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 41\n",
            "  Client 0: total grad norm = 62.648470 (253 params)\n",
            "  Client 1: total grad norm = 72.171045 (253 params)\n",
            "  Client 2: total grad norm = 90.104098 (253 params)\n",
            "\n",
            "=== Optimizer Step 41 ===\n",
            "\n",
            "=== Updating Client Weights (Step 41) ===\n",
            "Gradient norms: [7.47413444519043, 8.648876190185547, 10.084222793579102]\n",
            "Target weights: [0.38382628560066223, 0.33169275522232056, 0.2844809591770172]\n",
            "Updated weights: [0.370013028383255, 0.33215898275375366, 0.29782798886299133]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 41\n",
            "  Loss for client 0: 2.2491\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 41\n",
            "  Loss for client 1: 1.4022\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 41\n",
            "  Loss for client 2: 2.8204\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 42\n",
            "  Client 0: total grad norm = 76.478615 (253 params)\n",
            "  Client 1: total grad norm = 56.003960 (253 params)\n",
            "  Client 2: total grad norm = 93.800821 (253 params)\n",
            "\n",
            "=== Optimizer Step 42 ===\n",
            "\n",
            "=== Updating Client Weights (Step 42) ===\n",
            "Gradient norms: [8.704742431640625, 6.586951732635498, 10.08039665222168]\n",
            "Target weights: [0.3139673173427582, 0.4149118959903717, 0.27112075686454773]\n",
            "Updated weights: [0.3531993329524994, 0.35698485374450684, 0.2898158133029938]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 42\n",
            "  Loss for client 0: 1.7880\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 42\n",
            "  Loss for client 1: 3.1622\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 42\n",
            "  Loss for client 2: 2.6607\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 43\n",
            "  Client 0: total grad norm = 72.833898 (253 params)\n",
            "  Client 1: total grad norm = 95.677460 (253 params)\n",
            "  Client 2: total grad norm = 83.017399 (253 params)\n",
            "\n",
            "=== Optimizer Step 43 ===\n",
            "\n",
            "=== Updating Client Weights (Step 43) ===\n",
            "Gradient norms: [8.13233757019043, 11.156858444213867, 10.921289443969727]\n",
            "Target weights: [0.4042787253856659, 0.2946825325489044, 0.3010387420654297]\n",
            "Updated weights: [0.36852315068244934, 0.3382941484451294, 0.29318270087242126]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 43\n",
            "  Loss for client 0: 2.2801\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 43\n",
            "  Loss for client 1: 2.4030\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 43\n",
            "  Loss for client 2: 2.4694\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 44\n",
            "  Client 0: total grad norm = 86.532709 (253 params)\n",
            "  Client 1: total grad norm = 87.813120 (253 params)\n",
            "  Client 2: total grad norm = 65.740248 (253 params)\n",
            "\n",
            "=== Optimizer Step 44 ===\n",
            "\n",
            "=== Updating Client Weights (Step 44) ===\n",
            "Gradient norms: [11.364640235900879, 9.071354866027832, 7.606712818145752]\n",
            "Target weights: [0.26689207553863525, 0.33436375856399536, 0.398744136095047]\n",
            "Updated weights: [0.3380338251590729, 0.3371150493621826, 0.3248511254787445]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 44\n",
            "  Loss for client 0: 1.6874\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 44\n",
            "  Loss for client 1: 2.0471\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 44\n",
            "  Loss for client 2: 2.3238\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 45\n",
            "  Client 0: total grad norm = 80.610996 (253 params)\n",
            "  Client 1: total grad norm = 69.458841 (253 params)\n",
            "  Client 2: total grad norm = 72.805400 (253 params)\n",
            "\n",
            "=== Optimizer Step 45 ===\n",
            "\n",
            "=== Updating Client Weights (Step 45) ===\n",
            "Gradient norms: [8.338483810424805, 7.8959059715271, 8.503174781799316]\n",
            "Target weights: [0.32930663228034973, 0.3477647602558136, 0.3229285478591919]\n",
            "Updated weights: [0.33541566133499146, 0.3403099477291107, 0.32427436113357544]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 45\n",
            "  Loss for client 0: 2.3400\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 45\n",
            "  Loss for client 1: 2.0750\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 45\n",
            "  Loss for client 2: 2.3742\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 46\n",
            "  Client 0: total grad norm = 78.065523 (253 params)\n",
            "  Client 1: total grad norm = 82.738655 (253 params)\n",
            "  Client 2: total grad norm = 67.890347 (253 params)\n",
            "\n",
            "=== Optimizer Step 46 ===\n",
            "\n",
            "=== Updating Client Weights (Step 46) ===\n",
            "Gradient norms: [10.018035888671875, 10.667085647583008, 9.272690773010254]\n",
            "Target weights: [0.33117687702178955, 0.31102606654167175, 0.3577970862388611]\n",
            "Updated weights: [0.3341440260410309, 0.3315247893333435, 0.3343311846256256]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 46\n",
            "  Loss for client 0: 2.4646\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 46\n",
            "  Loss for client 1: 2.3499\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 46\n",
            "  Loss for client 2: 2.6866\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 47\n",
            "  Client 0: total grad norm = 125.476940 (253 params)\n",
            "  Client 1: total grad norm = 80.624365 (253 params)\n",
            "  Client 2: total grad norm = 90.695495 (253 params)\n",
            "\n",
            "=== Optimizer Step 47 ===\n",
            "\n",
            "=== Updating Client Weights (Step 47) ===\n",
            "Gradient norms: [13.890954971313477, 10.209242820739746, 12.584341049194336]\n",
            "Target weights: [0.28864577412605286, 0.39273878931999207, 0.3186154365539551]\n",
            "Updated weights: [0.32049456238746643, 0.34988901019096375, 0.3296164870262146]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 47\n",
            "  Loss for client 0: 1.6130\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 47\n",
            "  Loss for client 1: 2.7146\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 47\n",
            "  Loss for client 2: 2.4663\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 48\n",
            "  Client 0: total grad norm = 84.471441 (253 params)\n",
            "  Client 1: total grad norm = 82.855901 (253 params)\n",
            "  Client 2: total grad norm = 78.618631 (253 params)\n",
            "\n",
            "=== Optimizer Step 48 ===\n",
            "\n",
            "=== Updating Client Weights (Step 48) ===\n",
            "Gradient norms: [8.735090255737305, 10.900065422058105, 10.033958435058594]\n",
            "Target weights: [0.37426096200942993, 0.2999250888824463, 0.3258139193058014]\n",
            "Updated weights: [0.33662447333335876, 0.3348998427391052, 0.3284757137298584]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 48\n",
            "  Loss for client 0: 1.8712\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 48\n",
            "  Loss for client 1: 2.6444\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 48\n",
            "  Loss for client 2: 1.7823\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 49\n",
            "  Client 0: total grad norm = 77.127029 (253 params)\n",
            "  Client 1: total grad norm = 69.283923 (253 params)\n",
            "  Client 2: total grad norm = 68.785654 (253 params)\n",
            "\n",
            "=== Optimizer Step 49 ===\n",
            "\n",
            "=== Updating Client Weights (Step 49) ===\n",
            "Gradient norms: [8.302675247192383, 8.899402618408203, 9.071157455444336]\n",
            "Target weights: [0.35109519958496094, 0.327553391456604, 0.32135143876075745]\n",
            "Updated weights: [0.3409656882286072, 0.3326959013938904, 0.32633844017982483]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 49\n",
            "  Loss for client 0: 1.4410\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 49\n",
            "  Loss for client 1: 2.0945\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 49\n",
            "  Loss for client 2: 2.1629\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 50\n",
            "  Client 0: total grad norm = 62.121432 (253 params)\n",
            "  Client 1: total grad norm = 77.573995 (253 params)\n",
            "  Client 2: total grad norm = 61.734137 (253 params)\n",
            "\n",
            "=== Optimizer Step 50 ===\n",
            "\n",
            "=== Updating Client Weights (Step 50) ===\n",
            "Gradient norms: [6.991505146026611, 9.16862678527832, 7.306525707244873]\n",
            "Target weights: [0.36772391200065613, 0.2804066240787506, 0.35186949372291565]\n",
            "Updated weights: [0.3489931523799896, 0.3170091211795807, 0.3339977562427521]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 50\n",
            "  Val Loss = 2.1461 (5 batches)\n",
            "\n",
            "[Processing] Client 0, batch 50\n",
            "  Loss for client 0: 2.4912\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 50\n",
            "  Loss for client 1: 2.4768\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 50\n",
            "  Loss for client 2: 1.9453\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 51\n",
            "  Client 0: total grad norm = 97.800499 (253 params)\n",
            "  Client 1: total grad norm = 82.783901 (253 params)\n",
            "  Client 2: total grad norm = 76.737870 (253 params)\n",
            "\n",
            "=== Optimizer Step 51 ===\n",
            "\n",
            "=== Updating Client Weights (Step 51) ===\n",
            "Gradient norms: [10.763411521911621, 11.254528999328613, 9.711973190307617]\n",
            "Target weights: [0.32630422711372375, 0.3120652139186859, 0.3616306185722351]\n",
            "Updated weights: [0.34218648076057434, 0.31552594900131226, 0.3422876298427582]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 51\n",
            "  Loss for client 0: 2.5512\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 51\n",
            "  Loss for client 1: 2.0600\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 51\n",
            "  Loss for client 2: 1.6844\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 52\n",
            "  Client 0: total grad norm = 83.669852 (253 params)\n",
            "  Client 1: total grad norm = 78.364371 (253 params)\n",
            "  Client 2: total grad norm = 84.035942 (253 params)\n",
            "\n",
            "=== Optimizer Step 52 ===\n",
            "\n",
            "=== Updating Client Weights (Step 52) ===\n",
            "Gradient norms: [9.890670776367188, 9.341100692749023, 10.852550506591797]\n",
            "Target weights: [0.3366774618625641, 0.35648539662361145, 0.30683717131614685]\n",
            "Updated weights: [0.3405337929725647, 0.3278137743473053, 0.3316524922847748]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 52\n",
            "  Loss for client 0: 2.2510\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 52\n",
            "  Loss for client 1: 2.8099\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 52\n",
            "  Loss for client 2: 2.1187\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 53\n",
            "  Client 0: total grad norm = 86.825364 (253 params)\n",
            "  Client 1: total grad norm = 66.789023 (253 params)\n",
            "  Client 2: total grad norm = 71.838600 (253 params)\n",
            "\n",
            "=== Optimizer Step 53 ===\n",
            "\n",
            "=== Updating Client Weights (Step 53) ===\n",
            "Gradient norms: [10.472143173217773, 7.391852855682373, 10.154879570007324]\n",
            "Target weights: [0.29002684354782104, 0.41088518500328064, 0.2990880012512207]\n",
            "Updated weights: [0.32538169622421265, 0.3527351915836334, 0.3218831419944763]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 53\n",
            "  Loss for client 0: 2.0461\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 53\n",
            "  Loss for client 1: 2.6546\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 53\n",
            "  Loss for client 2: 1.9195\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 54\n",
            "  Client 0: total grad norm = 82.299887 (253 params)\n",
            "  Client 1: total grad norm = 82.391033 (253 params)\n",
            "  Client 2: total grad norm = 80.313654 (253 params)\n",
            "\n",
            "=== Optimizer Step 54 ===\n",
            "\n",
            "=== Updating Client Weights (Step 54) ===\n",
            "Gradient norms: [9.75816535949707, 9.162077903747559, 10.011024475097656]\n",
            "Target weights: [0.3289687931537628, 0.3503715693950653, 0.32065966725349426]\n",
            "Updated weights: [0.32645782828330994, 0.352026104927063, 0.32151609659194946]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 54\n",
            "  Loss for client 0: 2.7411\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 54\n",
            "  Loss for client 1: 1.8153\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 54\n",
            "  Loss for client 2: 2.5793\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 55\n",
            "  Client 0: total grad norm = 86.110675 (253 params)\n",
            "  Client 1: total grad norm = 72.422577 (253 params)\n",
            "  Client 2: total grad norm = 77.010249 (253 params)\n",
            "\n",
            "=== Optimizer Step 55 ===\n",
            "\n",
            "=== Updating Client Weights (Step 55) ===\n",
            "Gradient norms: [11.087599754333496, 7.676945209503174, 11.169986724853516]\n",
            "Target weights: [0.2909601628780365, 0.42022570967674255, 0.28881409764289856]\n",
            "Updated weights: [0.3158085346221924, 0.3724859952926636, 0.31170549988746643]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 55\n",
            "  Loss for client 0: 2.4767\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 55\n",
            "  Loss for client 1: 1.4652\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 55\n",
            "  Loss for client 2: 2.5224\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 56\n",
            "  Client 0: total grad norm = 77.483232 (253 params)\n",
            "  Client 1: total grad norm = 66.316160 (253 params)\n",
            "  Client 2: total grad norm = 83.938916 (253 params)\n",
            "\n",
            "=== Optimizer Step 56 ===\n",
            "\n",
            "=== Updating Client Weights (Step 56) ===\n",
            "Gradient norms: [9.053850173950195, 7.159444332122803, 10.323498725891113]\n",
            "Target weights: [0.318307489156723, 0.4025324285030365, 0.2791600525379181]\n",
            "Updated weights: [0.31655821204185486, 0.38149991631507874, 0.3019418716430664]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 56\n",
            "  Loss for client 0: 1.2949\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 56\n",
            "  Loss for client 1: 2.5459\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 56\n",
            "  Loss for client 2: 2.0708\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 57\n",
            "  Client 0: total grad norm = 58.483353 (253 params)\n",
            "  Client 1: total grad norm = 89.550052 (253 params)\n",
            "  Client 2: total grad norm = 72.685712 (253 params)\n",
            "\n",
            "=== Optimizer Step 57 ===\n",
            "\n",
            "=== Updating Client Weights (Step 57) ===\n",
            "Gradient norms: [6.108378887176514, 10.32657527923584, 9.262969017028809]\n",
            "Target weights: [0.44425469636917114, 0.2627857029438019, 0.29295963048934937]\n",
            "Updated weights: [0.354867160320282, 0.34588563442230225, 0.29924720525741577]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 57\n",
            "  Loss for client 0: 1.4428\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 57\n",
            "  Loss for client 1: 2.3198\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 57\n",
            "  Loss for client 2: 2.2579\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 58\n",
            "  Client 0: total grad norm = 62.869727 (253 params)\n",
            "  Client 1: total grad norm = 80.555208 (253 params)\n",
            "  Client 2: total grad norm = 65.485218 (253 params)\n",
            "\n",
            "=== Optimizer Step 58 ===\n",
            "\n",
            "=== Updating Client Weights (Step 58) ===\n",
            "Gradient norms: [7.8935041427612305, 8.572531700134277, 8.359373092651367]\n",
            "Target weights: [0.34903281927108765, 0.32138603925704956, 0.3295811414718628]\n",
            "Updated weights: [0.35311686992645264, 0.33853575587272644, 0.3083474040031433]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 58\n",
            "  Loss for client 0: 1.9660\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 58\n",
            "  Loss for client 1: 2.4939\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 58\n",
            "  Loss for client 2: 2.0450\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 59\n",
            "  Client 0: total grad norm = 72.335399 (253 params)\n",
            "  Client 1: total grad norm = 79.560725 (253 params)\n",
            "  Client 2: total grad norm = 78.800805 (253 params)\n",
            "\n",
            "=== Optimizer Step 59 ===\n",
            "\n",
            "=== Updating Client Weights (Step 59) ===\n",
            "Gradient norms: [9.118881225585938, 10.725914001464844, 8.661404609680176]\n",
            "Target weights: [0.34447231888771057, 0.2928610146045685, 0.36266660690307617]\n",
            "Updated weights: [0.3505235016345978, 0.32483333349227905, 0.32464316487312317]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 59\n",
            "  Loss for client 0: 1.9517\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 59\n",
            "  Loss for client 1: 2.4959\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 59\n",
            "  Loss for client 2: 2.0350\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 60\n",
            "  Client 0: total grad norm = 92.239354 (253 params)\n",
            "  Client 1: total grad norm = 73.807625 (253 params)\n",
            "  Client 2: total grad norm = 96.347541 (253 params)\n",
            "\n",
            "=== Optimizer Step 60 ===\n",
            "\n",
            "=== Updating Client Weights (Step 60) ===\n",
            "Gradient norms: [10.85251235961914, 8.483582496643066, 10.159660339355469]\n",
            "Target weights: [0.2987363636493683, 0.38215458393096924, 0.31910908222198486]\n",
            "Updated weights: [0.3349873721599579, 0.34202972054481506, 0.32298293709754944]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 60\n",
            "  Loss for client 0: 1.7297\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 60\n",
            "  Loss for client 1: 2.5126\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 60\n",
            "  Loss for client 2: 1.6399\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 61\n",
            "  Client 0: total grad norm = 174.875889 (253 params)\n",
            "  Client 1: total grad norm = 75.624314 (253 params)\n",
            "  Client 2: total grad norm = 53.434001 (253 params)\n",
            "\n",
            "=== Optimizer Step 61 ===\n",
            "\n",
            "=== Updating Client Weights (Step 61) ===\n",
            "Gradient norms: [21.125459671020508, 9.885969161987305, 5.835952281951904]\n",
            "Target weights: [0.1479991376399994, 0.316261351108551, 0.5357394814491272]\n",
            "Updated weights: [0.2788909077644348, 0.3342992067337036, 0.3868098855018616]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 61\n",
            "  Loss for client 0: 2.0135\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 61\n",
            "  Loss for client 1: 2.1411\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 61\n",
            "  Loss for client 2: 1.9718\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 62\n",
            "  Client 0: total grad norm = 69.523499 (253 params)\n",
            "  Client 1: total grad norm = 101.424254 (253 params)\n",
            "  Client 2: total grad norm = 87.274319 (253 params)\n",
            "\n",
            "=== Optimizer Step 62 ===\n",
            "\n",
            "=== Updating Client Weights (Step 62) ===\n",
            "Gradient norms: [8.312231063842773, 10.681507110595703, 9.325486183166504]\n",
            "Target weights: [0.37459713220596313, 0.291507363319397, 0.3338955342769623]\n",
            "Updated weights: [0.30760276317596436, 0.32146164774894714, 0.3709355890750885]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 62\n",
            "  Loss for client 0: 2.4878\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 62\n",
            "  Loss for client 1: 2.7393\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 62\n",
            "  Loss for client 2: 2.0773\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 63\n",
            "  Client 0: total grad norm = 85.971699 (253 params)\n",
            "  Client 1: total grad norm = 90.460511 (253 params)\n",
            "  Client 2: total grad norm = 67.698438 (253 params)\n",
            "\n",
            "=== Optimizer Step 63 ===\n",
            "\n",
            "=== Updating Client Weights (Step 63) ===\n",
            "Gradient norms: [8.580073356628418, 10.434964179992676, 7.512837886810303]\n",
            "Target weights: [0.3373480439186096, 0.277381956577301, 0.38526999950408936]\n",
            "Updated weights: [0.3165263533592224, 0.3082377314567566, 0.375235915184021]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 63\n",
            "  Loss for client 0: 2.1789\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 63\n",
            "  Loss for client 1: 2.4735\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 63\n",
            "  Loss for client 2: 1.4500\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 64\n",
            "  Client 0: total grad norm = 82.653089 (253 params)\n",
            "  Client 1: total grad norm = 94.709700 (253 params)\n",
            "  Client 2: total grad norm = 57.030183 (253 params)\n",
            "\n",
            "=== Optimizer Step 64 ===\n",
            "\n",
            "=== Updating Client Weights (Step 64) ===\n",
            "Gradient norms: [9.652114868164062, 10.661086082458496, 7.239206790924072]\n",
            "Target weights: [0.3087686598300934, 0.2795466482639313, 0.41168469190597534]\n",
            "Updated weights: [0.3141990602016449, 0.29963040351867676, 0.38617053627967834]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 64\n",
            "  Loss for client 0: 1.6765\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 64\n",
            "  Loss for client 1: 3.0112\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 64\n",
            "  Loss for client 2: 2.0724\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 65\n",
            "  Client 0: total grad norm = 62.109177 (253 params)\n",
            "  Client 1: total grad norm = 84.932601 (253 params)\n",
            "  Client 2: total grad norm = 79.345345 (253 params)\n",
            "\n",
            "=== Optimizer Step 65 ===\n",
            "\n",
            "=== Updating Client Weights (Step 65) ===\n",
            "Gradient norms: [7.242288589477539, 9.61513900756836, 8.95361328125]\n",
            "Target weights: [0.39030712842941284, 0.293986052274704, 0.3157068192958832]\n",
            "Updated weights: [0.3370314836502075, 0.2979370951652527, 0.3650314211845398]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 65\n",
            "  Loss for client 0: 2.2098\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 65\n",
            "  Loss for client 1: 1.3239\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 65\n",
            "  Loss for client 2: 2.2236\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 66\n",
            "  Client 0: total grad norm = 75.357162 (253 params)\n",
            "  Client 1: total grad norm = 53.921595 (253 params)\n",
            "  Client 2: total grad norm = 77.112779 (253 params)\n",
            "\n",
            "=== Optimizer Step 66 ===\n",
            "\n",
            "=== Updating Client Weights (Step 66) ===\n",
            "Gradient norms: [9.05125904083252, 5.762004852294922, 8.409977912902832]\n",
            "Target weights: [0.2741900682449341, 0.43071216344833374, 0.2950977385044098]\n",
            "Updated weights: [0.31817907094955444, 0.33776962757110596, 0.3440513014793396]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 66\n",
            "  Loss for client 0: 2.4195\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 66\n",
            "  Loss for client 1: 2.6006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 66\n",
            "  Loss for client 2: 2.5015\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 67\n",
            "  Client 0: total grad norm = 70.290068 (253 params)\n",
            "  Client 1: total grad norm = 92.354936 (253 params)\n",
            "  Client 2: total grad norm = 86.909473 (253 params)\n",
            "\n",
            "=== Optimizer Step 67 ===\n",
            "\n",
            "=== Updating Client Weights (Step 67) ===\n",
            "Gradient norms: [8.322049140930176, 10.415718078613281, 10.527315139770508]\n",
            "Target weights: [0.3861735761165619, 0.30854862928390503, 0.3052777945995331]\n",
            "Updated weights: [0.33857741951942444, 0.32900333404541016, 0.3324192464351654]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 67\n",
            "  Loss for client 0: 1.7106\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 67\n",
            "  Loss for client 1: 1.6340\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 67\n",
            "  Loss for client 2: 2.4145\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 68\n",
            "  Client 0: total grad norm = 67.342937 (253 params)\n",
            "  Client 1: total grad norm = 64.350026 (253 params)\n",
            "  Client 2: total grad norm = 79.648354 (253 params)\n",
            "\n",
            "=== Optimizer Step 68 ===\n",
            "\n",
            "=== Updating Client Weights (Step 68) ===\n",
            "Gradient norms: [7.7456746101379395, 7.899374961853027, 9.696677207946777]\n",
            "Target weights: [0.35979771614074707, 0.3527970016002655, 0.28740525245666504]\n",
            "Updated weights: [0.3449435234069824, 0.336141437292099, 0.3189150393009186]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 68\n",
            "  Loss for client 0: 1.1328\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 68\n",
            "  Loss for client 1: 2.9799\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 68\n",
            "  Loss for client 2: 2.5665\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 69\n",
            "  Client 0: total grad norm = 57.273275 (253 params)\n",
            "  Client 1: total grad norm = 94.816098 (253 params)\n",
            "  Client 2: total grad norm = 84.253252 (253 params)\n",
            "\n",
            "=== Optimizer Step 69 ===\n",
            "\n",
            "=== Updating Client Weights (Step 69) ===\n",
            "Gradient norms: [6.302259922027588, 10.808219909667969, 9.138978004455566]\n",
            "Target weights: [0.4400050938129425, 0.25656643509864807, 0.3034285008907318]\n",
            "Updated weights: [0.3734619915485382, 0.3122689425945282, 0.3142690658569336]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 69\n",
            "  Loss for client 0: 2.3755\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 69\n",
            "  Loss for client 1: 2.3857\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 69\n",
            "  Loss for client 2: 2.9153\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 70\n",
            "  Client 0: total grad norm = 76.767139 (253 params)\n",
            "  Client 1: total grad norm = 79.720525 (253 params)\n",
            "  Client 2: total grad norm = 92.935712 (253 params)\n",
            "\n",
            "=== Optimizer Step 70 ===\n",
            "\n",
            "=== Updating Client Weights (Step 70) ===\n",
            "Gradient norms: [9.854802131652832, 9.992074966430664, 12.123648643493652]\n",
            "Target weights: [0.3572551906108856, 0.35234716534614563, 0.29039764404296875]\n",
            "Updated weights: [0.36859995126724243, 0.3242924213409424, 0.3071076273918152]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 70\n",
            "  Loss for client 0: 2.0932\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 70\n",
            "  Loss for client 1: 1.8226\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 70\n",
            "  Loss for client 2: 2.3316\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 71\n",
            "  Client 0: total grad norm = 66.400010 (253 params)\n",
            "  Client 1: total grad norm = 66.861247 (253 params)\n",
            "  Client 2: total grad norm = 74.024941 (253 params)\n",
            "\n",
            "=== Optimizer Step 71 ===\n",
            "\n",
            "=== Updating Client Weights (Step 71) ===\n",
            "Gradient norms: [7.335916042327881, 7.436194896697998, 9.161551475524902]\n",
            "Target weights: [0.3587774336338043, 0.3539392054080963, 0.287283331155777]\n",
            "Updated weights: [0.36565321683883667, 0.33318647742271423, 0.30116036534309387]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 71\n",
            "  Loss for client 0: 1.9713\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 71\n",
            "  Loss for client 1: 2.5934\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 71\n",
            "  Loss for client 2: 2.3985\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 72\n",
            "  Client 0: total grad norm = 68.483429 (253 params)\n",
            "  Client 1: total grad norm = 92.888081 (253 params)\n",
            "  Client 2: total grad norm = 78.684469 (253 params)\n",
            "\n",
            "=== Optimizer Step 72 ===\n",
            "\n",
            "=== Updating Client Weights (Step 72) ===\n",
            "Gradient norms: [8.3216552734375, 10.413272857666016, 9.302839279174805]\n",
            "Target weights: [0.37124103307724, 0.2966732680797577, 0.3320856988430023]\n",
            "Updated weights: [0.36732956767082214, 0.32223251461982727, 0.31043797731399536]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 72\n",
            "  Loss for client 0: 1.8655\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 72\n",
            "  Loss for client 1: 2.2482\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 72\n",
            "  Loss for client 2: 1.8639\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 73\n",
            "  Client 0: total grad norm = 97.644869 (253 params)\n",
            "  Client 1: total grad norm = 72.029980 (253 params)\n",
            "  Client 2: total grad norm = 62.656797 (253 params)\n",
            "\n",
            "=== Optimizer Step 73 ===\n",
            "\n",
            "=== Updating Client Weights (Step 73) ===\n",
            "Gradient norms: [10.335990905761719, 8.152158737182617, 7.2499566078186035]\n",
            "Target weights: [0.2707425355911255, 0.34327009320259094, 0.38598743081092834]\n",
            "Updated weights: [0.3383534550666809, 0.3285437822341919, 0.333102822303772]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 73\n",
            "  Loss for client 0: 1.1198\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 73\n",
            "  Loss for client 1: 2.5996\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 73\n",
            "  Loss for client 2: 2.7182\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 74\n",
            "  Client 0: total grad norm = 68.785177 (253 params)\n",
            "  Client 1: total grad norm = 106.477624 (253 params)\n",
            "  Client 2: total grad norm = 83.759686 (253 params)\n",
            "\n",
            "=== Optimizer Step 74 ===\n",
            "\n",
            "=== Updating Client Weights (Step 74) ===\n",
            "Gradient norms: [6.636606693267822, 11.354229927062988, 9.366361618041992]\n",
            "Target weights: [0.43609797954559326, 0.25490155816078186, 0.30900052189826965]\n",
            "Updated weights: [0.3676767945289612, 0.30645111203193665, 0.32587212324142456]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 74\n",
            "  Loss for client 0: 1.9036\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 74\n",
            "  Loss for client 1: 3.1342\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 74\n",
            "  Loss for client 2: 2.4893\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 75\n",
            "  Client 0: total grad norm = 68.074914 (253 params)\n",
            "  Client 1: total grad norm = 95.318588 (253 params)\n",
            "  Client 2: total grad norm = 79.754732 (253 params)\n",
            "\n",
            "=== Optimizer Step 75 ===\n",
            "\n",
            "=== Updating Client Weights (Step 75) ===\n",
            "Gradient norms: [7.321863651275635, 11.034255027770996, 9.685507774353027]\n",
            "Target weights: [0.413305401802063, 0.27425193786621094, 0.3124426603317261]\n",
            "Updated weights: [0.3813653588294983, 0.2967913746833801, 0.3218432664871216]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 75\n",
            "  Loss for client 0: 2.1323\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 75\n",
            "  Loss for client 1: 2.4004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 75\n",
            "  Loss for client 2: 2.1861\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 76\n",
            "  Client 0: total grad norm = 79.578731 (253 params)\n",
            "  Client 1: total grad norm = 76.509937 (253 params)\n",
            "  Client 2: total grad norm = 79.357598 (253 params)\n",
            "\n",
            "=== Optimizer Step 76 ===\n",
            "\n",
            "=== Updating Client Weights (Step 76) ===\n",
            "Gradient norms: [8.846742630004883, 8.639791488647461, 9.508508682250977]\n",
            "Target weights: [0.3384832739830017, 0.3465910255908966, 0.31492576003074646]\n",
            "Updated weights: [0.3685007095336914, 0.3117312788963318, 0.3197680115699768]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 76\n",
            "  Loss for client 0: 1.7622\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 76\n",
            "  Loss for client 1: 2.5474\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 76\n",
            "  Loss for client 2: 1.7952\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 77\n",
            "  Client 0: total grad norm = 66.533330 (253 params)\n",
            "  Client 1: total grad norm = 82.538142 (253 params)\n",
            "  Client 2: total grad norm = 65.527920 (253 params)\n",
            "\n",
            "=== Optimizer Step 77 ===\n",
            "\n",
            "=== Updating Client Weights (Step 77) ===\n",
            "Gradient norms: [7.719571590423584, 8.36559009552002, 8.581202507019043]\n",
            "Target weights: [0.3543124496936798, 0.3269512951374054, 0.3187362551689148]\n",
            "Updated weights: [0.3642442226409912, 0.3162972927093506, 0.3194584846496582]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 77\n",
            "  Loss for client 0: 2.4155\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 77\n",
            "  Loss for client 1: 2.0163\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 77\n",
            "  Loss for client 2: 2.3712\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 78\n",
            "  Client 0: total grad norm = 85.575579 (253 params)\n",
            "  Client 1: total grad norm = 70.001327 (253 params)\n",
            "  Client 2: total grad norm = 91.154123 (253 params)\n",
            "\n",
            "=== Optimizer Step 78 ===\n",
            "\n",
            "=== Updating Client Weights (Step 78) ===\n",
            "Gradient norms: [9.158940315246582, 7.157399654388428, 11.099691390991211]\n",
            "Target weights: [0.3220820426940918, 0.41215112805366516, 0.26576685905456543]\n",
            "Updated weights: [0.35159555077552795, 0.34505343437194824, 0.3033509850502014]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 78\n",
            "  Loss for client 0: 2.1244\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 78\n",
            "  Loss for client 1: 1.5515\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 78\n",
            "  Loss for client 2: 2.3966\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 79\n",
            "  Client 0: total grad norm = 75.233379 (253 params)\n",
            "  Client 1: total grad norm = 63.328717 (253 params)\n",
            "  Client 2: total grad norm = 81.287649 (253 params)\n",
            "\n",
            "=== Optimizer Step 79 ===\n",
            "\n",
            "=== Updating Client Weights (Step 79) ===\n",
            "Gradient norms: [8.14745807647705, 6.943280220031738, 8.304960250854492]\n",
            "Target weights: [0.3170108497142792, 0.37199029326438904, 0.310998797416687]\n",
            "Updated weights: [0.3412201404571533, 0.35313451290130615, 0.3056453466415405]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 79\n",
            "  Loss for client 0: 1.7193\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 79\n",
            "  Loss for client 1: 2.5275\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 79\n",
            "  Loss for client 2: 2.5700\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 80\n",
            "  Client 0: total grad norm = 67.200641 (253 params)\n",
            "  Client 1: total grad norm = 83.515060 (253 params)\n",
            "  Client 2: total grad norm = 80.762371 (253 params)\n",
            "\n",
            "=== Optimizer Step 80 ===\n",
            "\n",
            "=== Updating Client Weights (Step 80) ===\n",
            "Gradient norms: [7.9371185302734375, 9.073285102844238, 7.993134498596191]\n",
            "Target weights: [0.34870287775993347, 0.3050379455089569, 0.34625914692878723]\n",
            "Updated weights: [0.3434649705886841, 0.33870553970336914, 0.3178294897079468]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 80\n",
            "  Loss for client 0: 1.4364\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 80\n",
            "  Loss for client 1: 1.5638\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 80\n",
            "  Loss for client 2: 2.4030\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 81\n",
            "  Client 0: total grad norm = 65.060748 (253 params)\n",
            "  Client 1: total grad norm = 67.881333 (253 params)\n",
            "  Client 2: total grad norm = 104.824342 (253 params)\n",
            "\n",
            "=== Optimizer Step 81 ===\n",
            "\n",
            "=== Updating Client Weights (Step 81) ===\n",
            "Gradient norms: [8.14748477935791, 7.1311726570129395, 9.42379093170166]\n",
            "Target weights: [0.33254826068878174, 0.3799419701099396, 0.2875097393989563]\n",
            "Updated weights: [0.34018996357917786, 0.35107648372650146, 0.30873358249664307]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 81\n",
            "  Loss for client 0: 1.2739\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 81\n",
            "  Loss for client 1: 2.0762\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 81\n",
            "  Loss for client 2: 2.5247\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 82\n",
            "  Client 0: total grad norm = 61.141642 (253 params)\n",
            "  Client 1: total grad norm = 87.053602 (253 params)\n",
            "  Client 2: total grad norm = 76.287540 (253 params)\n",
            "\n",
            "=== Optimizer Step 82 ===\n",
            "\n",
            "=== Updating Client Weights (Step 82) ===\n",
            "Gradient norms: [6.493854522705078, 9.144949913024902, 8.289299011230469]\n",
            "Target weights: [0.4010419249534607, 0.28478100895881653, 0.31417709589004517]\n",
            "Updated weights: [0.35844555497169495, 0.3311878442764282, 0.3103666305541992]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 82\n",
            "  Loss for client 0: 1.2925\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 82\n",
            "  Loss for client 1: 2.4976\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 82\n",
            "  Loss for client 2: 2.3024\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 83\n",
            "  Client 0: total grad norm = 53.637608 (253 params)\n",
            "  Client 1: total grad norm = 89.181283 (253 params)\n",
            "  Client 2: total grad norm = 106.502939 (253 params)\n",
            "\n",
            "=== Optimizer Step 83 ===\n",
            "\n",
            "=== Updating Client Weights (Step 83) ===\n",
            "Gradient norms: [6.309456825256348, 10.67361068725586, 10.540669441223145]\n",
            "Target weights: [0.45668166875839233, 0.2699567675590515, 0.2733615040779114]\n",
            "Updated weights: [0.3879163861274719, 0.3128185272216797, 0.2992650866508484]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 83\n",
            "  Loss for client 0: 2.5856\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 83\n",
            "  Loss for client 1: 2.0678\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 83\n",
            "  Loss for client 2: 2.7052\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 84\n",
            "  Client 0: total grad norm = 95.648812 (253 params)\n",
            "  Client 1: total grad norm = 81.986391 (253 params)\n",
            "  Client 2: total grad norm = 78.834857 (253 params)\n",
            "\n",
            "=== Optimizer Step 84 ===\n",
            "\n",
            "=== Updating Client Weights (Step 84) ===\n",
            "Gradient norms: [9.35616683959961, 9.630882263183594, 10.3195161819458]\n",
            "Target weights: [0.3474486470222473, 0.33753785490989685, 0.3150135576725006]\n",
            "Updated weights: [0.375776082277298, 0.3202343285083771, 0.3039896488189697]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 84\n",
            "  Loss for client 0: 1.8249\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 84\n",
            "  Loss for client 1: 2.1647\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 84\n",
            "  Loss for client 2: 1.9952\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 85\n",
            "  Client 0: total grad norm = 71.827871 (253 params)\n",
            "  Client 1: total grad norm = 70.593095 (253 params)\n",
            "  Client 2: total grad norm = 79.924800 (253 params)\n",
            "\n",
            "=== Optimizer Step 85 ===\n",
            "\n",
            "=== Updating Client Weights (Step 85) ===\n",
            "Gradient norms: [7.968288421630859, 7.706079959869385, 9.359039306640625]\n",
            "Target weights: [0.3465692102909088, 0.3583616614341736, 0.2950691282749176]\n",
            "Updated weights: [0.3670140206813812, 0.3316725194454193, 0.30131348967552185]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 85\n",
            "  Loss for client 0: 1.5999\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 85\n",
            "  Loss for client 1: 2.1670\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 85\n",
            "  Loss for client 2: 2.7645\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 86\n",
            "  Client 0: total grad norm = 91.807096 (253 params)\n",
            "  Client 1: total grad norm = 74.315675 (253 params)\n",
            "  Client 2: total grad norm = 82.849737 (253 params)\n",
            "\n",
            "=== Optimizer Step 86 ===\n",
            "\n",
            "=== Updating Client Weights (Step 86) ===\n",
            "Gradient norms: [9.901708602905273, 8.515052795410156, 9.763021469116211]\n",
            "Target weights: [0.3147571086883545, 0.3660145401954651, 0.31922832131385803]\n",
            "Updated weights: [0.3513369560241699, 0.3419751226902008, 0.30668795108795166]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 86\n",
            "  Loss for client 0: 2.0671\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 86\n",
            "  Loss for client 1: 2.1900\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 86\n",
            "  Loss for client 2: 2.0662\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 87\n",
            "  Client 0: total grad norm = 77.142541 (253 params)\n",
            "  Client 1: total grad norm = 75.272933 (253 params)\n",
            "  Client 2: total grad norm = 86.018101 (253 params)\n",
            "\n",
            "=== Optimizer Step 87 ===\n",
            "\n",
            "=== Updating Client Weights (Step 87) ===\n",
            "Gradient norms: [8.301580429077148, 8.553481101989746, 10.081775665283203]\n",
            "Target weights: [0.3579131066799164, 0.34737253189086914, 0.29471439123153687]\n",
            "Updated weights: [0.3533098101615906, 0.34359434247016907, 0.30309587717056274]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 87\n",
            "  Loss for client 0: 1.9462\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 87\n",
            "  Loss for client 1: 2.0615\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 87\n",
            "  Loss for client 2: 1.7814\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 88\n",
            "  Client 0: total grad norm = 103.978047 (253 params)\n",
            "  Client 1: total grad norm = 74.100814 (253 params)\n",
            "  Client 2: total grad norm = 64.228817 (253 params)\n",
            "\n",
            "=== Optimizer Step 88 ===\n",
            "\n",
            "=== Updating Client Weights (Step 88) ===\n",
            "Gradient norms: [12.875097274780273, 7.798985481262207, 7.114212989807129]\n",
            "Target weights: [0.2241831123828888, 0.3700968027114868, 0.4057201147079468]\n",
            "Updated weights: [0.3145717978477478, 0.3515450954437256, 0.3338831663131714]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 88\n",
            "  Loss for client 0: 2.3094\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 88\n",
            "  Loss for client 1: 1.9169\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 88\n",
            "  Loss for client 2: 2.5022\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 89\n",
            "  Client 0: total grad norm = 89.901156 (253 params)\n",
            "  Client 1: total grad norm = 79.313645 (253 params)\n",
            "  Client 2: total grad norm = 96.515896 (253 params)\n",
            "\n",
            "=== Optimizer Step 89 ===\n",
            "\n",
            "=== Updating Client Weights (Step 89) ===\n",
            "Gradient norms: [10.725830078125, 10.005579948425293, 12.533531188964844]\n",
            "Target weights: [0.341558575630188, 0.36614561080932617, 0.2922958433628082]\n",
            "Updated weights: [0.32266783714294434, 0.3559252619743347, 0.3214069604873657]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 89\n",
            "  Loss for client 0: 2.3602\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 89\n",
            "  Loss for client 1: 1.5898\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 89\n",
            "  Loss for client 2: 2.4488\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 90\n",
            "  Client 0: total grad norm = 87.669804 (253 params)\n",
            "  Client 1: total grad norm = 72.580974 (253 params)\n",
            "  Client 2: total grad norm = 89.035592 (253 params)\n",
            "\n",
            "=== Optimizer Step 90 ===\n",
            "\n",
            "=== Updating Client Weights (Step 90) ===\n",
            "Gradient norms: [9.822994232177734, 7.365731239318848, 11.973355293273926]\n",
            "Target weights: [0.3170565366744995, 0.42282891273498535, 0.2601146101951599]\n",
            "Updated weights: [0.32098445296287537, 0.37599635124206543, 0.303019255399704]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 90\n",
            "  Loss for client 0: 1.4182\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 90\n",
            "  Loss for client 1: 2.2426\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 90\n",
            "  Loss for client 2: 2.5040\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 91\n",
            "  Client 0: total grad norm = 68.163655 (253 params)\n",
            "  Client 1: total grad norm = 70.639714 (253 params)\n",
            "  Client 2: total grad norm = 87.076259 (253 params)\n",
            "\n",
            "=== Optimizer Step 91 ===\n",
            "\n",
            "=== Updating Client Weights (Step 91) ===\n",
            "Gradient norms: [8.458476066589355, 8.53293228149414, 9.563079833984375]\n",
            "Target weights: [0.3477332890033722, 0.34469905495643616, 0.30756762623786926]\n",
            "Updated weights: [0.32900911569595337, 0.3666071891784668, 0.3043837547302246]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 91\n",
            "  Loss for client 0: 2.0688\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 91\n",
            "  Loss for client 1: 2.5351\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 91\n",
            "  Loss for client 2: 1.5790\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 92\n",
            "  Client 0: total grad norm = 81.647121 (253 params)\n",
            "  Client 1: total grad norm = 75.004197 (253 params)\n",
            "  Client 2: total grad norm = 69.477198 (253 params)\n",
            "\n",
            "=== Optimizer Step 92 ===\n",
            "\n",
            "=== Updating Client Weights (Step 92) ===\n",
            "Gradient norms: [9.205984115600586, 8.243513107299805, 7.555546283721924]\n",
            "Target weights: [0.2998324930667877, 0.33483943343162537, 0.3653280436992645]\n",
            "Updated weights: [0.3202561140060425, 0.35707685351371765, 0.32266703248023987]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 92\n",
            "  Loss for client 0: 1.2904\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 92\n",
            "  Loss for client 1: 2.2986\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 92\n",
            "  Loss for client 2: 2.3863\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 93\n",
            "  Client 0: total grad norm = 65.858191 (253 params)\n",
            "  Client 1: total grad norm = 81.687544 (253 params)\n",
            "  Client 2: total grad norm = 130.320973 (253 params)\n",
            "\n",
            "=== Optimizer Step 93 ===\n",
            "\n",
            "=== Updating Client Weights (Step 93) ===\n",
            "Gradient norms: [8.13416862487793, 8.671686172485352, 15.077320098876953]\n",
            "Target weights: [0.403630793094635, 0.37861159443855286, 0.21775759756565094]\n",
            "Updated weights: [0.34526851773262024, 0.3635372817516327, 0.29119420051574707]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 93\n",
            "  Loss for client 0: 2.0075\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 93\n",
            "  Loss for client 1: 2.2599\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 93\n",
            "  Loss for client 2: 2.3552\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 94\n",
            "  Client 0: total grad norm = 95.416491 (253 params)\n",
            "  Client 1: total grad norm = 85.817329 (253 params)\n",
            "  Client 2: total grad norm = 84.754312 (253 params)\n",
            "\n",
            "=== Optimizer Step 94 ===\n",
            "\n",
            "=== Updating Client Weights (Step 94) ===\n",
            "Gradient norms: [11.429116249084473, 11.23708724975586, 11.740996360778809]\n",
            "Target weights: [0.3343893885612488, 0.34010374546051025, 0.32550692558288574]\n",
            "Updated weights: [0.34200477600097656, 0.35650724172592163, 0.3014880120754242]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 94\n",
            "  Loss for client 0: 2.0001\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 94\n",
            "  Loss for client 1: 2.0765\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 94\n",
            "  Loss for client 2: 2.5759\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 95\n",
            "  Client 0: total grad norm = 75.055275 (253 params)\n",
            "  Client 1: total grad norm = 81.249707 (253 params)\n",
            "  Client 2: total grad norm = 98.602124 (253 params)\n",
            "\n",
            "=== Optimizer Step 95 ===\n",
            "\n",
            "=== Updating Client Weights (Step 95) ===\n",
            "Gradient norms: [8.643705368041992, 9.958155632019043, 11.397953987121582]\n",
            "Target weights: [0.38075533509254456, 0.3304966390132904, 0.2887480556964874]\n",
            "Updated weights: [0.3536299467086792, 0.348704069852829, 0.2976660132408142]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 95\n",
            "  Loss for client 0: 2.0376\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 95\n",
            "  Loss for client 1: 1.8372\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 95\n",
            "  Loss for client 2: 2.4478\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 96\n",
            "  Client 0: total grad norm = 100.645494 (253 params)\n",
            "  Client 1: total grad norm = 72.717010 (253 params)\n",
            "  Client 2: total grad norm = 77.329794 (253 params)\n",
            "\n",
            "=== Optimizer Step 96 ===\n",
            "\n",
            "=== Updating Client Weights (Step 96) ===\n",
            "Gradient norms: [12.737247467041016, 9.205659866333008, 9.41636848449707]\n",
            "Target weights: [0.2676442563533783, 0.37032121419906616, 0.3620345890522003]\n",
            "Updated weights: [0.32783424854278564, 0.3551892042160034, 0.3169765770435333]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 96\n",
            "  Loss for client 0: 1.4581\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 96\n",
            "  Loss for client 1: 2.5438\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 96\n",
            "  Loss for client 2: 1.7899\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 97\n",
            "  Client 0: total grad norm = 83.255061 (253 params)\n",
            "  Client 1: total grad norm = 86.885377 (253 params)\n",
            "  Client 2: total grad norm = 71.325397 (253 params)\n",
            "\n",
            "=== Optimizer Step 97 ===\n",
            "\n",
            "=== Updating Client Weights (Step 97) ===\n",
            "Gradient norms: [9.012625694274902, 9.47819995880127, 7.706748008728027]\n",
            "Target weights: [0.3204793930053711, 0.30473726987838745, 0.37478336691856384]\n",
            "Updated weights: [0.32562780380249023, 0.34005361795425415, 0.334318608045578]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 97\n",
            "  Loss for client 0: 1.4503\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 97\n",
            "  Loss for client 1: 2.0628\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 97\n",
            "  Loss for client 2: 2.4512\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 98\n",
            "  Client 0: total grad norm = 65.301712 (253 params)\n",
            "  Client 1: total grad norm = 83.825789 (253 params)\n",
            "  Client 2: total grad norm = 87.286355 (253 params)\n",
            "\n",
            "=== Optimizer Step 98 ===\n",
            "\n",
            "=== Updating Client Weights (Step 98) ===\n",
            "Gradient norms: [7.707313060760498, 10.387248039245605, 12.892053604125977]\n",
            "Target weights: [0.42738109827041626, 0.31711575388908386, 0.2555031180381775]\n",
            "Updated weights: [0.35615378618240356, 0.3331722617149353, 0.31067395210266113]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 98\n",
            "  Loss for client 0: 2.0776\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 98\n",
            "  Loss for client 1: 1.9791\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 98\n",
            "  Loss for client 2: 2.3528\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 99\n",
            "  Client 0: total grad norm = 88.995169 (253 params)\n",
            "  Client 1: total grad norm = 96.044168 (253 params)\n",
            "  Client 2: total grad norm = 92.470956 (253 params)\n",
            "\n",
            "=== Optimizer Step 99 ===\n",
            "\n",
            "=== Updating Client Weights (Step 99) ===\n",
            "Gradient norms: [10.374502182006836, 9.441313743591309, 10.251169204711914]\n",
            "Target weights: [0.3214532732963562, 0.35322603583335876, 0.3253207206726074]\n",
            "Updated weights: [0.3457436263561249, 0.3391883969306946, 0.31506797671318054]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 99\n",
            "  Loss for client 0: 1.8750\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 99\n",
            "  Loss for client 1: 2.3623\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 99\n",
            "  Loss for client 2: 1.9636\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 100\n",
            "  Client 0: total grad norm = 76.263257 (253 params)\n",
            "  Client 1: total grad norm = 79.136175 (253 params)\n",
            "  Client 2: total grad norm = 123.491635 (253 params)\n",
            "\n",
            "=== Optimizer Step 100 ===\n",
            "\n",
            "=== Updating Client Weights (Step 100) ===\n",
            "Gradient norms: [8.852987289428711, 9.24306869506836, 10.401871681213379]\n",
            "Target weights: [0.35601213574409485, 0.34098750352859497, 0.3030003607273102]\n",
            "Updated weights: [0.3488241732120514, 0.33972811698913574, 0.3114476799964905]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 100\n",
            "  Val Loss = 1.8744 (5 batches)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62517]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ –æ–±—Ä–µ–∑–∞–Ω—ã –¥–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç—Ä–æ–∫ (5000).\u001b[0m\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 301\n",
            "  Loss for client 1: 1.8004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 301\n",
            "  Loss for client 2: 1.7311\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 302\n",
            "  Client 0: total grad norm = 71.450495 (253 params)\n",
            "  Client 1: total grad norm = 81.558299 (253 params)\n",
            "  Client 2: total grad norm = 79.692411 (253 params)\n",
            "\n",
            "=== Optimizer Step 302 ===\n",
            "\n",
            "=== Updating Client Weights (Step 302) ===\n",
            "Gradient norms: [7.924553394317627, 8.768647193908691, 8.222269058227539]\n",
            "Target weights: [0.3487323224544525, 0.31516239047050476, 0.33610525727272034]\n",
            "Updated weights: [0.3343738615512848, 0.3296582102775574, 0.33596792817115784]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 302\n",
            "  Loss for client 0: 1.1511\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 302\n",
            "  Loss for client 1: 1.9491\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 302\n",
            "  Loss for client 2: 1.7083\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 303\n",
            "  Client 0: total grad norm = 69.081250 (253 params)\n",
            "  Client 1: total grad norm = 87.578328 (253 params)\n",
            "  Client 2: total grad norm = 82.739715 (253 params)\n",
            "\n",
            "=== Optimizer Step 303 ===\n",
            "\n",
            "=== Updating Client Weights (Step 303) ===\n",
            "Gradient norms: [8.00510311126709, 9.0433349609375, 8.338691711425781]\n",
            "Target weights: [0.3514705002307892, 0.3111194968223572, 0.33740997314453125]\n",
            "Updated weights: [0.33950287103652954, 0.32409659028053284, 0.3364005386829376]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 303\n",
            "  Loss for client 0: 1.7428\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 303\n",
            "  Loss for client 1: 2.0502\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 303\n",
            "  Loss for client 2: 1.8307\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 304\n",
            "  Client 0: total grad norm = 70.994072 (253 params)\n",
            "  Client 1: total grad norm = 78.202033 (253 params)\n",
            "  Client 2: total grad norm = 79.616517 (253 params)\n",
            "\n",
            "=== Optimizer Step 304 ===\n",
            "\n",
            "=== Updating Client Weights (Step 304) ===\n",
            "Gradient norms: [7.700445652008057, 9.693037986755371, 8.277631759643555]\n",
            "Target weights: [0.3670125901699066, 0.29156601428985596, 0.3414213955402374]\n",
            "Updated weights: [0.3477557897567749, 0.31433743238449097, 0.33790677785873413]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 304\n",
            "  Loss for client 0: 1.1673\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 304\n",
            "  Loss for client 1: 1.9299\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 304\n",
            "  Loss for client 2: 0.8931\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 305\n",
            "  Client 0: total grad norm = 55.945505 (253 params)\n",
            "  Client 1: total grad norm = 80.904957 (253 params)\n",
            "  Client 2: total grad norm = 52.860809 (253 params)\n",
            "\n",
            "=== Optimizer Step 305 ===\n",
            "\n",
            "=== Updating Client Weights (Step 305) ===\n",
            "Gradient norms: [6.138987064361572, 9.333300590515137, 6.837260723114014]\n",
            "Target weights: [0.391294002532959, 0.25737398862838745, 0.35133203864097595]\n",
            "Updated weights: [0.3608172535896301, 0.29724839329719543, 0.34193435311317444]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 305\n",
            "  Loss for client 0: 2.0933\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 305\n",
            "  Loss for client 1: 1.5581\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 305\n",
            "  Loss for client 2: 1.8350\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 306\n",
            "  Client 0: total grad norm = 102.156631 (253 params)\n",
            "  Client 1: total grad norm = 62.787706 (253 params)\n",
            "  Client 2: total grad norm = 83.911899 (253 params)\n",
            "\n",
            "=== Optimizer Step 306 ===\n",
            "\n",
            "=== Updating Client Weights (Step 306) ===\n",
            "Gradient norms: [11.564518928527832, 6.57413911819458, 9.799144744873047]\n",
            "Target weights: [0.25385552644729614, 0.44655534625053406, 0.2995890974998474]\n",
            "Updated weights: [0.32872873544692993, 0.342040479183197, 0.32923078536987305]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 306\n",
            "  Loss for client 0: 1.7710\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 306\n",
            "  Loss for client 1: 1.5441\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 306\n",
            "  Loss for client 2: 2.3319\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 307\n",
            "  Client 0: total grad norm = 79.513480 (253 params)\n",
            "  Client 1: total grad norm = 72.300991 (253 params)\n",
            "  Client 2: total grad norm = 90.937888 (253 params)\n",
            "\n",
            "=== Optimizer Step 307 ===\n",
            "\n",
            "=== Updating Client Weights (Step 307) ===\n",
            "Gradient norms: [8.344094276428223, 8.32505989074707, 9.97469425201416]\n",
            "Target weights: [0.35225996375083923, 0.3530653715133667, 0.29467472434043884]\n",
            "Updated weights: [0.3357881009578705, 0.34534794092178345, 0.31886398792266846]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 307\n",
            "  Loss for client 0: 1.2195\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 307\n",
            "  Loss for client 1: 2.1856\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 307\n",
            "  Loss for client 2: 2.2378\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 308\n",
            "  Client 0: total grad norm = 69.590934 (253 params)\n",
            "  Client 1: total grad norm = 92.640659 (253 params)\n",
            "  Client 2: total grad norm = 95.902212 (253 params)\n",
            "\n",
            "=== Optimizer Step 308 ===\n",
            "\n",
            "=== Updating Client Weights (Step 308) ===\n",
            "Gradient norms: [7.87617826461792, 9.267434120178223, 10.295336723327637]\n",
            "Target weights: [0.3824236989021301, 0.32501307129859924, 0.292563259601593]\n",
            "Updated weights: [0.34977877140045166, 0.339247465133667, 0.31097376346588135]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 308\n",
            "  Loss for client 0: 1.4470\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 308\n",
            "  Loss for client 1: 1.3792\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 308\n",
            "  Loss for client 2: 2.3294\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 309\n",
            "  Client 0: total grad norm = 85.529246 (253 params)\n",
            "  Client 1: total grad norm = 75.598077 (253 params)\n",
            "  Client 2: total grad norm = 80.499571 (253 params)\n",
            "\n",
            "=== Optimizer Step 309 ===\n",
            "\n",
            "=== Updating Client Weights (Step 309) ===\n",
            "Gradient norms: [9.065505027770996, 7.323015213012695, 8.528414726257324]\n",
            "Target weights: [0.3029455244541168, 0.375030517578125, 0.32202398777008057]\n",
            "Updated weights: [0.33572879433631897, 0.3499823808670044, 0.31428882479667664]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 309\n",
            "  Loss for client 0: 1.8603\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 309\n",
            "  Loss for client 1: 1.7317\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 309\n",
            "  Loss for client 2: 2.1293\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 310\n",
            "  Client 0: total grad norm = 86.216761 (253 params)\n",
            "  Client 1: total grad norm = 94.902443 (253 params)\n",
            "  Client 2: total grad norm = 76.999514 (253 params)\n",
            "\n",
            "=== Optimizer Step 310 ===\n",
            "\n",
            "=== Updating Client Weights (Step 310) ===\n",
            "Gradient norms: [10.210103988647461, 10.955229759216309, 8.895970344543457]\n",
            "Target weights: [0.3247062563896179, 0.30262115597724915, 0.3726726174354553]\n",
            "Updated weights: [0.33242201805114746, 0.3357740044593811, 0.33180397748947144]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 310\n",
            "  Loss for client 0: 1.1188\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 310\n",
            "  Loss for client 1: 1.6599\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 310\n",
            "  Loss for client 2: 2.1062\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 311\n",
            "  Client 0: total grad norm = 76.125927 (253 params)\n",
            "  Client 1: total grad norm = 73.968458 (253 params)\n",
            "  Client 2: total grad norm = 88.033147 (253 params)\n",
            "\n",
            "=== Optimizer Step 311 ===\n",
            "\n",
            "=== Updating Client Weights (Step 311) ===\n",
            "Gradient norms: [8.022039413452148, 8.578465461730957, 9.902780532836914]\n",
            "Target weights: [0.36427003145217896, 0.3406423330307007, 0.29508766531944275]\n",
            "Updated weights: [0.34197643399238586, 0.3372344970703125, 0.32078906893730164]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 311\n",
            "  Loss for client 0: 1.7717\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 311\n",
            "  Loss for client 1: 1.6733\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 311\n",
            "  Loss for client 2: 1.3001\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 312\n",
            "  Client 0: total grad norm = 72.141248 (253 params)\n",
            "  Client 1: total grad norm = 84.695933 (253 params)\n",
            "  Client 2: total grad norm = 61.213078 (253 params)\n",
            "\n",
            "=== Optimizer Step 312 ===\n",
            "\n",
            "=== Updating Client Weights (Step 312) ===\n",
            "Gradient norms: [8.61721134185791, 9.53918170928955, 7.417435169219971]\n",
            "Target weights: [0.32625362277030945, 0.29472091794013977, 0.3790254294872284]\n",
            "Updated weights: [0.33725959062576294, 0.32448041439056396, 0.3382599949836731]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 312\n",
            "  Loss for client 0: 1.5646\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 312\n",
            "  Loss for client 1: 1.5741\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 312\n",
            "  Loss for client 2: 1.9896\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 313\n",
            "  Client 0: total grad norm = 76.290936 (253 params)\n",
            "  Client 1: total grad norm = 59.967242 (253 params)\n",
            "  Client 2: total grad norm = 79.603020 (253 params)\n",
            "\n",
            "=== Optimizer Step 313 ===\n",
            "\n",
            "=== Updating Client Weights (Step 313) ===\n",
            "Gradient norms: [8.59988021850586, 7.104909420013428, 9.44466495513916]\n",
            "Target weights: [0.32041341066360474, 0.38783279061317444, 0.2917537987232208]\n",
            "Updated weights: [0.33220574259757996, 0.34348613023757935, 0.3243081569671631]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 313\n",
            "  Loss for client 0: 2.0763\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 313\n",
            "  Loss for client 1: 1.1591\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 313\n",
            "  Loss for client 2: 1.1217\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 314\n",
            "  Client 0: total grad norm = 146.066292 (253 params)\n",
            "  Client 1: total grad norm = 66.198948 (253 params)\n",
            "  Client 2: total grad norm = 55.023201 (253 params)\n",
            "\n",
            "=== Optimizer Step 314 ===\n",
            "\n",
            "=== Updating Client Weights (Step 314) ===\n",
            "Gradient norms: [14.498183250427246, 6.94966459274292, 7.176070213317871]\n",
            "Target weights: [0.19582802057266235, 0.408530592918396, 0.39564141631126404]\n",
            "Updated weights: [0.2912924289703369, 0.36299946904182434, 0.34570813179016113]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 314\n",
            "  Loss for client 0: 1.2782\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 314\n",
            "  Loss for client 1: 2.1674\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 314\n",
            "  Loss for client 2: 1.0022\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 315\n",
            "  Client 0: total grad norm = 63.554975 (253 params)\n",
            "  Client 1: total grad norm = 98.246314 (253 params)\n",
            "  Client 2: total grad norm = 59.122346 (253 params)\n",
            "\n",
            "=== Optimizer Step 315 ===\n",
            "\n",
            "=== Updating Client Weights (Step 315) ===\n",
            "Gradient norms: [7.746467590332031, 9.653538703918457, 5.954604148864746]\n",
            "Target weights: [0.3222304582595825, 0.25857335329055786, 0.4191962778568268]\n",
            "Updated weights: [0.30057385563850403, 0.33167165517807007, 0.36775457859039307]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 315\n",
            "  Loss for client 0: 2.0120\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 315\n",
            "  Loss for client 1: 1.8037\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 315\n",
            "  Loss for client 2: 1.7892\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 316\n",
            "  Client 0: total grad norm = 89.534055 (253 params)\n",
            "  Client 1: total grad norm = 108.511127 (253 params)\n",
            "  Client 2: total grad norm = 79.113147 (253 params)\n",
            "\n",
            "=== Optimizer Step 316 ===\n",
            "\n",
            "=== Updating Client Weights (Step 316) ===\n",
            "Gradient norms: [9.042064666748047, 10.389830589294434, 9.2144136428833]\n",
            "Target weights: [0.3506832420825958, 0.3051927387714386, 0.3441239595413208]\n",
            "Updated weights: [0.3156066834926605, 0.32372796535491943, 0.36066538095474243]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 316\n",
            "  Loss for client 0: 1.3599\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 316\n",
            "  Loss for client 1: 2.5531\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 316\n",
            "  Loss for client 2: 1.9880\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 317\n",
            "  Client 0: total grad norm = 75.761930 (253 params)\n",
            "  Client 1: total grad norm = 86.673127 (253 params)\n",
            "  Client 2: total grad norm = 83.250578 (253 params)\n",
            "\n",
            "=== Optimizer Step 317 ===\n",
            "\n",
            "=== Updating Client Weights (Step 317) ===\n",
            "Gradient norms: [7.3698649406433105, 9.204619407653809, 10.722490310668945]\n",
            "Target weights: [0.4019296169281006, 0.32181307673454285, 0.27625736594200134]\n",
            "Updated weights: [0.3415035605430603, 0.3231534957885742, 0.33534297347068787]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 317\n",
            "  Loss for client 0: 1.6113\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 317\n",
            "  Loss for client 1: 2.3377\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 317\n",
            "  Loss for client 2: 1.5871\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 318\n",
            "  Client 0: total grad norm = 79.618089 (253 params)\n",
            "  Client 1: total grad norm = 81.262077 (253 params)\n",
            "  Client 2: total grad norm = 83.231049 (253 params)\n",
            "\n",
            "=== Optimizer Step 318 ===\n",
            "\n",
            "=== Updating Client Weights (Step 318) ===\n",
            "Gradient norms: [8.060513496398926, 8.774395942687988, 8.23151683807373]\n",
            "Target weights: [0.34508153796195984, 0.31700578331947327, 0.33791273832321167]\n",
            "Updated weights: [0.3425769507884979, 0.3213091790676117, 0.33611389994621277]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 318\n",
            "  Loss for client 0: 0.8600\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 318\n",
            "  Loss for client 1: 1.8425\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 318\n",
            "  Loss for client 2: 1.5260\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 319\n",
            "  Client 0: total grad norm = 57.372528 (253 params)\n",
            "  Client 1: total grad norm = 73.413178 (253 params)\n",
            "  Client 2: total grad norm = 64.393562 (253 params)\n",
            "\n",
            "=== Optimizer Step 319 ===\n",
            "\n",
            "=== Updating Client Weights (Step 319) ===\n",
            "Gradient norms: [7.379461288452148, 9.45669174194336, 7.1947021484375]\n",
            "Target weights: [0.3563763201236725, 0.27809569239616394, 0.36552801728248596]\n",
            "Updated weights: [0.3467167615890503, 0.30834513902664185, 0.34493812918663025]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 319\n",
            "  Loss for client 0: 1.7456\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 319\n",
            "  Loss for client 1: 1.9743\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 319\n",
            "  Loss for client 2: 2.0104\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 320\n",
            "  Client 0: total grad norm = 84.071787 (253 params)\n",
            "  Client 1: total grad norm = 78.060487 (253 params)\n",
            "  Client 2: total grad norm = 90.096481 (253 params)\n",
            "\n",
            "=== Optimizer Step 320 ===\n",
            "\n",
            "=== Updating Client Weights (Step 320) ===\n",
            "Gradient norms: [10.03636360168457, 8.76362133026123, 8.994292259216309]\n",
            "Target weights: [0.3066459894180298, 0.35118022561073303, 0.3421737551689148]\n",
            "Updated weights: [0.3346955180168152, 0.32119566202163696, 0.34410881996154785]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 320\n",
            "  Loss for client 0: 1.2765\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 320\n",
            "  Loss for client 1: 2.0765\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 320\n",
            "  Loss for client 2: 1.8236\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 321\n",
            "  Client 0: total grad norm = 67.837380 (253 params)\n",
            "  Client 1: total grad norm = 82.657967 (253 params)\n",
            "  Client 2: total grad norm = 76.038516 (253 params)\n",
            "\n",
            "=== Optimizer Step 321 ===\n",
            "\n",
            "=== Updating Client Weights (Step 321) ===\n",
            "Gradient norms: [8.763622283935547, 9.40117359161377, 8.639323234558105]\n",
            "Target weights: [0.3393774926662445, 0.316362202167511, 0.3442603051662445]\n",
            "Updated weights: [0.33610010147094727, 0.31974563002586365, 0.3441542685031891]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 321\n",
            "  Loss for client 0: 1.1278\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 321\n",
            "  Loss for client 1: 1.7117\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 321\n",
            "  Loss for client 2: 1.9160\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 322\n",
            "  Client 0: total grad norm = 79.766945 (253 params)\n",
            "  Client 1: total grad norm = 69.090953 (253 params)\n",
            "  Client 2: total grad norm = 76.961353 (253 params)\n",
            "\n",
            "=== Optimizer Step 322 ===\n",
            "\n",
            "=== Updating Client Weights (Step 322) ===\n",
            "Gradient norms: [9.312689781188965, 7.1939191818237305, 8.761140823364258]\n",
            "Target weights: [0.29784274101257324, 0.3855641186237335, 0.31659314036369324]\n",
            "Updated weights: [0.32462289929389954, 0.33949118852615356, 0.3358859419822693]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 322\n",
            "  Loss for client 0: 1.3171\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 322\n",
            "  Loss for client 1: 1.6337\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 322\n",
            "  Loss for client 2: 1.4576\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 323\n",
            "  Client 0: total grad norm = 66.435551 (253 params)\n",
            "  Client 1: total grad norm = 71.836580 (253 params)\n",
            "  Client 2: total grad norm = 61.723376 (253 params)\n",
            "\n",
            "=== Optimizer Step 323 ===\n",
            "\n",
            "=== Updating Client Weights (Step 323) ===\n",
            "Gradient norms: [8.289070129394531, 7.5046491622924805, 7.00727653503418]\n",
            "Target weights: [0.3041873276233673, 0.33598241209983826, 0.35983026027679443]\n",
            "Updated weights: [0.31849223375320435, 0.3384385406970978, 0.34306925535202026]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 323\n",
            "  Loss for client 0: 1.8969\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 323\n",
            "  Loss for client 1: 1.6265\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 323\n",
            "  Loss for client 2: 1.8574\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 324\n",
            "  Client 0: total grad norm = 77.367452 (253 params)\n",
            "  Client 1: total grad norm = 80.036179 (253 params)\n",
            "  Client 2: total grad norm = 79.490583 (253 params)\n",
            "\n",
            "=== Optimizer Step 324 ===\n",
            "\n",
            "=== Updating Client Weights (Step 324) ===\n",
            "Gradient norms: [8.812087059020996, 8.340714454650879, 8.153958320617676]\n",
            "Target weights: [0.3187527656555176, 0.3367669880390167, 0.3444801867008209]\n",
            "Updated weights: [0.3185703754425049, 0.3379370868206024, 0.3434925377368927]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 324\n",
            "  Loss for client 0: 1.8534\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 324\n",
            "  Loss for client 1: 1.7884\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 324\n",
            "  Loss for client 2: 1.6411\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 325\n",
            "  Client 0: total grad norm = 87.468199 (253 params)\n",
            "  Client 1: total grad norm = 75.741044 (253 params)\n",
            "  Client 2: total grad norm = 81.079068 (253 params)\n",
            "\n",
            "=== Optimizer Step 325 ===\n",
            "\n",
            "=== Updating Client Weights (Step 325) ===\n",
            "Gradient norms: [10.336071014404297, 9.328532218933105, 9.178487777709961]\n",
            "Target weights: [0.3092026114463806, 0.34259840846061707, 0.3481989800930023]\n",
            "Updated weights: [0.3157600462436676, 0.33933550119400024, 0.34490448236465454]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 325\n",
            "  Loss for client 0: 1.1845\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 325\n",
            "  Loss for client 1: 1.6055\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 325\n",
            "  Loss for client 2: 2.0398\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 326\n",
            "  Client 0: total grad norm = 61.103206 (253 params)\n",
            "  Client 1: total grad norm = 74.153282 (253 params)\n",
            "  Client 2: total grad norm = 81.545649 (253 params)\n",
            "\n",
            "=== Optimizer Step 326 ===\n",
            "\n",
            "=== Updating Client Weights (Step 326) ===\n",
            "Gradient norms: [6.399160385131836, 8.60748291015625, 8.599547386169434]\n",
            "Target weights: [0.4019988775253296, 0.29886266589164734, 0.29913845658302307]\n",
            "Updated weights: [0.341631680727005, 0.32719364762306213, 0.33117467164993286]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 326\n",
            "  Loss for client 0: 1.5164\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 326\n",
            "  Loss for client 1: 1.9227\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 326\n",
            "  Loss for client 2: 2.0964\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 327\n",
            "  Client 0: total grad norm = 72.434696 (253 params)\n",
            "  Client 1: total grad norm = 94.575979 (253 params)\n",
            "  Client 2: total grad norm = 83.812326 (253 params)\n",
            "\n",
            "=== Optimizer Step 327 ===\n",
            "\n",
            "=== Updating Client Weights (Step 327) ===\n",
            "Gradient norms: [8.149959564208984, 8.425970077514648, 9.331600189208984]\n",
            "Target weights: [0.3520364761352539, 0.3405047357082367, 0.3074588477611542]\n",
            "Updated weights: [0.34475311636924744, 0.331186980009079, 0.32405993342399597]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 327\n",
            "  Loss for client 0: 0.9603\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 327\n",
            "  Loss for client 1: 2.0518\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 327\n",
            "  Loss for client 2: 1.9416\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 328\n",
            "  Client 0: total grad norm = 90.688246 (253 params)\n",
            "  Client 1: total grad norm = 107.798308 (253 params)\n",
            "  Client 2: total grad norm = 88.462779 (253 params)\n",
            "\n",
            "=== Optimizer Step 328 ===\n",
            "\n",
            "=== Updating Client Weights (Step 328) ===\n",
            "Gradient norms: [9.129672050476074, 10.226771354675293, 10.368020057678223]\n",
            "Target weights: [0.3605833649635315, 0.32190099358558655, 0.3175155818462372]\n",
            "Updated weights: [0.34950220584869385, 0.3284011781215668, 0.32209664583206177]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 328\n",
            "  Loss for client 0: 1.4457\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 328\n",
            "  Loss for client 1: 1.7931\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 328\n",
            "  Loss for client 2: 1.8298\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 329\n",
            "  Client 0: total grad norm = 75.173975 (253 params)\n",
            "  Client 1: total grad norm = 94.364555 (253 params)\n",
            "  Client 2: total grad norm = 78.936833 (253 params)\n",
            "\n",
            "=== Optimizer Step 329 ===\n",
            "\n",
            "=== Updating Client Weights (Step 329) ===\n",
            "Gradient norms: [8.28917407989502, 9.855188369750977, 9.412437438964844]\n",
            "Target weights: [0.3674094080924988, 0.30902713537216187, 0.32356342673301697]\n",
            "Updated weights: [0.3548743724822998, 0.3225889801979065, 0.3225366771221161]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 329\n",
            "  Loss for client 0: 1.7526\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 329\n",
            "  Loss for client 1: 1.9173\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 329\n",
            "  Loss for client 2: 1.8403\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 330\n",
            "  Client 0: total grad norm = 83.477889 (253 params)\n",
            "  Client 1: total grad norm = 93.209972 (253 params)\n",
            "  Client 2: total grad norm = 90.618975 (253 params)\n",
            "\n",
            "=== Optimizer Step 330 ===\n",
            "\n",
            "=== Updating Client Weights (Step 330) ===\n",
            "Gradient norms: [9.341766357421875, 9.297018051147461, 10.179401397705078]\n",
            "Target weights: [0.34216976165771484, 0.34381669759750366, 0.3140135407447815]\n",
            "Updated weights: [0.35106298327445984, 0.32895728945732117, 0.319979727268219]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 330\n",
            "  Loss for client 0: 0.8574\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 330\n",
            "  Loss for client 1: 1.3579\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 330\n",
            "  Loss for client 2: 1.7511\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 331\n",
            "  Client 0: total grad norm = 60.942893 (253 params)\n",
            "  Client 1: total grad norm = 67.823460 (253 params)\n",
            "  Client 2: total grad norm = 79.839960 (253 params)\n",
            "\n",
            "=== Optimizer Step 331 ===\n",
            "\n",
            "=== Updating Client Weights (Step 331) ===\n",
            "Gradient norms: [6.403876781463623, 7.649789333343506, 7.887432098388672]\n",
            "Target weights: [0.3774952292442322, 0.31601300835609436, 0.30649179220199585]\n",
            "Updated weights: [0.35899263620376587, 0.3250740170478821, 0.31593334674835205]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 331\n",
            "  Loss for client 0: 0.9413\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 331\n",
            "  Loss for client 1: 1.5583\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 331\n",
            "  Loss for client 2: 1.4732\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 332\n",
            "  Client 0: total grad norm = 78.145580 (253 params)\n",
            "  Client 1: total grad norm = 134.033800 (253 params)\n",
            "  Client 2: total grad norm = 76.241207 (253 params)\n",
            "\n",
            "=== Optimizer Step 332 ===\n",
            "\n",
            "=== Updating Client Weights (Step 332) ===\n",
            "Gradient norms: [8.78194808959961, 14.752849578857422, 7.689837455749512]\n",
            "Target weights: [0.3653246760368347, 0.2174672782421112, 0.41720807552337646]\n",
            "Updated weights: [0.36089226603507996, 0.2927919924259186, 0.34631574153900146]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 332\n",
            "  Loss for client 0: 1.2012\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 332\n",
            "  Loss for client 1: 2.4715\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 332\n",
            "  Loss for client 2: 1.7011\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 333\n",
            "  Client 0: total grad norm = 61.862755 (253 params)\n",
            "  Client 1: total grad norm = 102.976713 (253 params)\n",
            "  Client 2: total grad norm = 91.517121 (253 params)\n",
            "\n",
            "=== Optimizer Step 333 ===\n",
            "\n",
            "=== Updating Client Weights (Step 333) ===\n",
            "Gradient norms: [7.28298807144165, 10.554155349731445, 9.131352424621582]\n",
            "Target weights: [0.4019875228404999, 0.2773950695991516, 0.3206174075603485]\n",
            "Updated weights: [0.37322086095809937, 0.2881729304790497, 0.3386062681674957]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 333\n",
            "  Loss for client 0: 1.4821\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 333\n",
            "  Loss for client 1: 2.0649\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 333\n",
            "  Loss for client 2: 1.4499\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 334\n",
            "  Client 0: total grad norm = 95.229894 (253 params)\n",
            "  Client 1: total grad norm = 89.091577 (253 params)\n",
            "  Client 2: total grad norm = 84.927380 (253 params)\n",
            "\n",
            "=== Optimizer Step 334 ===\n",
            "\n",
            "=== Updating Client Weights (Step 334) ===\n",
            "Gradient norms: [9.559198379516602, 10.582700729370117, 8.303374290466309]\n",
            "Target weights: [0.3273828327655792, 0.29572010040283203, 0.37689706683158875]\n",
            "Updated weights: [0.3594694435596466, 0.2904370427131653, 0.3500934839248657]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 334\n",
            "  Loss for client 0: 1.2812\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 334\n",
            "  Loss for client 1: 1.8219\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 334\n",
            "  Loss for client 2: 1.4053\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 335\n",
            "  Client 0: total grad norm = 67.326769 (253 params)\n",
            "  Client 1: total grad norm = 84.074081 (253 params)\n",
            "  Client 2: total grad norm = 76.399176 (253 params)\n",
            "\n",
            "=== Optimizer Step 335 ===\n",
            "\n",
            "=== Updating Client Weights (Step 335) ===\n",
            "Gradient norms: [7.981388092041016, 8.811227798461914, 8.896428108215332]\n",
            "Target weights: [0.3567650318145752, 0.3231649696826935, 0.3200700283050537]\n",
            "Updated weights: [0.3586581349372864, 0.3002554178237915, 0.3410864472389221]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 335\n",
            "  Loss for client 0: 1.2047\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 335\n",
            "  Loss for client 1: 1.7633\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 335\n",
            "  Loss for client 2: 1.6013\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 336\n",
            "  Client 0: total grad norm = 55.426344 (253 params)\n",
            "  Client 1: total grad norm = 84.701400 (253 params)\n",
            "  Client 2: total grad norm = 68.595206 (253 params)\n",
            "\n",
            "=== Optimizer Step 336 ===\n",
            "\n",
            "=== Updating Client Weights (Step 336) ===\n",
            "Gradient norms: [6.600549221038818, 9.942049026489258, 8.46922779083252]\n",
            "Target weights: [0.4092893898487091, 0.27172815799713135, 0.31898239254951477]\n",
            "Updated weights: [0.37384751439094543, 0.291697233915329, 0.3344552516937256]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 336\n",
            "  Loss for client 0: 1.9123\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 336\n",
            "  Loss for client 1: 2.3530\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 336\n",
            "  Loss for client 2: 1.3658\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 337\n",
            "  Client 0: total grad norm = 91.293961 (253 params)\n",
            "  Client 1: total grad norm = 92.298967 (253 params)\n",
            "  Client 2: total grad norm = 73.326388 (253 params)\n",
            "\n",
            "=== Optimizer Step 337 ===\n",
            "\n",
            "=== Updating Client Weights (Step 337) ===\n",
            "Gradient norms: [8.826889038085938, 10.308445930480957, 8.468729019165039]\n",
            "Target weights: [0.344997763633728, 0.2954137921333313, 0.3595884144306183]\n",
            "Updated weights: [0.36519259214401245, 0.29281219840049744, 0.3419952094554901]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 337\n",
            "  Loss for client 0: 2.1763\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 337\n",
            "  Loss for client 1: 1.7733\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 337\n",
            "  Loss for client 2: 1.7815\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 338\n",
            "  Client 0: total grad norm = 86.234403 (253 params)\n",
            "  Client 1: total grad norm = 79.205329 (253 params)\n",
            "  Client 2: total grad norm = 75.969117 (253 params)\n",
            "\n",
            "=== Optimizer Step 338 ===\n",
            "\n",
            "=== Updating Client Weights (Step 338) ===\n",
            "Gradient norms: [9.410256385803223, 8.10265064239502, 9.043719291687012]\n",
            "Target weights: [0.31231364607810974, 0.36271482706069946, 0.3249715566635132]\n",
            "Updated weights: [0.3493289053440094, 0.3137829899787903, 0.3368881344795227]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 338\n",
            "  Loss for client 0: 1.4252\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 338\n",
            "  Loss for client 1: 2.1004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 338\n",
            "  Loss for client 2: 1.5254\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 339\n",
            "  Client 0: total grad norm = 74.762024 (253 params)\n",
            "  Client 1: total grad norm = 83.227585 (253 params)\n",
            "  Client 2: total grad norm = 84.337017 (253 params)\n",
            "\n",
            "=== Optimizer Step 339 ===\n",
            "\n",
            "=== Updating Client Weights (Step 339) ===\n",
            "Gradient norms: [8.545180320739746, 9.362935066223145, 8.585915565490723]\n",
            "Target weights: [0.34388890862464905, 0.31385380029678345, 0.3422573506832123]\n",
            "Updated weights: [0.3476969003677368, 0.3138042390346527, 0.33849889039993286]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 339\n",
            "  Loss for client 0: 1.3962\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 339\n",
            "  Loss for client 1: 1.3909\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 339\n",
            "  Loss for client 2: 1.5651\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 340\n",
            "  Client 0: total grad norm = 76.697536 (253 params)\n",
            "  Client 1: total grad norm = 80.214160 (253 params)\n",
            "  Client 2: total grad norm = 78.338043 (253 params)\n",
            "\n",
            "=== Optimizer Step 340 ===\n",
            "\n",
            "=== Updating Client Weights (Step 340) ===\n",
            "Gradient norms: [8.46884822845459, 8.262992858886719, 8.530794143676758]\n",
            "Target weights: [0.33138352632522583, 0.33963924646377563, 0.32897719740867615]\n",
            "Updated weights: [0.34280288219451904, 0.3215547502040863, 0.33564239740371704]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 340\n",
            "  Loss for client 0: 1.0665\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 340\n",
            "  Loss for client 1: 2.0005\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 340\n",
            "  Loss for client 2: 1.9187\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 341\n",
            "  Client 0: total grad norm = 61.812268 (253 params)\n",
            "  Client 1: total grad norm = 94.817142 (253 params)\n",
            "  Client 2: total grad norm = 70.404734 (253 params)\n",
            "\n",
            "=== Optimizer Step 341 ===\n",
            "\n",
            "=== Updating Client Weights (Step 341) ===\n",
            "Gradient norms: [6.243873596191406, 9.343524932861328, 7.7394280433654785]\n",
            "Target weights: [0.40403738617897034, 0.270000696182251, 0.3259618878364563]\n",
            "Updated weights: [0.36117324233055115, 0.30608853697776794, 0.3327382504940033]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 341\n",
            "  Loss for client 0: 1.4314\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 341\n",
            "  Loss for client 1: 1.0406\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 341\n",
            "  Loss for client 2: 1.8677\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 342\n",
            "  Client 0: total grad norm = 68.199909 (253 params)\n",
            "  Client 1: total grad norm = 53.848143 (253 params)\n",
            "  Client 2: total grad norm = 87.830750 (253 params)\n",
            "\n",
            "=== Optimizer Step 342 ===\n",
            "\n",
            "=== Updating Client Weights (Step 342) ===\n",
            "Gradient norms: [7.416827201843262, 6.6432976722717285, 9.147114753723145]\n",
            "Target weights: [0.34161457419395447, 0.38139137625694275, 0.2769940495491028]\n",
            "Updated weights: [0.35530564188957214, 0.3286793828010559, 0.31601500511169434]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 342\n",
            "  Loss for client 0: 1.7511\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 342\n",
            "  Loss for client 1: 1.5810\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 342\n",
            "  Loss for client 2: 1.8239\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 343\n",
            "  Client 0: total grad norm = 74.720064 (253 params)\n",
            "  Client 1: total grad norm = 73.350714 (253 params)\n",
            "  Client 2: total grad norm = 92.168574 (253 params)\n",
            "\n",
            "=== Optimizer Step 343 ===\n",
            "\n",
            "=== Updating Client Weights (Step 343) ===\n",
            "Gradient norms: [9.138901710510254, 7.757588863372803, 9.494147300720215]\n",
            "Target weights: [0.31840622425079346, 0.3751014769077301, 0.30649229884147644]\n",
            "Updated weights: [0.3442358076572418, 0.3426060080528259, 0.31315818428993225]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 343\n",
            "  Loss for client 0: 1.8780\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 343\n",
            "  Loss for client 1: 1.1860\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 343\n",
            "  Loss for client 2: 1.4001\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 344\n",
            "  Client 0: total grad norm = 97.167087 (253 params)\n",
            "  Client 1: total grad norm = 55.849077 (253 params)\n",
            "  Client 2: total grad norm = 66.914850 (253 params)\n",
            "\n",
            "=== Optimizer Step 344 ===\n",
            "\n",
            "=== Updating Client Weights (Step 344) ===\n",
            "Gradient norms: [9.806132316589355, 6.669654846191406, 7.6129679679870605]\n",
            "Target weights: [0.26607468724250793, 0.3911992013454437, 0.3427261710166931]\n",
            "Updated weights: [0.3207874894142151, 0.35718396306037903, 0.32202857732772827]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 344\n",
            "  Loss for client 0: 1.6926\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 344\n",
            "  Loss for client 1: 1.5324\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 344\n",
            "  Loss for client 2: 1.9698\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 345\n",
            "  Client 0: total grad norm = 110.501660 (253 params)\n",
            "  Client 1: total grad norm = 71.231509 (253 params)\n",
            "  Client 2: total grad norm = 91.343433 (253 params)\n",
            "\n",
            "=== Optimizer Step 345 ===\n",
            "\n",
            "=== Updating Client Weights (Step 345) ===\n",
            "Gradient norms: [10.957952499389648, 7.621041774749756, 8.98752498626709]\n",
            "Target weights: [0.27344104647636414, 0.3931685984134674, 0.33339035511016846]\n",
            "Updated weights: [0.30658355355262756, 0.36797934770584106, 0.32543709874153137]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 345\n",
            "  Loss for client 0: 1.4429\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 345\n",
            "  Loss for client 1: 1.7184\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 345\n",
            "  Loss for client 2: 2.2274\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 346\n",
            "  Client 0: total grad norm = 75.419581 (253 params)\n",
            "  Client 1: total grad norm = 72.198159 (253 params)\n",
            "  Client 2: total grad norm = 78.341191 (253 params)\n",
            "\n",
            "=== Optimizer Step 346 ===\n",
            "\n",
            "=== Updating Client Weights (Step 346) ===\n",
            "Gradient norms: [7.898736953735352, 7.804455280303955, 8.070602416992188]\n",
            "Target weights: [0.3343603312969208, 0.33839958906173706, 0.3272400498390198]\n",
            "Updated weights: [0.31491661071777344, 0.3591054081916809, 0.32597798109054565]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 346\n",
            "  Loss for client 0: 1.4360\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 346\n",
            "  Loss for client 1: 1.9454\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 346\n",
            "  Loss for client 2: 1.4717\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 347\n",
            "  Client 0: total grad norm = 83.261823 (253 params)\n",
            "  Client 1: total grad norm = 84.902562 (253 params)\n",
            "  Client 2: total grad norm = 71.954098 (253 params)\n",
            "\n",
            "=== Optimizer Step 347 ===\n",
            "\n",
            "=== Updating Client Weights (Step 347) ===\n",
            "Gradient norms: [8.159770011901855, 9.487837791442871, 7.30621337890625]\n",
            "Target weights: [0.33592554926872253, 0.288904070854187, 0.37517040967941284]\n",
            "Updated weights: [0.3212192952632904, 0.33804500102996826, 0.34073570370674133]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 347\n",
            "  Loss for client 0: 1.5279\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 347\n",
            "  Loss for client 1: 2.4064\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 347\n",
            "  Loss for client 2: 1.9910\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 348\n",
            "  Client 0: total grad norm = 80.038908 (253 params)\n",
            "  Client 1: total grad norm = 89.546059 (253 params)\n",
            "  Client 2: total grad norm = 95.802759 (253 params)\n",
            "\n",
            "=== Optimizer Step 348 ===\n",
            "\n",
            "=== Updating Client Weights (Step 348) ===\n",
            "Gradient norms: [8.90571403503418, 10.185543060302734, 9.351754188537598]\n",
            "Target weights: [0.3537753224372864, 0.30932292342185974, 0.3369017243385315]\n",
            "Updated weights: [0.3309861123561859, 0.32942837476730347, 0.3395855128765106]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 348\n",
            "  Loss for client 0: 1.7055\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 348\n",
            "  Loss for client 1: 2.1543\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 348\n",
            "  Loss for client 2: 1.7268\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 349\n",
            "  Client 0: total grad norm = 112.553597 (253 params)\n",
            "  Client 1: total grad norm = 73.506071 (253 params)\n",
            "  Client 2: total grad norm = 77.055242 (253 params)\n",
            "\n",
            "=== Optimizer Step 349 ===\n",
            "\n",
            "=== Updating Client Weights (Step 349) ===\n",
            "Gradient norms: [11.251935958862305, 7.721205234527588, 9.415853500366211]\n",
            "Target weights: [0.27380186319351196, 0.3990051746368408, 0.3271929621696472]\n",
            "Updated weights: [0.3138308525085449, 0.3503014147281647, 0.3358677327632904]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 349\n",
            "  Loss for client 0: 1.2873\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 349\n",
            "  Loss for client 1: 0.9030\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 349\n",
            "  Loss for client 2: 2.0497\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 350\n",
            "  Client 0: total grad norm = 71.168412 (253 params)\n",
            "  Client 1: total grad norm = 50.575557 (253 params)\n",
            "  Client 2: total grad norm = 87.182971 (253 params)\n",
            "\n",
            "=== Optimizer Step 350 ===\n",
            "\n",
            "=== Updating Client Weights (Step 350) ===\n",
            "Gradient norms: [7.770540237426758, 5.495590686798096, 9.590734481811523]\n",
            "Target weights: [0.3101571202278137, 0.4385494589805603, 0.251293420791626]\n",
            "Updated weights: [0.31272873282432556, 0.3767758309841156, 0.31049543619155884]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 350\n",
            "  Val Loss = 1.6014 (5 batches)\n",
            "\n",
            "[Processing] Client 0, batch 350\n",
            "  Loss for client 0: 1.2000\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 350\n",
            "  Loss for client 1: 1.6010\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 350\n",
            "  Loss for client 2: 1.6414\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 351\n",
            "  Client 0: total grad norm = 54.851007 (253 params)\n",
            "  Client 1: total grad norm = 68.840859 (253 params)\n",
            "  Client 2: total grad norm = 74.982391 (253 params)\n",
            "\n",
            "=== Optimizer Step 351 ===\n",
            "\n",
            "=== Updating Client Weights (Step 351) ===\n",
            "Gradient norms: [6.458481788635254, 7.352973461151123, 7.621061325073242]\n",
            "Target weights: [0.36686456203460693, 0.3222353756427765, 0.3109000325202942]\n",
            "Updated weights: [0.32896947860717773, 0.36041370034217834, 0.3106168210506439]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 351\n",
            "  Loss for client 0: 1.2297\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 351\n",
            "  Loss for client 1: 2.0123\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 351\n",
            "  Loss for client 2: 1.6473\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 352\n",
            "  Client 0: total grad norm = 71.837552 (253 params)\n",
            "  Client 1: total grad norm = 76.233552 (253 params)\n",
            "  Client 2: total grad norm = 62.905775 (253 params)\n",
            "\n",
            "=== Optimizer Step 352 ===\n",
            "\n",
            "=== Updating Client Weights (Step 352) ===\n",
            "Gradient norms: [8.133430480957031, 9.033468246459961, 7.344388961791992]\n",
            "Target weights: [0.33246883749961853, 0.2993437349796295, 0.36818748712539673]\n",
            "Updated weights: [0.3300192952156067, 0.34209272265434265, 0.32788801193237305]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 352\n",
            "  Loss for client 0: 2.1602\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 352\n",
            "  Loss for client 1: 1.8661\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 352\n",
            "  Loss for client 2: 2.0488\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 353\n",
            "  Client 0: total grad norm = 88.424141 (253 params)\n",
            "  Client 1: total grad norm = 71.920359 (253 params)\n",
            "  Client 2: total grad norm = 78.601843 (253 params)\n",
            "\n",
            "=== Optimizer Step 353 ===\n",
            "\n",
            "=== Updating Client Weights (Step 353) ===\n",
            "Gradient norms: [11.90495491027832, 8.162083625793457, 10.013092041015625]\n",
            "Target weights: [0.27415984869003296, 0.3998807966709137, 0.32595932483673096]\n",
            "Updated weights: [0.3132614493370056, 0.35942915081977844, 0.32730939984321594]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 353\n",
            "  Loss for client 0: 1.8663\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 353\n",
            "  Loss for client 1: 2.0626\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 353\n",
            "  Loss for client 2: 2.2453\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 354\n",
            "  Client 0: total grad norm = 71.261982 (253 params)\n",
            "  Client 1: total grad norm = 109.154705 (253 params)\n",
            "  Client 2: total grad norm = 80.600554 (253 params)\n",
            "\n",
            "=== Optimizer Step 354 ===\n",
            "\n",
            "=== Updating Client Weights (Step 354) ===\n",
            "Gradient norms: [7.454716682434082, 12.06878662109375, 9.396769523620605]\n",
            "Target weights: [0.4147633910179138, 0.2561933994293213, 0.3290432393550873]\n",
            "Updated weights: [0.3437120318412781, 0.32845842838287354, 0.3278295397758484]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 354\n",
            "  Loss for client 0: 1.7362\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 354\n",
            "  Loss for client 1: 1.5893\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 354\n",
            "  Loss for client 2: 1.9218\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 355\n",
            "  Client 0: total grad norm = 76.911450 (253 params)\n",
            "  Client 1: total grad norm = 69.716783 (253 params)\n",
            "  Client 2: total grad norm = 79.549908 (253 params)\n",
            "\n",
            "=== Optimizer Step 355 ===\n",
            "\n",
            "=== Updating Client Weights (Step 355) ===\n",
            "Gradient norms: [8.749844551086426, 8.174541473388672, 8.519916534423828]\n",
            "Target weights: [0.3228551149368286, 0.34557685256004333, 0.33156806230545044]\n",
            "Updated weights: [0.33745497465133667, 0.3335939645767212, 0.3289510905742645]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 355\n",
            "  Loss for client 0: 1.9307\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 355\n",
            "  Loss for client 1: 2.2141\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 355\n",
            "  Loss for client 2: 2.0210\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 356\n",
            "  Client 0: total grad norm = 83.627292 (253 params)\n",
            "  Client 1: total grad norm = 85.347105 (253 params)\n",
            "  Client 2: total grad norm = 90.204331 (253 params)\n",
            "\n",
            "=== Optimizer Step 356 ===\n",
            "\n",
            "=== Updating Client Weights (Step 356) ===\n",
            "Gradient norms: [10.509671211242676, 9.2538423538208, 8.898000717163086]\n",
            "Target weights: [0.3014921247959137, 0.3424072861671448, 0.35610055923461914]\n",
            "Updated weights: [0.32666611671447754, 0.33623796701431274, 0.3370959460735321]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 356\n",
            "  Loss for client 0: 1.0745\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 356\n",
            "  Loss for client 1: 1.9042\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 356\n",
            "  Loss for client 2: 1.7789\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 357\n",
            "  Client 0: total grad norm = 71.850222 (253 params)\n",
            "  Client 1: total grad norm = 82.868824 (253 params)\n",
            "  Client 2: total grad norm = 76.265199 (253 params)\n",
            "\n",
            "=== Optimizer Step 357 ===\n",
            "\n",
            "=== Updating Client Weights (Step 357) ===\n",
            "Gradient norms: [6.595404148101807, 9.14537525177002, 8.31657886505127]\n",
            "Target weights: [0.3977382183074951, 0.2868383526802063, 0.31542348861694336]\n",
            "Updated weights: [0.34798774123191833, 0.32141807675361633, 0.3305942118167877]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 357\n",
            "  Loss for client 0: 1.4321\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 357\n",
            "  Loss for client 1: 1.6579\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 357\n",
            "  Loss for client 2: 2.2318\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 358\n",
            "  Client 0: total grad norm = 67.875487 (253 params)\n",
            "  Client 1: total grad norm = 90.983186 (253 params)\n",
            "  Client 2: total grad norm = 92.178161 (253 params)\n",
            "\n",
            "=== Optimizer Step 358 ===\n",
            "\n",
            "=== Updating Client Weights (Step 358) ===\n",
            "Gradient norms: [8.426546096801758, 8.740976333618164, 11.022377967834473]\n",
            "Target weights: [0.366498738527298, 0.3533150553703308, 0.2801862359046936]\n",
            "Updated weights: [0.3535410463809967, 0.3309871554374695, 0.3154718279838562]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 358\n",
            "  Loss for client 0: 1.7680\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 358\n",
            "  Loss for client 1: 1.3993\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 358\n",
            "  Loss for client 2: 1.9100\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 359\n",
            "  Client 0: total grad norm = 73.262336 (253 params)\n",
            "  Client 1: total grad norm = 76.291465 (253 params)\n",
            "  Client 2: total grad norm = 73.334118 (253 params)\n",
            "\n",
            "=== Optimizer Step 359 ===\n",
            "\n",
            "=== Updating Client Weights (Step 359) ===\n",
            "Gradient norms: [7.787750244140625, 8.621671676635742, 8.3327054977417]\n",
            "Target weights: [0.35237616300582886, 0.3182929456233978, 0.3293308913707733]\n",
            "Updated weights: [0.3531915843486786, 0.3271788954734802, 0.3196295499801636]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 359\n",
            "  Loss for client 0: 1.3445\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 359\n",
            "  Loss for client 1: 1.9738\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 359\n",
            "  Loss for client 2: 2.1882\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 360\n",
            "  Client 0: total grad norm = 62.664347 (253 params)\n",
            "  Client 1: total grad norm = 79.358731 (253 params)\n",
            "  Client 2: total grad norm = 78.006986 (253 params)\n",
            "\n",
            "=== Optimizer Step 360 ===\n",
            "\n",
            "=== Updating Client Weights (Step 360) ===\n",
            "Gradient norms: [6.678677082061768, 8.298615455627441, 9.970900535583496]\n",
            "Target weights: [0.40410393476486206, 0.3252204954624176, 0.2706756293773651]\n",
            "Updated weights: [0.3684653043746948, 0.3265913724899292, 0.30494338274002075]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 360\n",
            "  Loss for client 0: 1.8723\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 360\n",
            "  Loss for client 1: 1.7289\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 360\n",
            "  Loss for client 2: 2.0683\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 361\n",
            "  Client 0: total grad norm = 76.171628 (253 params)\n",
            "  Client 1: total grad norm = 88.872293 (253 params)\n",
            "  Client 2: total grad norm = 84.778285 (253 params)\n",
            "\n",
            "=== Optimizer Step 361 ===\n",
            "\n",
            "=== Updating Client Weights (Step 361) ===\n",
            "Gradient norms: [8.444286346435547, 8.37458610534668, 9.773272514343262]\n",
            "Target weights: [0.34814804792404175, 0.3510456383228302, 0.30080631375312805]\n",
            "Updated weights: [0.3623701333999634, 0.3339276611804962, 0.3037022650241852]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 361\n",
            "  Loss for client 0: 1.2386\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 361\n",
            "  Loss for client 1: 2.0721\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 361\n",
            "  Loss for client 2: 2.0856\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 362\n",
            "  Client 0: total grad norm = 66.794627 (253 params)\n",
            "  Client 1: total grad norm = 74.683935 (253 params)\n",
            "  Client 2: total grad norm = 84.797922 (253 params)\n",
            "\n",
            "=== Optimizer Step 362 ===\n",
            "\n",
            "=== Updating Client Weights (Step 362) ===\n",
            "Gradient norms: [7.0180745124816895, 8.11526870727539, 7.976289749145508]\n",
            "Target weights: [0.3643430471420288, 0.31508344411849976, 0.32057344913482666]\n",
            "Updated weights: [0.362962007522583, 0.3282743990421295, 0.30876362323760986]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 362\n",
            "  Loss for client 0: 1.2115\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 362\n",
            "  Loss for client 1: 1.8642\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 362\n",
            "  Loss for client 2: 1.9599\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 363\n",
            "  Client 0: total grad norm = 83.458381 (253 params)\n",
            "  Client 1: total grad norm = 74.663111 (253 params)\n",
            "  Client 2: total grad norm = 80.110985 (253 params)\n",
            "\n",
            "=== Optimizer Step 363 ===\n",
            "\n",
            "=== Updating Client Weights (Step 363) ===\n",
            "Gradient norms: [9.10065746307373, 8.160552978515625, 8.739182472229004]\n",
            "Target weights: [0.31680020689964294, 0.3532959222793579, 0.32990387082099915]\n",
            "Updated weights: [0.34911346435546875, 0.3357808589935303, 0.31510570645332336]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 363\n",
            "  Loss for client 0: 1.7452\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 363\n",
            "  Loss for client 1: 2.1508\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 363\n",
            "  Loss for client 2: 1.3372\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 364\n",
            "  Client 0: total grad norm = 76.578998 (253 params)\n",
            "  Client 1: total grad norm = 89.700742 (253 params)\n",
            "  Client 2: total grad norm = 68.872290 (253 params)\n",
            "\n",
            "=== Optimizer Step 364 ===\n",
            "\n",
            "=== Updating Client Weights (Step 364) ===\n",
            "Gradient norms: [8.23890495300293, 9.318305015563965, 7.857863426208496]\n",
            "Target weights: [0.3409879207611084, 0.30148905515670776, 0.35752302408218384]\n",
            "Updated weights: [0.3466758131980896, 0.32549330592155457, 0.3278309106826782]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 364\n",
            "  Loss for client 0: 1.0130\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 364\n",
            "  Loss for client 1: 1.7312\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 364\n",
            "  Loss for client 2: 1.8506\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 365\n",
            "  Client 0: total grad norm = 60.358577 (253 params)\n",
            "  Client 1: total grad norm = 71.035079 (253 params)\n",
            "  Client 2: total grad norm = 68.090928 (253 params)\n",
            "\n",
            "=== Optimizer Step 365 ===\n",
            "\n",
            "=== Updating Client Weights (Step 365) ===\n",
            "Gradient norms: [6.597640514373779, 8.190954208374023, 8.030879020690918]\n",
            "Target weights: [0.3806604743003845, 0.3066139817237854, 0.3127255439758301]\n",
            "Updated weights: [0.35687121748924255, 0.319829523563385, 0.3232992887496948]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 365\n",
            "  Loss for client 0: 1.5615\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 365\n",
            "  Loss for client 1: 1.8223\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 365\n",
            "  Loss for client 2: 2.0459\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 366\n",
            "  Client 0: total grad norm = 77.102179 (253 params)\n",
            "  Client 1: total grad norm = 76.297841 (253 params)\n",
            "  Client 2: total grad norm = 101.583599 (253 params)\n",
            "\n",
            "=== Optimizer Step 366 ===\n",
            "\n",
            "=== Updating Client Weights (Step 366) ===\n",
            "Gradient norms: [9.281462669372559, 9.470908164978027, 9.770026206970215]\n",
            "Target weights: [0.34129801392555237, 0.33447107672691345, 0.32423093914985657]\n",
            "Updated weights: [0.3521992564201355, 0.32422199845314026, 0.32357877492904663]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 366\n",
            "  Loss for client 0: 1.6035\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 366\n",
            "  Loss for client 1: 1.7275\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 366\n",
            "  Loss for client 2: 1.8293\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 367\n",
            "  Client 0: total grad norm = 85.615226 (253 params)\n",
            "  Client 1: total grad norm = 78.022906 (253 params)\n",
            "  Client 2: total grad norm = 69.660073 (253 params)\n",
            "\n",
            "=== Optimizer Step 367 ===\n",
            "\n",
            "=== Updating Client Weights (Step 367) ===\n",
            "Gradient norms: [10.314615249633789, 8.38525104522705, 8.338574409484863]\n",
            "Target weights: [0.28842830657958984, 0.35479283332824707, 0.3567788600921631]\n",
            "Updated weights: [0.33306795358657837, 0.33339324593544006, 0.33353880047798157]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 367\n",
            "  Loss for client 0: 1.8637\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 367\n",
            "  Loss for client 1: 1.6787\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 367\n",
            "  Loss for client 2: 1.9914\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 368\n",
            "  Client 0: total grad norm = 88.537841 (253 params)\n",
            "  Client 1: total grad norm = 70.400881 (253 params)\n",
            "  Client 2: total grad norm = 81.842746 (253 params)\n",
            "\n",
            "=== Optimizer Step 368 ===\n",
            "\n",
            "=== Updating Client Weights (Step 368) ===\n",
            "Gradient norms: [8.590678215026855, 8.209524154663086, 9.124236106872559]\n",
            "Target weights: [0.33467745780944824, 0.35021597146987915, 0.3151065409183502]\n",
            "Updated weights: [0.3335508108139038, 0.33844006061553955, 0.32800912857055664]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 368\n",
            "  Loss for client 0: 1.8820\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 368\n",
            "  Loss for client 1: 1.4139\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 368\n",
            "  Loss for client 2: 1.3834\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 369\n",
            "  Client 0: total grad norm = 74.795519 (253 params)\n",
            "  Client 1: total grad norm = 75.508257 (253 params)\n",
            "  Client 2: total grad norm = 64.480545 (253 params)\n",
            "\n",
            "=== Optimizer Step 369 ===\n",
            "\n",
            "=== Updating Client Weights (Step 369) ===\n",
            "Gradient norms: [7.070667743682861, 7.518708229064941, 6.620716094970703]\n",
            "Target weights: [0.33240580558776855, 0.31259769201278687, 0.3549964725971222]\n",
            "Updated weights: [0.33320730924606323, 0.33068734407424927, 0.3361053466796875]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 369\n",
            "  Loss for client 0: 1.1899\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 369\n",
            "  Loss for client 1: 2.0147\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 369\n",
            "  Loss for client 2: 2.0432\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 370\n",
            "  Client 0: total grad norm = 70.949898 (253 params)\n",
            "  Client 1: total grad norm = 86.891297 (253 params)\n",
            "  Client 2: total grad norm = 91.433071 (253 params)\n",
            "\n",
            "=== Optimizer Step 370 ===\n",
            "\n",
            "=== Updating Client Weights (Step 370) ===\n",
            "Gradient norms: [7.2560811042785645, 8.379267692565918, 9.507735252380371]\n",
            "Target weights: [0.38035351037979126, 0.32936957478523254, 0.2902768850326538]\n",
            "Updated weights: [0.34735119342803955, 0.3302920162677765, 0.32235682010650635]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 370\n",
            "  Loss for client 0: 1.6245\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 370\n",
            "  Loss for client 1: 1.9530\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 370\n",
            "  Loss for client 2: 1.6850\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 371\n",
            "  Client 0: total grad norm = 76.709586 (253 params)\n",
            "  Client 1: total grad norm = 77.918092 (253 params)\n",
            "  Client 2: total grad norm = 92.273574 (253 params)\n",
            "\n",
            "=== Optimizer Step 371 ===\n",
            "\n",
            "=== Updating Client Weights (Step 371) ===\n",
            "Gradient norms: [9.163576126098633, 9.413652420043945, 9.867835998535156]\n",
            "Target weights: [0.34458214044570923, 0.33542823791503906, 0.3199895918369293]\n",
            "Updated weights: [0.34652048349380493, 0.3318328857421875, 0.32164666056632996]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 371\n",
            "  Loss for client 0: 1.3534\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 371\n",
            "  Loss for client 1: 2.5591\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 371\n",
            "  Loss for client 2: 1.9624\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 372\n",
            "  Client 0: total grad norm = 75.463234 (253 params)\n",
            "  Client 1: total grad norm = 102.016596 (253 params)\n",
            "  Client 2: total grad norm = 95.510143 (253 params)\n",
            "\n",
            "=== Optimizer Step 372 ===\n",
            "\n",
            "=== Updating Client Weights (Step 372) ===\n",
            "Gradient norms: [8.272449493408203, 10.049595832824707, 9.029932022094727]\n",
            "Target weights: [0.36505988240242004, 0.30050358176231384, 0.3344365656375885]\n",
            "Updated weights: [0.3520823121070862, 0.32243409752845764, 0.32548362016677856]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 372\n",
            "  Loss for client 0: 1.4933\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 372\n",
            "  Loss for client 1: 1.8902\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 372\n",
            "  Loss for client 2: 2.0475\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 373\n",
            "  Client 0: total grad norm = 76.079427 (253 params)\n",
            "  Client 1: total grad norm = 87.835095 (253 params)\n",
            "  Client 2: total grad norm = 84.096544 (253 params)\n",
            "\n",
            "=== Optimizer Step 373 ===\n",
            "\n",
            "=== Updating Client Weights (Step 373) ===\n",
            "Gradient norms: [8.216085433959961, 9.609142303466797, 9.88503360748291]\n",
            "Target weights: [0.37227416038513184, 0.3183048367500305, 0.30942094326019287]\n",
            "Updated weights: [0.35813987255096436, 0.3211953043937683, 0.32066482305526733]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 373\n",
            "  Loss for client 0: 1.6703\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 373\n",
            "  Loss for client 1: 1.9876\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 373\n",
            "  Loss for client 2: 1.5717\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 374\n",
            "  Client 0: total grad norm = 81.686706 (253 params)\n",
            "  Client 1: total grad norm = 74.294194 (253 params)\n",
            "  Client 2: total grad norm = 69.971129 (253 params)\n",
            "\n",
            "=== Optimizer Step 374 ===\n",
            "\n",
            "=== Updating Client Weights (Step 374) ===\n",
            "Gradient norms: [7.981757640838623, 7.749791145324707, 7.808623313903809]\n",
            "Target weights: [0.3276427984237671, 0.3374498188495636, 0.3349073529243469]\n",
            "Updated weights: [0.3489907383918762, 0.3260716497898102, 0.3249375820159912]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 374\n",
            "  Loss for client 0: 1.6215\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 374\n",
            "  Loss for client 1: 1.4265\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 374\n",
            "  Loss for client 2: 2.0897\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 375\n",
            "  Client 0: total grad norm = 100.885558 (253 params)\n",
            "  Client 1: total grad norm = 69.958514 (253 params)\n",
            "  Client 2: total grad norm = 83.045090 (253 params)\n",
            "\n",
            "=== Optimizer Step 375 ===\n",
            "\n",
            "=== Updating Client Weights (Step 375) ===\n",
            "Gradient norms: [12.155952453613281, 7.897324085235596, 9.542500495910645]\n",
            "Target weights: [0.2622520923614502, 0.40367141366004944, 0.3340764045715332]\n",
            "Updated weights: [0.3229691684246063, 0.3493516147136688, 0.32767924666404724]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 375\n",
            "  Loss for client 0: 1.3780\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 375\n",
            "  Loss for client 1: 1.8372\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 375\n",
            "  Loss for client 2: 2.0511\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 376\n",
            "  Client 0: total grad norm = 70.963298 (253 params)\n",
            "  Client 1: total grad norm = 93.431695 (253 params)\n",
            "  Client 2: total grad norm = 87.454378 (253 params)\n",
            "\n",
            "=== Optimizer Step 376 ===\n",
            "\n",
            "=== Updating Client Weights (Step 376) ===\n",
            "Gradient norms: [8.081393241882324, 8.803945541381836, 9.934269905090332]\n",
            "Target weights: [0.36611059308052063, 0.3360633850097656, 0.29782599210739136]\n",
            "Updated weights: [0.3359116017818451, 0.34536516666412354, 0.31872326135635376]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 376\n",
            "  Loss for client 0: 1.4943\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 376\n",
            "  Loss for client 1: 1.0517\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 376\n",
            "  Loss for client 2: 1.9617\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 377\n",
            "  Client 0: total grad norm = 76.610532 (253 params)\n",
            "  Client 1: total grad norm = 71.228981 (253 params)\n",
            "  Client 2: total grad norm = 71.306999 (253 params)\n",
            "\n",
            "=== Optimizer Step 377 ===\n",
            "\n",
            "=== Updating Client Weights (Step 377) ===\n",
            "Gradient norms: [8.375965118408203, 6.281859397888184, 6.944670677185059]\n",
            "Target weights: [0.28252917528152466, 0.376712441444397, 0.340758353471756]\n",
            "Updated weights: [0.3198968768119812, 0.35476934909820557, 0.32533377408981323]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 377\n",
            "  Loss for client 0: 1.1192\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 377\n",
            "  Loss for client 1: 1.5817\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 377\n",
            "  Loss for client 2: 1.9920\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 378\n",
            "  Client 0: total grad norm = 86.840260 (253 params)\n",
            "  Client 1: total grad norm = 81.530611 (253 params)\n",
            "  Client 2: total grad norm = 95.197317 (253 params)\n",
            "\n",
            "=== Optimizer Step 378 ===\n",
            "\n",
            "=== Updating Client Weights (Step 378) ===\n",
            "Gradient norms: [8.408884048461914, 9.839400291442871, 9.128692626953125]\n",
            "Target weights: [0.3602614402770996, 0.30788424611091614, 0.33185437321662903]\n",
            "Updated weights: [0.3320062458515167, 0.3407038152217865, 0.32728996872901917]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 378\n",
            "  Loss for client 0: 1.0719\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 378\n",
            "  Loss for client 1: 1.6226\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 378\n",
            "  Loss for client 2: 1.2346\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 379\n",
            "  Client 0: total grad norm = 70.146964 (253 params)\n",
            "  Client 1: total grad norm = 97.641433 (253 params)\n",
            "  Client 2: total grad norm = 75.174896 (253 params)\n",
            "\n",
            "=== Optimizer Step 379 ===\n",
            "\n",
            "=== Updating Client Weights (Step 379) ===\n",
            "Gradient norms: [6.5285444259643555, 8.370354652404785, 8.150845527648926]\n",
            "Target weights: [0.3874579071998596, 0.3022017776966095, 0.31034034490585327]\n",
            "Updated weights: [0.3486417531967163, 0.3291532099246979, 0.3222050666809082]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 379\n",
            "  Loss for client 0: 1.1605\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 379\n",
            "  Loss for client 1: 1.7761\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 379\n",
            "  Loss for client 2: 1.7397\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 380\n",
            "  Client 0: total grad norm = 75.086107 (253 params)\n",
            "  Client 1: total grad norm = 80.152869 (253 params)\n",
            "  Client 2: total grad norm = 89.121362 (253 params)\n",
            "\n",
            "=== Optimizer Step 380 ===\n",
            "\n",
            "=== Updating Client Weights (Step 380) ===\n",
            "Gradient norms: [7.609714984893799, 8.77878475189209, 8.871546745300293]\n",
            "Target weights: [0.36702683568000793, 0.3181498944759369, 0.31482329964637756]\n",
            "Updated weights: [0.3541572690010071, 0.3258522152900696, 0.3199905455112457]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 380\n",
            "  Loss for client 0: 1.1108\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 380\n",
            "  Loss for client 1: 1.4102\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 380\n",
            "  Loss for client 2: 1.4214\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 381\n",
            "  Client 0: total grad norm = 146.608918 (253 params)\n",
            "  Client 1: total grad norm = 94.441899 (253 params)\n",
            "  Client 2: total grad norm = 76.472143 (253 params)\n",
            "\n",
            "=== Optimizer Step 381 ===\n",
            "\n",
            "=== Updating Client Weights (Step 381) ===\n",
            "Gradient norms: [13.93582820892334, 9.93992805480957, 7.899935245513916]\n",
            "Target weights: [0.24003562331199646, 0.3365311026573181, 0.4234332740306854]\n",
            "Updated weights: [0.31992077827453613, 0.32905587553977966, 0.3510233759880066]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 381\n",
            "  Loss for client 0: 1.3032\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 381\n",
            "  Loss for client 1: 1.9214\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 381\n",
            "  Loss for client 2: 1.7285\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 382\n",
            "  Client 0: total grad norm = 69.215425 (253 params)\n",
            "  Client 1: total grad norm = 97.933351 (253 params)\n",
            "  Client 2: total grad norm = 69.062660 (253 params)\n",
            "\n",
            "=== Optimizer Step 382 ===\n",
            "\n",
            "=== Updating Client Weights (Step 382) ===\n",
            "Gradient norms: [7.544425010681152, 9.830328941345215, 8.787993431091309]\n",
            "Target weights: [0.3808136284351349, 0.2922607958316803, 0.3269255757331848]\n",
            "Updated weights: [0.33818864822387695, 0.3180173635482788, 0.343794047832489]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 382\n",
            "  Loss for client 0: 1.9510\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 382\n",
            "  Loss for client 1: 0.9982\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 382\n",
            "  Loss for client 2: 1.9726\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 383\n",
            "  Client 0: total grad norm = 91.580375 (253 params)\n",
            "  Client 1: total grad norm = 66.412115 (253 params)\n",
            "  Client 2: total grad norm = 82.179161 (253 params)\n",
            "\n",
            "=== Optimizer Step 383 ===\n",
            "\n",
            "=== Updating Client Weights (Step 383) ===\n",
            "Gradient norms: [8.890076637268066, 6.031101703643799, 9.212584495544434]\n",
            "Target weights: [0.29077956080436707, 0.428620308637619, 0.2806001603603363]\n",
            "Updated weights: [0.3239659070968628, 0.3511982560157776, 0.324835866689682]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 383\n",
            "  Loss for client 0: 1.9103\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 383\n",
            "  Loss for client 1: 1.8855\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 383\n",
            "  Loss for client 2: 2.0932\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 384\n",
            "  Client 0: total grad norm = 87.652453 (253 params)\n",
            "  Client 1: total grad norm = 88.105166 (253 params)\n",
            "  Client 2: total grad norm = 84.707538 (253 params)\n",
            "\n",
            "=== Optimizer Step 384 ===\n",
            "\n",
            "=== Updating Client Weights (Step 384) ===\n",
            "Gradient norms: [9.836326599121094, 8.710443496704102, 8.671799659729004]\n",
            "Target weights: [0.3064152002334595, 0.3460214138031006, 0.34756338596343994]\n",
            "Updated weights: [0.3187007009983063, 0.34964519739151, 0.3316541314125061]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 384\n",
            "  Loss for client 0: 1.6347\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 384\n",
            "  Loss for client 1: 2.1384\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 384\n",
            "  Loss for client 2: 1.0603\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 385\n",
            "  Client 0: total grad norm = 78.069183 (253 params)\n",
            "  Client 1: total grad norm = 134.031834 (253 params)\n",
            "  Client 2: total grad norm = 60.483410 (253 params)\n",
            "\n",
            "=== Optimizer Step 385 ===\n",
            "\n",
            "=== Updating Client Weights (Step 385) ===\n",
            "Gradient norms: [8.436182975769043, 10.713930130004883, 6.600310802459717]\n",
            "Target weights: [0.32620546221733093, 0.25685521960258484, 0.41693928837776184]\n",
            "Updated weights: [0.3209521174430847, 0.32180821895599365, 0.35723966360092163]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 385\n",
            "  Loss for client 0: 1.7423\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 385\n",
            "  Loss for client 1: 1.9715\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 385\n",
            "  Loss for client 2: 1.7644\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 386\n",
            "  Client 0: total grad norm = 82.700864 (253 params)\n",
            "  Client 1: total grad norm = 83.672008 (253 params)\n",
            "  Client 2: total grad norm = 61.196374 (253 params)\n",
            "\n",
            "=== Optimizer Step 386 ===\n",
            "\n",
            "=== Updating Client Weights (Step 386) ===\n",
            "Gradient norms: [8.433353424072266, 8.267237663269043, 6.708115100860596]\n",
            "Target weights: [0.30513107776641846, 0.3112621605396271, 0.38360676169395447]\n",
            "Updated weights: [0.31620579957962036, 0.3186444044113159, 0.3651497960090637]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 386\n",
            "  Loss for client 0: 1.6395\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 386\n",
            "  Loss for client 1: 2.0706\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 386\n",
            "  Loss for client 2: 2.1700\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 387\n",
            "  Client 0: total grad norm = 107.835685 (253 params)\n",
            "  Client 1: total grad norm = 86.664473 (253 params)\n",
            "  Client 2: total grad norm = 124.785750 (253 params)\n",
            "\n",
            "=== Optimizer Step 387 ===\n",
            "\n",
            "=== Updating Client Weights (Step 387) ===\n",
            "Gradient norms: [11.598567962646484, 8.89476490020752, 11.438947677612305]\n",
            "Target weights: [0.30139264464378357, 0.3930090367794037, 0.30559831857681274]\n",
            "Updated weights: [0.31176185607910156, 0.3409537971019745, 0.34728437662124634]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 387\n",
            "  Loss for client 0: 1.9253\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 387\n",
            "  Loss for client 1: 1.3539\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 387\n",
            "  Loss for client 2: 1.8341\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 388\n",
            "  Client 0: total grad norm = 85.094848 (253 params)\n",
            "  Client 1: total grad norm = 77.853064 (253 params)\n",
            "  Client 2: total grad norm = 73.779413 (253 params)\n",
            "\n",
            "=== Optimizer Step 388 ===\n",
            "\n",
            "=== Updating Client Weights (Step 388) ===\n",
            "Gradient norms: [10.849126815795898, 8.477588653564453, 7.835678577423096]\n",
            "Target weights: [0.2729017436504364, 0.34924381971359253, 0.3778544068336487]\n",
            "Updated weights: [0.3001038432121277, 0.34344080090522766, 0.35645538568496704]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 388\n",
            "  Loss for client 0: 1.3717\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 388\n",
            "  Loss for client 1: 1.9013\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 388\n",
            "  Loss for client 2: 2.3821\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 389\n",
            "  Client 0: total grad norm = 69.793955 (253 params)\n",
            "  Client 1: total grad norm = 90.815468 (253 params)\n",
            "  Client 2: total grad norm = 106.310414 (253 params)\n",
            "\n",
            "=== Optimizer Step 389 ===\n",
            "\n",
            "=== Updating Client Weights (Step 389) ===\n",
            "Gradient norms: [7.168766021728516, 10.414740562438965, 10.22459602355957]\n",
            "Target weights: [0.41850489377975464, 0.2880689799785614, 0.29342615604400635]\n",
            "Updated weights: [0.33562415838241577, 0.3268292546272278, 0.33754661679267883]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 389\n",
            "  Loss for client 0: 1.3023\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 389\n",
            "  Loss for client 1: 2.0058\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 389\n",
            "  Loss for client 2: 1.6345\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 390\n",
            "  Client 0: total grad norm = 102.329339 (253 params)\n",
            "  Client 1: total grad norm = 91.529288 (253 params)\n",
            "  Client 2: total grad norm = 82.482353 (253 params)\n",
            "\n",
            "=== Optimizer Step 390 ===\n",
            "\n",
            "=== Updating Client Weights (Step 390) ===\n",
            "Gradient norms: [12.490092277526855, 9.932778358459473, 9.993396759033203]\n",
            "Target weights: [0.285119891166687, 0.35852745175361633, 0.35635265707969666]\n",
            "Updated weights: [0.3204728662967682, 0.33633869886398315, 0.34318840503692627]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 390\n",
            "  Loss for client 0: 1.4498\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 390\n",
            "  Loss for client 1: 2.0608\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 390\n",
            "  Loss for client 2: 2.3099\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 391\n",
            "  Client 0: total grad norm = 63.018196 (253 params)\n",
            "  Client 1: total grad norm = 94.294318 (253 params)\n",
            "  Client 2: total grad norm = 90.814544 (253 params)\n",
            "\n",
            "=== Optimizer Step 391 ===\n",
            "\n",
            "=== Updating Client Weights (Step 391) ===\n",
            "Gradient norms: [7.515445709228516, 10.50488567352295, 11.009506225585938]\n",
            "Target weights: [0.41700440645217896, 0.29833489656448364, 0.2846607267856598]\n",
            "Updated weights: [0.3494323492050171, 0.3249375522136688, 0.3256300985813141]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 391\n",
            "  Loss for client 0: 2.1309\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 391\n",
            "  Loss for client 1: 1.4361\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 391\n",
            "  Loss for client 2: 2.3211\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 392\n",
            "  Client 0: total grad norm = 77.365880 (253 params)\n",
            "  Client 1: total grad norm = 62.504098 (253 params)\n",
            "  Client 2: total grad norm = 79.445154 (253 params)\n",
            "\n",
            "=== Optimizer Step 392 ===\n",
            "\n",
            "=== Updating Client Weights (Step 392) ===\n",
            "Gradient norms: [8.710721015930176, 7.262436389923096, 9.238920211791992]\n",
            "Target weights: [0.31824326515197754, 0.3817077577114105, 0.30004894733428955]\n",
            "Updated weights: [0.34007561206817627, 0.3419686257839203, 0.31795576214790344]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 392\n",
            "  Loss for client 0: 1.0789\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 392\n",
            "  Loss for client 1: 2.0969\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 392\n",
            "  Loss for client 2: 1.7445\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 393\n",
            "  Client 0: total grad norm = 65.926547 (253 params)\n",
            "  Client 1: total grad norm = 73.891243 (253 params)\n",
            "  Client 2: total grad norm = 74.563476 (253 params)\n",
            "\n",
            "=== Optimizer Step 393 ===\n",
            "\n",
            "=== Updating Client Weights (Step 393) ===\n",
            "Gradient norms: [6.9526567459106445, 9.188570976257324, 8.297394752502441]\n",
            "Target weights: [0.3854164779186249, 0.2916305959224701, 0.3229529857635498]\n",
            "Updated weights: [0.3536778688430786, 0.3268672227859497, 0.31945493817329407]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 393\n",
            "  Loss for client 0: 0.9666\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 393\n",
            "  Loss for client 1: 1.8211\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 393\n",
            "  Loss for client 2: 2.0495\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 394\n",
            "  Client 0: total grad norm = 61.924963 (253 params)\n",
            "  Client 1: total grad norm = 70.786803 (253 params)\n",
            "  Client 2: total grad norm = 86.812281 (253 params)\n",
            "\n",
            "=== Optimizer Step 394 ===\n",
            "\n",
            "=== Updating Client Weights (Step 394) ===\n",
            "Gradient norms: [6.695390224456787, 7.591207504272461, 9.570130348205566]\n",
            "Target weights: [0.387355774641037, 0.3416449725627899, 0.2709992229938507]\n",
            "Updated weights: [0.36378124356269836, 0.3313005566596985, 0.30491822957992554]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 394\n",
            "  Loss for client 0: 1.5807\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 394\n",
            "  Loss for client 1: 1.0664\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 394\n",
            "  Loss for client 2: 2.1954\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 395\n",
            "  Client 0: total grad norm = 72.019087 (253 params)\n",
            "  Client 1: total grad norm = 61.531451 (253 params)\n",
            "  Client 2: total grad norm = 92.644793 (253 params)\n",
            "\n",
            "=== Optimizer Step 395 ===\n",
            "\n",
            "=== Updating Client Weights (Step 395) ===\n",
            "Gradient norms: [8.954835891723633, 6.585913181304932, 8.156661987304688]\n",
            "Target weights: [0.2892220914363861, 0.3932539224624634, 0.3175240457057953]\n",
            "Updated weights: [0.3414134979248047, 0.34988656640052795, 0.30869996547698975]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 395\n",
            "  Loss for client 0: 1.7646\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 395\n",
            "  Loss for client 1: 1.9258\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 395\n",
            "  Loss for client 2: 1.7983\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 396\n",
            "  Client 0: total grad norm = 69.275827 (253 params)\n",
            "  Client 1: total grad norm = 81.989947 (253 params)\n",
            "  Client 2: total grad norm = 70.763729 (253 params)\n",
            "\n",
            "=== Optimizer Step 396 ===\n",
            "\n",
            "=== Updating Client Weights (Step 396) ===\n",
            "Gradient norms: [7.554644584655762, 8.113269805908203, 8.173676490783691]\n",
            "Target weights: [0.3502121865749359, 0.32609888911247253, 0.32368889451026917]\n",
            "Updated weights: [0.34405308961868286, 0.3427502512931824, 0.31319665908813477]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 396\n",
            "  Loss for client 0: 1.6796\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 396\n",
            "  Loss for client 1: 1.6196\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 396\n",
            "  Loss for client 2: 1.7880\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 397\n",
            "  Client 0: total grad norm = 63.644152 (253 params)\n",
            "  Client 1: total grad norm = 62.097428 (253 params)\n",
            "  Client 2: total grad norm = 73.481716 (253 params)\n",
            "\n",
            "=== Optimizer Step 397 ===\n",
            "\n",
            "=== Updating Client Weights (Step 397) ===\n",
            "Gradient norms: [7.556414604187012, 6.995096683502197, 8.400652885437012]\n",
            "Target weights: [0.335598886013031, 0.36252886056900024, 0.30187228322029114]\n",
            "Updated weights: [0.3415168225765228, 0.34868383407592773, 0.30979934334754944]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 397\n",
            "  Loss for client 0: 0.9186\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 397\n",
            "  Loss for client 1: 1.3186\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 397\n",
            "  Loss for client 2: 1.4896\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 398\n",
            "  Client 0: total grad norm = 63.750810 (253 params)\n",
            "  Client 1: total grad norm = 62.126561 (253 params)\n",
            "  Client 2: total grad norm = 88.042800 (253 params)\n",
            "\n",
            "=== Optimizer Step 398 ===\n",
            "\n",
            "=== Updating Client Weights (Step 398) ===\n",
            "Gradient norms: [8.442852973937988, 7.4213714599609375, 9.444405555725098]\n",
            "Target weights: [0.32985955476760864, 0.3752615749835968, 0.29487887024879456]\n",
            "Updated weights: [0.33801963925361633, 0.35665714740753174, 0.30532318353652954]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 398\n",
            "  Loss for client 0: 1.5692\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 398\n",
            "  Loss for client 1: 1.7966\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 398\n",
            "  Loss for client 2: 1.6349\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 399\n",
            "  Client 0: total grad norm = 73.454794 (253 params)\n",
            "  Client 1: total grad norm = 76.141022 (253 params)\n",
            "  Client 2: total grad norm = 67.664592 (253 params)\n",
            "\n",
            "=== Optimizer Step 399 ===\n",
            "\n",
            "=== Updating Client Weights (Step 399) ===\n",
            "Gradient norms: [7.309985637664795, 8.181642532348633, 7.7194342613220215]\n",
            "Target weights: [0.3520605266094208, 0.3145526647567749, 0.3333868086338043]\n",
            "Updated weights: [0.3422319293022156, 0.34402579069137573, 0.3137422800064087]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 399\n",
            "  Loss for client 0: 1.7735\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 399\n",
            "  Loss for client 1: 2.1889\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 399\n",
            "  Loss for client 2: 1.7555\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 400\n",
            "  Client 0: total grad norm = 69.267169 (253 params)\n",
            "  Client 1: total grad norm = 100.518609 (253 params)\n",
            "  Client 2: total grad norm = 83.000252 (253 params)\n",
            "\n",
            "=== Optimizer Step 400 ===\n",
            "\n",
            "=== Updating Client Weights (Step 400) ===\n",
            "Gradient norms: [8.665117263793945, 11.433333396911621, 8.587881088256836]\n",
            "Target weights: [0.3614184856414795, 0.27391254901885986, 0.36466893553733826]\n",
            "Updated weights: [0.3479878902435303, 0.32299181818962097, 0.32902026176452637]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 400\n",
            "  Val Loss = 1.5637 (5 batches)\n",
            "–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ /content/drive/MyDrive/meritopt_checkpoints/step_400\n",
            "\n",
            "[Processing] Client 0, batch 400\n",
            "  Loss for client 0: 1.1631\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 400\n",
            "  Loss for client 1: 1.3627\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 400\n",
            "  Loss for client 2: 1.7186\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 401\n",
            "  Client 0: total grad norm = 65.301581 (253 params)\n",
            "  Client 1: total grad norm = 68.360508 (253 params)\n",
            "  Client 2: total grad norm = 68.168492 (253 params)\n",
            "\n",
            "=== Optimizer Step 401 ===\n",
            "\n",
            "=== Updating Client Weights (Step 401) ===\n",
            "Gradient norms: [6.249353885650635, 7.315402984619141, 6.9922871589660645]\n",
            "Target weights: [0.3638979494571686, 0.31086835265159607, 0.3252336382865906]\n",
            "Updated weights: [0.352760910987854, 0.319354772567749, 0.3278842866420746]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 401\n",
            "  Loss for client 0: 1.6214\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 401\n",
            "  Loss for client 1: 1.7425\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 401\n",
            "  Loss for client 2: 1.6374\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 402\n",
            "  Client 0: total grad norm = 69.833092 (253 params)\n",
            "  Client 1: total grad norm = 72.241853 (253 params)\n",
            "  Client 2: total grad norm = 71.680454 (253 params)\n",
            "\n",
            "=== Optimizer Step 402 ===\n",
            "\n",
            "=== Updating Client Weights (Step 402) ===\n",
            "Gradient norms: [7.209412574768066, 8.349345207214355, 7.935387134552002]\n",
            "Target weights: [0.3607523739337921, 0.3114989697933197, 0.3277486562728882]\n",
            "Updated weights: [0.35515835881233215, 0.31699803471565247, 0.3278436064720154]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 402\n",
            "  Loss for client 0: 1.8545\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 402\n",
            "  Loss for client 1: 1.8354\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 402\n",
            "  Loss for client 2: 2.5103\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 403\n",
            "  Client 0: total grad norm = 86.793844 (253 params)\n",
            "  Client 1: total grad norm = 81.408096 (253 params)\n",
            "  Client 2: total grad norm = 105.380193 (253 params)\n",
            "\n",
            "=== Optimizer Step 403 ===\n",
            "\n",
            "=== Updating Client Weights (Step 403) ===\n",
            "Gradient norms: [10.438309669494629, 8.764731407165527, 9.458683967590332]\n",
            "Target weights: [0.30353495478630066, 0.3614932894706726, 0.33497175574302673]\n",
            "Updated weights: [0.3396713435649872, 0.33034661412239075, 0.32998204231262207]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 403\n",
            "  Loss for client 0: 1.2524\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 403\n",
            "  Loss for client 1: 1.6454\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 403\n",
            "  Loss for client 2: 1.5957\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 404\n",
            "  Client 0: total grad norm = 58.219092 (253 params)\n",
            "  Client 1: total grad norm = 85.718546 (253 params)\n",
            "  Client 2: total grad norm = 73.306020 (253 params)\n",
            "\n",
            "=== Optimizer Step 404 ===\n",
            "\n",
            "=== Updating Client Weights (Step 404) ===\n",
            "Gradient norms: [7.095773220062256, 9.254082679748535, 9.292736053466797]\n",
            "Target weights: [0.3952014744281769, 0.30302947759628296, 0.30176904797554016]\n",
            "Updated weights: [0.35633039474487305, 0.3221514821052551, 0.3215181529521942]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 404\n",
            "  Loss for client 0: 1.7593\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 404\n",
            "  Loss for client 1: 2.0911\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 404\n",
            "  Loss for client 2: 1.7563\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 405\n",
            "  Client 0: total grad norm = 75.677852 (253 params)\n",
            "  Client 1: total grad norm = 80.699912 (253 params)\n",
            "  Client 2: total grad norm = 66.030976 (253 params)\n",
            "\n",
            "=== Optimizer Step 405 ===\n",
            "\n",
            "=== Updating Client Weights (Step 405) ===\n",
            "Gradient norms: [8.59367847442627, 10.009991645812988, 7.375007152557373]\n",
            "Target weights: [0.33071479201316833, 0.2839219868183136, 0.38536322116851807]\n",
            "Updated weights: [0.34864571690559387, 0.31068265438079834, 0.3406716585159302]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 405\n",
            "  Loss for client 0: 0.9015\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 405\n",
            "  Loss for client 1: 1.5047\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 405\n",
            "  Loss for client 2: 1.7378\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 406\n",
            "  Client 0: total grad norm = 58.978730 (253 params)\n",
            "  Client 1: total grad norm = 73.133360 (253 params)\n",
            "  Client 2: total grad norm = 69.907439 (253 params)\n",
            "\n",
            "=== Optimizer Step 406 ===\n",
            "\n",
            "=== Updating Client Weights (Step 406) ===\n",
            "Gradient norms: [6.341513156890869, 9.16699504852295, 7.782639026641846]\n",
            "Target weights: [0.39894604682922363, 0.2759815454483032, 0.32507243752479553]\n",
            "Updated weights: [0.36373579502105713, 0.3002723157405853, 0.33599188923835754]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 406\n",
            "  Loss for client 0: 1.5023\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 406\n",
            "  Loss for client 1: 1.7451\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 406\n",
            "  Loss for client 2: 1.9133\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 407\n",
            "  Client 0: total grad norm = 68.519175 (253 params)\n",
            "  Client 1: total grad norm = 103.742364 (253 params)\n",
            "  Client 2: total grad norm = 87.307325 (253 params)\n",
            "\n",
            "=== Optimizer Step 407 ===\n",
            "\n",
            "=== Updating Client Weights (Step 407) ===\n",
            "Gradient norms: [7.024155616760254, 10.138578414916992, 10.916082382202148]\n",
            "Target weights: [0.4280303418636322, 0.2965456545352936, 0.2754240334033966]\n",
            "Updated weights: [0.3830241560935974, 0.2991543412208557, 0.31782153248786926]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 407\n",
            "  Loss for client 0: 1.4874\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 407\n",
            "  Loss for client 1: 1.9783\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 407\n",
            "  Loss for client 2: 1.5024\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 408\n",
            "  Client 0: total grad norm = 71.745651 (253 params)\n",
            "  Client 1: total grad norm = 96.058381 (253 params)\n",
            "  Client 2: total grad norm = 67.425287 (253 params)\n",
            "\n",
            "=== Optimizer Step 408 ===\n",
            "\n",
            "=== Updating Client Weights (Step 408) ===\n",
            "Gradient norms: [8.427699089050293, 9.820466041564941, 6.8016815185546875]\n",
            "Target weights: [0.322868287563324, 0.27707815170288086, 0.40005356073379517]\n",
            "Updated weights: [0.3649773895740509, 0.29253149032592773, 0.34249114990234375]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 408\n",
            "  Loss for client 0: 0.9950\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 408\n",
            "  Loss for client 1: 1.9644\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 408\n",
            "  Loss for client 2: 1.7644\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 409\n",
            "  Client 0: total grad norm = 57.399452 (253 params)\n",
            "  Client 1: total grad norm = 105.714650 (253 params)\n",
            "  Client 2: total grad norm = 89.921629 (253 params)\n",
            "\n",
            "=== Optimizer Step 409 ===\n",
            "\n",
            "=== Updating Client Weights (Step 409) ===\n",
            "Gradient norms: [6.324864864349365, 10.892426490783691, 10.25442123413086]\n",
            "Target weights: [0.4550707936286926, 0.2642442584037781, 0.2806848883628845]\n",
            "Updated weights: [0.39200541377067566, 0.28404533863067627, 0.32394927740097046]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 409\n",
            "  Loss for client 0: 1.2896\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 409\n",
            "  Loss for client 1: 2.1141\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 409\n",
            "  Loss for client 2: 1.7120\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 410\n",
            "  Client 0: total grad norm = 63.444766 (253 params)\n",
            "  Client 1: total grad norm = 87.354563 (253 params)\n",
            "  Client 2: total grad norm = 90.793387 (253 params)\n",
            "\n",
            "=== Optimizer Step 410 ===\n",
            "\n",
            "=== Updating Client Weights (Step 410) ===\n",
            "Gradient norms: [7.211122989654541, 9.282068252563477, 11.05146312713623]\n",
            "Target weights: [0.4116257131099701, 0.3197868764400482, 0.2685873806476593]\n",
            "Updated weights: [0.39789149165153503, 0.2947677969932556, 0.30734071135520935]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 410\n",
            "  Loss for client 0: 1.7451\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 410\n",
            "  Loss for client 1: 2.0627\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 410\n",
            "  Loss for client 2: 2.1531\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 411\n",
            "  Client 0: total grad norm = 76.309821 (253 params)\n",
            "  Client 1: total grad norm = 83.404451 (253 params)\n",
            "  Client 2: total grad norm = 124.797826 (253 params)\n",
            "\n",
            "=== Optimizer Step 411 ===\n",
            "\n",
            "=== Updating Client Weights (Step 411) ===\n",
            "Gradient norms: [8.453264236450195, 10.599669456481934, 10.910801887512207]\n",
            "Target weights: [0.38876262307167053, 0.310039222240448, 0.3011981248855591]\n",
            "Updated weights: [0.39515283703804016, 0.29934921860694885, 0.305497944355011]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 411\n",
            "  Loss for client 0: 2.0824\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 411\n",
            "  Loss for client 1: 1.2471\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 411\n",
            "  Loss for client 2: 1.5291\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 412\n",
            "  Client 0: total grad norm = 81.798724 (253 params)\n",
            "  Client 1: total grad norm = 75.362363 (253 params)\n",
            "  Client 2: total grad norm = 81.189492 (253 params)\n",
            "\n",
            "=== Optimizer Step 412 ===\n",
            "\n",
            "=== Updating Client Weights (Step 412) ===\n",
            "Gradient norms: [8.62240982055664, 7.684250831604004, 8.908534049987793]\n",
            "Target weights: [0.32362765073776245, 0.36313891410827637, 0.3132334053516388]\n",
            "Updated weights: [0.3736952841281891, 0.31848612427711487, 0.30781859159469604]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 412\n",
            "  Loss for client 0: 1.3303\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 412\n",
            "  Loss for client 1: 1.6404\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 412\n",
            "  Loss for client 2: 1.2668\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 413\n",
            "  Client 0: total grad norm = 120.760190 (253 params)\n",
            "  Client 1: total grad norm = 76.664330 (253 params)\n",
            "  Client 2: total grad norm = 73.829369 (253 params)\n",
            "\n",
            "=== Optimizer Step 413 ===\n",
            "\n",
            "=== Updating Client Weights (Step 413) ===\n",
            "Gradient norms: [11.874931335449219, 7.3725104331970215, 8.925728797912598]\n",
            "Target weights: [0.2537350058555603, 0.408691942691803, 0.3375730514526367]\n",
            "Updated weights: [0.3377072215080261, 0.3455478847026825, 0.31674495339393616]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 413\n",
            "  Loss for client 0: 1.7654\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 413\n",
            "  Loss for client 1: 1.3322\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 413\n",
            "  Loss for client 2: 1.4112\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 414\n",
            "  Client 0: total grad norm = 91.989158 (253 params)\n",
            "  Client 1: total grad norm = 69.406414 (253 params)\n",
            "  Client 2: total grad norm = 70.522930 (253 params)\n",
            "\n",
            "=== Optimizer Step 414 ===\n",
            "\n",
            "=== Updating Client Weights (Step 414) ===\n",
            "Gradient norms: [10.138357162475586, 7.5780930519104, 8.480051040649414]\n",
            "Target weights: [0.28301316499710083, 0.3786293864250183, 0.33835747838020325]\n",
            "Updated weights: [0.3212990164756775, 0.3554723262786865, 0.32322871685028076]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 414\n",
            "  Loss for client 0: 1.8130\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 414\n",
            "  Loss for client 1: 2.1181\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 414\n",
            "  Loss for client 2: 2.1517\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 415\n",
            "  Client 0: total grad norm = 72.380445 (253 params)\n",
            "  Client 1: total grad norm = 91.578999 (253 params)\n",
            "  Client 2: total grad norm = 81.081129 (253 params)\n",
            "\n",
            "=== Optimizer Step 415 ===\n",
            "\n",
            "=== Updating Client Weights (Step 415) ===\n",
            "Gradient norms: [8.453720092773438, 9.276575088500977, 8.43667221069336]\n",
            "Target weights: [0.34325122833251953, 0.3128039836883545, 0.34394481778144836]\n",
            "Updated weights: [0.32788464426994324, 0.34267181158065796, 0.3294435143470764]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 415\n",
            "  Loss for client 0: 2.1452\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 415\n",
            "  Loss for client 1: 1.5969\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 415\n",
            "  Loss for client 2: 1.9095\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 416\n",
            "  Client 0: total grad norm = 89.814274 (253 params)\n",
            "  Client 1: total grad norm = 68.523300 (253 params)\n",
            "  Client 2: total grad norm = 86.569241 (253 params)\n",
            "\n",
            "=== Optimizer Step 416 ===\n",
            "\n",
            "=== Updating Client Weights (Step 416) ===\n",
            "Gradient norms: [9.675406455993652, 7.780313968658447, 8.027979850769043]\n",
            "Target weights: [0.28995710611343384, 0.36058348417282104, 0.34945937991142273]\n",
            "Updated weights: [0.31650638580322266, 0.34804531931877136, 0.3354482650756836]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 416\n",
            "  Loss for client 0: 1.5932\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 416\n",
            "  Loss for client 1: 1.6104\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 416\n",
            "  Loss for client 2: 1.3388\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 417\n",
            "  Client 0: total grad norm = 81.158619 (253 params)\n",
            "  Client 1: total grad norm = 84.863154 (253 params)\n",
            "  Client 2: total grad norm = 80.336332 (253 params)\n",
            "\n",
            "=== Optimizer Step 417 ===\n",
            "\n",
            "=== Updating Client Weights (Step 417) ===\n",
            "Gradient norms: [9.631088256835938, 8.603402137756348, 7.12173318862915]\n",
            "Target weights: [0.2880347967147827, 0.32244089245796204, 0.38952434062957764]\n",
            "Updated weights: [0.30796492099761963, 0.3403639793395996, 0.35167109966278076]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 417\n",
            "  Loss for client 0: 0.8758\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 417\n",
            "  Loss for client 1: 1.4282\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 417\n",
            "  Loss for client 2: 1.9678\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 418\n",
            "  Client 0: total grad norm = 59.618918 (253 params)\n",
            "  Client 1: total grad norm = 73.855050 (253 params)\n",
            "  Client 2: total grad norm = 92.405785 (253 params)\n",
            "\n",
            "=== Optimizer Step 418 ===\n",
            "\n",
            "=== Updating Client Weights (Step 418) ===\n",
            "Gradient norms: [5.746433258056641, 7.883378028869629, 9.776193618774414]\n",
            "Target weights: [0.4316430687904358, 0.3146377205848694, 0.2537192106246948]\n",
            "Updated weights: [0.3450683653354645, 0.33264610171318054, 0.322285532951355]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 418\n",
            "  Loss for client 0: 1.5952\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 418\n",
            "  Loss for client 1: 2.0558\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 418\n",
            "  Loss for client 2: 2.1552\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 419\n",
            "  Client 0: total grad norm = 71.057774 (253 params)\n",
            "  Client 1: total grad norm = 78.741646 (253 params)\n",
            "  Client 2: total grad norm = 79.716408 (253 params)\n",
            "\n",
            "=== Optimizer Step 419 ===\n",
            "\n",
            "=== Updating Client Weights (Step 419) ===\n",
            "Gradient norms: [7.3946309089660645, 7.985760688781738, 8.626805305480957]\n",
            "Target weights: [0.35930559039115906, 0.3327087461948395, 0.3079856336116791]\n",
            "Updated weights: [0.3493395447731018, 0.3326649069786072, 0.317995548248291]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 419\n",
            "  Loss for client 0: 1.2470\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 419\n",
            "  Loss for client 1: 1.3715\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 419\n",
            "  Loss for client 2: 1.9848\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 420\n",
            "  Client 0: total grad norm = 71.373766 (253 params)\n",
            "  Client 1: total grad norm = 61.959712 (253 params)\n",
            "  Client 2: total grad norm = 88.734039 (253 params)\n",
            "\n",
            "=== Optimizer Step 420 ===\n",
            "\n",
            "=== Updating Client Weights (Step 420) ===\n",
            "Gradient norms: [9.141242980957031, 6.726028919219971, 10.23415756225586]\n",
            "Target weights: [0.3074752986431122, 0.4178849756717682, 0.274639755487442]\n",
            "Updated weights: [0.33678027987480164, 0.35823094844818115, 0.3049888014793396]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 420\n",
            "  Loss for client 0: 2.3245\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 420\n",
            "  Loss for client 1: 1.6062\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 420\n",
            "  Loss for client 2: 1.8248\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 421\n",
            "  Client 0: total grad norm = 95.583840 (253 params)\n",
            "  Client 1: total grad norm = 73.201865 (253 params)\n",
            "  Client 2: total grad norm = 71.370380 (253 params)\n",
            "\n",
            "=== Optimizer Step 421 ===\n",
            "\n",
            "=== Updating Client Weights (Step 421) ===\n",
            "Gradient norms: [11.797416687011719, 8.542317390441895, 9.087103843688965]\n",
            "Target weights: [0.2717897295951843, 0.3753567636013031, 0.35285353660583496]\n",
            "Updated weights: [0.31728309392929077, 0.3633686900138855, 0.31934821605682373]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 421\n",
            "  Loss for client 0: 1.8483\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 421\n",
            "  Loss for client 1: 2.1257\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 421\n",
            "  Loss for client 2: 1.6152\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 422\n",
            "  Client 0: total grad norm = 91.635250 (253 params)\n",
            "  Client 1: total grad norm = 103.488083 (253 params)\n",
            "  Client 2: total grad norm = 83.205034 (253 params)\n",
            "\n",
            "=== Optimizer Step 422 ===\n",
            "\n",
            "=== Updating Client Weights (Step 422) ===\n",
            "Gradient norms: [10.058623313903809, 9.323634147644043, 8.820330619812012]\n",
            "Target weights: [0.3106340765953064, 0.33512163162231445, 0.3542442321777344]\n",
            "Updated weights: [0.31528839468955994, 0.35489460825920105, 0.3298170566558838]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 422\n",
            "  Loss for client 0: 1.8188\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 422\n",
            "  Loss for client 1: 1.8382\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 422\n",
            "  Loss for client 2: 1.5666\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 423\n",
            "  Client 0: total grad norm = 76.406033 (253 params)\n",
            "  Client 1: total grad norm = 90.244880 (253 params)\n",
            "  Client 2: total grad norm = 69.514749 (253 params)\n",
            "\n",
            "=== Optimizer Step 423 ===\n",
            "\n",
            "=== Updating Client Weights (Step 423) ===\n",
            "Gradient norms: [7.8357014656066895, 9.182488441467285, 7.303123474121094]\n",
            "Target weights: [0.3417336344718933, 0.2916118800640106, 0.3666544556617737]\n",
            "Updated weights: [0.32322198152542114, 0.33590978384017944, 0.3408682644367218]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 423\n",
            "  Loss for client 0: 1.3022\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 423\n",
            "  Loss for client 1: 1.6928\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 423\n",
            "  Loss for client 2: 1.4670\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 424\n",
            "  Client 0: total grad norm = 62.855043 (253 params)\n",
            "  Client 1: total grad norm = 67.133170 (253 params)\n",
            "  Client 2: total grad norm = 78.630426 (253 params)\n",
            "\n",
            "=== Optimizer Step 424 ===\n",
            "\n",
            "=== Updating Client Weights (Step 424) ===\n",
            "Gradient norms: [7.035455226898193, 7.093135833740234, 8.25660228729248]\n",
            "Target weights: [0.3516213297843933, 0.34876197576522827, 0.2996166944503784]\n",
            "Updated weights: [0.3317417800426483, 0.3397654592990875, 0.32849279046058655]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 424\n",
            "  Loss for client 0: 1.2159\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 424\n",
            "  Loss for client 1: 1.7870\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 424\n",
            "  Loss for client 2: 2.1052\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 425\n",
            "  Client 0: total grad norm = 85.631384 (253 params)\n",
            "  Client 1: total grad norm = 69.464500 (253 params)\n",
            "  Client 2: total grad norm = 87.326497 (253 params)\n",
            "\n",
            "=== Optimizer Step 425 ===\n",
            "\n",
            "=== Updating Client Weights (Step 425) ===\n",
            "Gradient norms: [9.18539810180664, 7.491827011108398, 10.922259330749512]\n",
            "Target weights: [0.32604774832725525, 0.39975273609161377, 0.274199515581131]\n",
            "Updated weights: [0.3300335705280304, 0.3577616512775421, 0.3122048079967499]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 425\n",
            "  Loss for client 0: 1.2463\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 425\n",
            "  Loss for client 1: 2.0624\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 425\n",
            "  Loss for client 2: 1.9438\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 426\n",
            "  Client 0: total grad norm = 64.975459 (253 params)\n",
            "  Client 1: total grad norm = 90.358001 (253 params)\n",
            "  Client 2: total grad norm = 74.307352 (253 params)\n",
            "\n",
            "=== Optimizer Step 426 ===\n",
            "\n",
            "=== Updating Client Weights (Step 426) ===\n",
            "Gradient norms: [6.922643661499023, 10.313509941101074, 8.976943969726562]\n",
            "Target weights: [0.4094368815422058, 0.2748225927352905, 0.31574058532714844]\n",
            "Updated weights: [0.35385456681251526, 0.3328799307346344, 0.3132655620574951]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 426\n",
            "  Loss for client 0: 1.5733\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 426\n",
            "  Loss for client 1: 2.0963\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 426\n",
            "  Loss for client 2: 2.0018\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 427\n",
            "  Client 0: total grad norm = 81.465034 (253 params)\n",
            "  Client 1: total grad norm = 70.642890 (253 params)\n",
            "  Client 2: total grad norm = 88.439813 (253 params)\n",
            "\n",
            "=== Optimizer Step 427 ===\n",
            "\n",
            "=== Updating Client Weights (Step 427) ===\n",
            "Gradient norms: [8.967178344726562, 8.405096054077148, 9.600518226623535]\n",
            "Target weights: [0.3332328796386719, 0.3555174767971039, 0.311249703168869]\n",
            "Updated weights: [0.3476680517196655, 0.33967119455337524, 0.312660813331604]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 427\n",
            "  Loss for client 0: 1.4848\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 427\n",
            "  Loss for client 1: 1.4719\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 427\n",
            "  Loss for client 2: 1.8950\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 428\n",
            "  Client 0: total grad norm = 86.012915 (253 params)\n",
            "  Client 1: total grad norm = 62.539940 (253 params)\n",
            "  Client 2: total grad norm = 88.396861 (253 params)\n",
            "\n",
            "=== Optimizer Step 428 ===\n",
            "\n",
            "=== Updating Client Weights (Step 428) ===\n",
            "Gradient norms: [8.643034934997559, 7.343044281005859, 9.859739303588867]\n",
            "Target weights: [0.32747846841812134, 0.38545429706573486, 0.2870672345161438]\n",
            "Updated weights: [0.34161117672920227, 0.3534061312675476, 0.3049827218055725]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 428\n",
            "  Loss for client 0: 1.7174\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 428\n",
            "  Loss for client 1: 1.8004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 428\n",
            "  Loss for client 2: 1.8439\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 429\n",
            "  Client 0: total grad norm = 85.884583 (253 params)\n",
            "  Client 1: total grad norm = 71.399564 (253 params)\n",
            "  Client 2: total grad norm = 84.784638 (253 params)\n",
            "\n",
            "=== Optimizer Step 429 ===\n",
            "\n",
            "=== Updating Client Weights (Step 429) ===\n",
            "Gradient norms: [8.401015281677246, 7.201550483703613, 8.27561092376709]\n",
            "Target weights: [0.3142963945865631, 0.3666445016860962, 0.3190591037273407]\n",
            "Updated weights: [0.33341673016548157, 0.35737764835357666, 0.30920565128326416]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 429\n",
            "  Loss for client 0: 1.7969\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 429\n",
            "  Loss for client 1: 1.7448\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 429\n",
            "  Loss for client 2: 1.8804\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 430\n",
            "  Client 0: total grad norm = 91.690373 (253 params)\n",
            "  Client 1: total grad norm = 83.030161 (253 params)\n",
            "  Client 2: total grad norm = 77.527320 (253 params)\n",
            "\n",
            "=== Optimizer Step 430 ===\n",
            "\n",
            "=== Updating Client Weights (Step 430) ===\n",
            "Gradient norms: [9.927742958068848, 9.358442306518555, 9.263529777526855]\n",
            "Target weights: [0.3192302882671356, 0.3386499881744385, 0.3421197235584259]\n",
            "Updated weights: [0.32916080951690674, 0.3517593741416931, 0.3190798759460449]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 430\n",
            "  Loss for client 0: 1.8609\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 430\n",
            "  Loss for client 1: 1.9628\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 430\n",
            "  Loss for client 2: 1.8170\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 431\n",
            "  Client 0: total grad norm = 73.217521 (253 params)\n",
            "  Client 1: total grad norm = 76.230632 (253 params)\n",
            "  Client 2: total grad norm = 71.298638 (253 params)\n",
            "\n",
            "=== Optimizer Step 431 ===\n",
            "\n",
            "=== Updating Client Weights (Step 431) ===\n",
            "Gradient norms: [8.246033668518066, 8.974627494812012, 8.860076904296875]\n",
            "Target weights: [0.3509373068809509, 0.32244691252708435, 0.3266157805919647]\n",
            "Updated weights: [0.33569374680519104, 0.34296563267707825, 0.3213406503200531]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 431\n",
            "  Loss for client 0: 1.7477\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 431\n",
            "  Loss for client 1: 1.7786\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 431\n",
            "  Loss for client 2: 1.9502\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 432\n",
            "  Client 0: total grad norm = 96.640342 (253 params)\n",
            "  Client 1: total grad norm = 72.796294 (253 params)\n",
            "  Client 2: total grad norm = 94.743148 (253 params)\n",
            "\n",
            "=== Optimizer Step 432 ===\n",
            "\n",
            "=== Updating Client Weights (Step 432) ===\n",
            "Gradient norms: [9.930643081665039, 8.443731307983398, 8.209476470947266]\n",
            "Target weights: [0.29535531997680664, 0.3473663628101349, 0.35727834701538086]\n",
            "Updated weights: [0.3235922157764435, 0.34428584575653076, 0.33212196826934814]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 432\n",
            "  Loss for client 0: 1.4549\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 432\n",
            "  Loss for client 1: 2.2388\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 432\n",
            "  Loss for client 2: 1.7552\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 433\n",
            "  Client 0: total grad norm = 108.378776 (253 params)\n",
            "  Client 1: total grad norm = 84.325959 (253 params)\n",
            "  Client 2: total grad norm = 71.348552 (253 params)\n",
            "\n",
            "=== Optimizer Step 433 ===\n",
            "\n",
            "=== Updating Client Weights (Step 433) ===\n",
            "Gradient norms: [12.673505783081055, 9.0118408203125, 7.986456394195557]\n",
            "Target weights: [0.2504262328147888, 0.352178692817688, 0.3973950743675232]\n",
            "Updated weights: [0.30164241790771484, 0.34665369987487793, 0.3517038822174072]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 433\n",
            "  Loss for client 0: 0.9001\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 433\n",
            "  Loss for client 1: 1.6071\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 433\n",
            "  Loss for client 2: 1.3085\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 434\n",
            "  Client 0: total grad norm = 56.654701 (253 params)\n",
            "  Client 1: total grad norm = 70.294416 (253 params)\n",
            "  Client 2: total grad norm = 69.755842 (253 params)\n",
            "\n",
            "=== Optimizer Step 434 ===\n",
            "\n",
            "=== Updating Client Weights (Step 434) ===\n",
            "Gradient norms: [6.857174396514893, 7.305386543273926, 6.606125831604004]\n",
            "Target weights: [0.33594828844070435, 0.3153366446495056, 0.3487151265144348]\n",
            "Updated weights: [0.3119341731071472, 0.33725857734680176, 0.350807249546051]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 434\n",
            "  Loss for client 0: 1.4835\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 434\n",
            "  Loss for client 1: 1.5851\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 434\n",
            "  Loss for client 2: 1.4622\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 435\n",
            "  Client 0: total grad norm = 71.333108 (253 params)\n",
            "  Client 1: total grad norm = 68.701628 (253 params)\n",
            "  Client 2: total grad norm = 70.030361 (253 params)\n",
            "\n",
            "=== Optimizer Step 435 ===\n",
            "\n",
            "=== Updating Client Weights (Step 435) ===\n",
            "Gradient norms: [7.672787189483643, 7.666856288909912, 8.506964683532715]\n",
            "Target weights: [0.3445050120353699, 0.34477153420448303, 0.3107234537601471]\n",
            "Updated weights: [0.3217054307460785, 0.3395124673843384, 0.33878210186958313]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 435\n",
            "  Loss for client 0: 1.1723\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 435\n",
            "  Loss for client 1: 1.8988\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 435\n",
            "  Loss for client 2: 1.4403\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 436\n",
            "  Client 0: total grad norm = 73.481111 (253 params)\n",
            "  Client 1: total grad norm = 94.191950 (253 params)\n",
            "  Client 2: total grad norm = 63.832173 (253 params)\n",
            "\n",
            "=== Optimizer Step 436 ===\n",
            "\n",
            "=== Updating Client Weights (Step 436) ===\n",
            "Gradient norms: [8.988163948059082, 8.433584213256836, 7.619377136230469]\n",
            "Target weights: [0.3081280589103699, 0.32839009165763855, 0.3634818494319916]\n",
            "Updated weights: [0.3176322281360626, 0.33617573976516724, 0.34619203209877014]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 436\n",
            "  Loss for client 0: 1.5164\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 436\n",
            "  Loss for client 1: 1.6134\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 436\n",
            "  Loss for client 2: 1.5496\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 437\n",
            "  Client 0: total grad norm = 72.790688 (253 params)\n",
            "  Client 1: total grad norm = 74.312916 (253 params)\n",
            "  Client 2: total grad norm = 82.114141 (253 params)\n",
            "\n",
            "=== Optimizer Step 437 ===\n",
            "\n",
            "=== Updating Client Weights (Step 437) ===\n",
            "Gradient norms: [8.028609275817871, 8.0884370803833, 7.1094231605529785]\n",
            "Target weights: [0.320318341255188, 0.3179490268230438, 0.36173269152641296]\n",
            "Updated weights: [0.3184380531311035, 0.33070772886276245, 0.3508542478084564]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 437\n",
            "  Loss for client 0: 1.5195\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 437\n",
            "  Loss for client 1: 1.6388\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 437\n",
            "  Loss for client 2: 1.8302\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 438\n",
            "  Client 0: total grad norm = 83.795381 (253 params)\n",
            "  Client 1: total grad norm = 76.333537 (253 params)\n",
            "  Client 2: total grad norm = 77.000736 (253 params)\n",
            "\n",
            "=== Optimizer Step 438 ===\n",
            "\n",
            "=== Updating Client Weights (Step 438) ===\n",
            "Gradient norms: [9.211448669433594, 9.950136184692383, 8.284677505493164]\n",
            "Target weights: [0.32920435070991516, 0.3047645688056946, 0.3660310208797455]\n",
            "Updated weights: [0.32166793942451477, 0.32292479276657104, 0.3554072976112366]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 438\n",
            "  Loss for client 0: 1.7907\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 438\n",
            "  Loss for client 1: 1.6367\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 438\n",
            "  Loss for client 2: 1.7403\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 439\n",
            "  Client 0: total grad norm = 105.364036 (253 params)\n",
            "  Client 1: total grad norm = 76.912408 (253 params)\n",
            "  Client 2: total grad norm = 87.172310 (253 params)\n",
            "\n",
            "=== Optimizer Step 439 ===\n",
            "\n",
            "=== Updating Client Weights (Step 439) ===\n",
            "Gradient norms: [10.051595687866211, 7.910641670227051, 8.252094268798828]\n",
            "Target weights: [0.2866390347480774, 0.3642156720161438, 0.3491452932357788]\n",
            "Updated weights: [0.31115925312042236, 0.3353120684623718, 0.3535287082195282]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 439\n",
            "  Loss for client 0: 1.1066\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 439\n",
            "  Loss for client 1: 1.9153\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 439\n",
            "  Loss for client 2: 1.4624\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 440\n",
            "  Client 0: total grad norm = 73.309893 (253 params)\n",
            "  Client 1: total grad norm = 87.264224 (253 params)\n",
            "  Client 2: total grad norm = 68.416995 (253 params)\n",
            "\n",
            "=== Optimizer Step 440 ===\n",
            "\n",
            "=== Updating Client Weights (Step 440) ===\n",
            "Gradient norms: [6.088284492492676, 8.890883445739746, 8.318572044372559]\n",
            "Target weights: [0.4137927293777466, 0.28335630893707275, 0.30285099148750305]\n",
            "Updated weights: [0.34194931387901306, 0.3197253346443176, 0.3383253812789917]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 440\n",
            "  Loss for client 0: 1.6823\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 440\n",
            "  Loss for client 1: 1.5475\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 440\n",
            "  Loss for client 2: 1.4926\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 441\n",
            "  Client 0: total grad norm = 82.611105 (253 params)\n",
            "  Client 1: total grad norm = 69.982180 (253 params)\n",
            "  Client 2: total grad norm = 78.396820 (253 params)\n",
            "\n",
            "=== Optimizer Step 441 ===\n",
            "\n",
            "=== Updating Client Weights (Step 441) ===\n",
            "Gradient norms: [8.122920989990234, 7.0215840339660645, 8.298754692077637]\n",
            "Target weights: [0.31891191005706787, 0.3689333200454712, 0.3121547996997833]\n",
            "Updated weights: [0.33503809571266174, 0.3344877362251282, 0.33047419786453247]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 441\n",
            "  Loss for client 0: 1.8713\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 441\n",
            "  Loss for client 1: 1.8219\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 441\n",
            "  Loss for client 2: 1.6080\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 442\n",
            "  Client 0: total grad norm = 93.014305 (253 params)\n",
            "  Client 1: total grad norm = 74.052856 (253 params)\n",
            "  Client 2: total grad norm = 72.761243 (253 params)\n",
            "\n",
            "=== Optimizer Step 442 ===\n",
            "\n",
            "=== Updating Client Weights (Step 442) ===\n",
            "Gradient norms: [9.079519271850586, 8.195487976074219, 8.409991264343262]\n",
            "Target weights: [0.31372755765914917, 0.34756872057914734, 0.3387037217617035]\n",
            "Updated weights: [0.32864493131637573, 0.3384120464324951, 0.33294305205345154]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 442\n",
            "  Loss for client 0: 1.7342\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 442\n",
            "  Loss for client 1: 1.8251\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 442\n",
            "  Loss for client 2: 2.0186\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 443\n",
            "  Client 0: total grad norm = 81.577215 (253 params)\n",
            "  Client 1: total grad norm = 90.087523 (253 params)\n",
            "  Client 2: total grad norm = 96.878774 (253 params)\n",
            "\n",
            "=== Optimizer Step 443 ===\n",
            "\n",
            "=== Updating Client Weights (Step 443) ===\n",
            "Gradient norms: [8.945144653320312, 9.81447982788086, 9.768635749816895]\n",
            "Target weights: [0.3537164032459259, 0.32238534092903137, 0.3238982856273651]\n",
            "Updated weights: [0.3361663818359375, 0.33360403776168823, 0.33022964000701904]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 443\n",
            "  Loss for client 0: 1.1366\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 443\n",
            "  Loss for client 1: 2.1049\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 443\n",
            "  Loss for client 2: 1.5772\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 444\n",
            "  Client 0: total grad norm = 65.234751 (253 params)\n",
            "  Client 1: total grad norm = 82.006587 (253 params)\n",
            "  Client 2: total grad norm = 72.988129 (253 params)\n",
            "\n",
            "=== Optimizer Step 444 ===\n",
            "\n",
            "=== Updating Client Weights (Step 444) ===\n",
            "Gradient norms: [7.3675856590271, 8.90305233001709, 8.006949424743652]\n",
            "Target weights: [0.3639428913593292, 0.30117541551589966, 0.33488166332244873]\n",
            "Updated weights: [0.3444993495941162, 0.32387542724609375, 0.3316252529621124]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 444\n",
            "  Loss for client 0: 1.7752\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 444\n",
            "  Loss for client 1: 1.7072\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 444\n",
            "  Loss for client 2: 0.9477\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 445\n",
            "  Client 0: total grad norm = 88.071537 (253 params)\n",
            "  Client 1: total grad norm = 103.428406 (253 params)\n",
            "  Client 2: total grad norm = 48.079848 (253 params)\n",
            "\n",
            "=== Optimizer Step 445 ===\n",
            "\n",
            "=== Updating Client Weights (Step 445) ===\n",
            "Gradient norms: [9.58788013458252, 8.961477279663086, 5.701882362365723]\n",
            "Target weights: [0.26656514406204224, 0.285197913646698, 0.44823700189590454]\n",
            "Updated weights: [0.3211190700531006, 0.3122721314430237, 0.36660876870155334]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 445\n",
            "  Loss for client 0: 0.4788\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 445\n",
            "  Loss for client 1: 2.3606\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 445\n",
            "  Loss for client 2: 1.8931\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 446\n",
            "  Client 0: total grad norm = 50.483445 (253 params)\n",
            "  Client 1: total grad norm = 87.978881 (253 params)\n",
            "  Client 2: total grad norm = 92.047335 (253 params)\n",
            "\n",
            "=== Optimizer Step 446 ===\n",
            "\n",
            "=== Updating Client Weights (Step 446) ===\n",
            "Gradient norms: [6.093926429748535, 10.335351943969727, 12.094809532165527]\n",
            "Target weights: [0.47767671942710876, 0.28164756298065186, 0.240675687789917]\n",
            "Updated weights: [0.3680863678455353, 0.30308476090431213, 0.3288288414478302]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 446\n",
            "  Loss for client 0: 0.9990\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 446\n",
            "  Loss for client 1: 1.6749\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 446\n",
            "  Loss for client 2: 2.0696\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 447\n",
            "  Client 0: total grad norm = 60.807409 (253 params)\n",
            "  Client 1: total grad norm = 84.687084 (253 params)\n",
            "  Client 2: total grad norm = 86.593332 (253 params)\n",
            "\n",
            "=== Optimizer Step 447 ===\n",
            "\n",
            "=== Updating Client Weights (Step 447) ===\n",
            "Gradient norms: [6.362582683563232, 8.812686920166016, 10.277670860290527]\n",
            "Target weights: [0.42715904116630554, 0.3084002435207367, 0.26444074511528015]\n",
            "Updated weights: [0.38580816984176636, 0.30467939376831055, 0.3095124065876007]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 447\n",
            "  Loss for client 0: 1.2464\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 447\n",
            "  Loss for client 1: 1.5295\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 447\n",
            "  Loss for client 2: 1.5259\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 448\n",
            "  Client 0: total grad norm = 71.226205 (253 params)\n",
            "  Client 1: total grad norm = 76.734791 (253 params)\n",
            "  Client 2: total grad norm = 69.669898 (253 params)\n",
            "\n",
            "=== Optimizer Step 448 ===\n",
            "\n",
            "=== Updating Client Weights (Step 448) ===\n",
            "Gradient norms: [6.736082077026367, 8.211610794067383, 7.384390354156494]\n",
            "Target weights: [0.3659628927707672, 0.30020371079444885, 0.33383339643478394]\n",
            "Updated weights: [0.37985461950302124, 0.3033367097377777, 0.3168087303638458]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 448\n",
            "  Loss for client 0: 1.0023\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 448\n",
            "  Loss for client 1: 1.5911\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 448\n",
            "  Loss for client 2: 1.7712\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 449\n",
            "  Client 0: total grad norm = 54.719203 (253 params)\n",
            "  Client 1: total grad norm = 72.645425 (253 params)\n",
            "  Client 2: total grad norm = 71.185192 (253 params)\n",
            "\n",
            "=== Optimizer Step 449 ===\n",
            "\n",
            "=== Updating Client Weights (Step 449) ===\n",
            "Gradient norms: [5.962027549743652, 8.522760391235352, 8.768245697021484]\n",
            "Target weights: [0.4202566146850586, 0.29398709535598755, 0.28575631976127625]\n",
            "Updated weights: [0.3919752240180969, 0.300531804561615, 0.30749303102493286]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 449\n",
            "  Loss for client 0: 0.8286\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 449\n",
            "  Loss for client 1: 1.5821\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 449\n",
            "  Loss for client 2: 1.6171\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 450\n",
            "  Client 0: total grad norm = 86.800358 (253 params)\n",
            "  Client 1: total grad norm = 68.891626 (253 params)\n",
            "  Client 2: total grad norm = 88.677855 (253 params)\n",
            "\n",
            "=== Optimizer Step 450 ===\n",
            "\n",
            "=== Updating Client Weights (Step 450) ===\n",
            "Gradient norms: [8.632811546325684, 7.538458824157715, 8.733036041259766]\n",
            "Target weights: [0.3191123306751251, 0.3654375970363617, 0.3154500126838684]\n",
            "Updated weights: [0.37011635303497314, 0.32000353932380676, 0.3098801374435425]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 450\n",
            "  Val Loss = 1.5545 (5 batches)\n",
            "\n",
            "[Processing] Client 0, batch 450\n",
            "  Loss for client 0: 0.9144\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 450\n",
            "  Loss for client 1: 2.0124\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 450\n",
            "  Loss for client 2: 2.3800\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 451\n",
            "  Client 0: total grad norm = 74.503967 (253 params)\n",
            "  Client 1: total grad norm = 74.564757 (253 params)\n",
            "  Client 2: total grad norm = 87.655676 (253 params)\n",
            "\n",
            "=== Optimizer Step 451 ===\n",
            "\n",
            "=== Updating Client Weights (Step 451) ===\n",
            "Gradient norms: [7.39151668548584, 8.230916023254395, 9.529804229736328]\n",
            "Target weights: [0.374021977186203, 0.33587872982025146, 0.29009929299354553]\n",
            "Updated weights: [0.3712880611419678, 0.3247660994529724, 0.3039458990097046]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 451\n",
            "  Loss for client 0: 0.9594\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 451\n",
            "  Loss for client 1: 1.9483\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 451\n",
            "  Loss for client 2: 1.3459\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 452\n",
            "  Client 0: total grad norm = 66.629163 (253 params)\n",
            "  Client 1: total grad norm = 96.861182 (253 params)\n",
            "  Client 2: total grad norm = 71.319222 (253 params)\n",
            "\n",
            "=== Optimizer Step 452 ===\n",
            "\n",
            "=== Updating Client Weights (Step 452) ===\n",
            "Gradient norms: [7.866840839385986, 9.357651710510254, 7.433648586273193]\n",
            "Target weights: [0.3449513018131256, 0.2899955213069916, 0.3650531470775604]\n",
            "Updated weights: [0.3633870482444763, 0.3143349289894104, 0.32227808237075806]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 452\n",
            "  Loss for client 0: 1.6497\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 452\n",
            "  Loss for client 1: 2.1962\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 452\n",
            "  Loss for client 2: 1.5621\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 453\n",
            "  Client 0: total grad norm = 80.451031 (253 params)\n",
            "  Client 1: total grad norm = 88.343001 (253 params)\n",
            "  Client 2: total grad norm = 69.685600 (253 params)\n",
            "\n",
            "=== Optimizer Step 453 ===\n",
            "\n",
            "=== Updating Client Weights (Step 453) ===\n",
            "Gradient norms: [8.21274185180664, 9.805497169494629, 7.222445487976074]\n",
            "Target weights: [0.3361707627773285, 0.281564861536026, 0.3822643756866455]\n",
            "Updated weights: [0.3552221655845642, 0.3045039176940918, 0.34027397632598877]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 453\n",
            "  Loss for client 0: 1.7505\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 453\n",
            "  Loss for client 1: 1.3031\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 453\n",
            "  Loss for client 2: 1.7920\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 454\n",
            "  Client 0: total grad norm = 92.936853 (253 params)\n",
            "  Client 1: total grad norm = 63.652471 (253 params)\n",
            "  Client 2: total grad norm = 91.265606 (253 params)\n",
            "\n",
            "=== Optimizer Step 454 ===\n",
            "\n",
            "=== Updating Client Weights (Step 454) ===\n",
            "Gradient norms: [8.190860748291016, 7.122319221496582, 8.978494644165039]\n",
            "Target weights: [0.3265516757965088, 0.3755432963371277, 0.2979050874710083]\n",
            "Updated weights: [0.346621036529541, 0.32581573724746704, 0.3275633156299591]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 454\n",
            "  Loss for client 0: 1.6862\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 454\n",
            "  Loss for client 1: 1.4161\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 454\n",
            "  Loss for client 2: 1.3446\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 455\n",
            "  Client 0: total grad norm = 84.446575 (253 params)\n",
            "  Client 1: total grad norm = 83.555333 (253 params)\n",
            "  Client 2: total grad norm = 66.117290 (253 params)\n",
            "\n",
            "=== Optimizer Step 455 ===\n",
            "\n",
            "=== Updating Client Weights (Step 455) ===\n",
            "Gradient norms: [9.434825897216797, 7.480887413024902, 7.049002170562744]\n",
            "Target weights: [0.27780452370643616, 0.3503645062446594, 0.3718309700489044]\n",
            "Updated weights: [0.32597607374191284, 0.33318036794662476, 0.3408436179161072]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 455\n",
            "  Loss for client 0: 2.0565\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 455\n",
            "  Loss for client 1: 1.4311\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 455\n",
            "  Loss for client 2: 1.8984\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 456\n",
            "  Client 0: total grad norm = 89.881911 (253 params)\n",
            "  Client 1: total grad norm = 79.472597 (253 params)\n",
            "  Client 2: total grad norm = 86.401267 (253 params)\n",
            "\n",
            "=== Optimizer Step 456 ===\n",
            "\n",
            "=== Updating Client Weights (Step 456) ===\n",
            "Gradient norms: [8.8855562210083, 6.963552474975586, 8.197990417480469]\n",
            "Target weights: [0.29762977361679077, 0.37977826595306396, 0.32259199023246765]\n",
            "Updated weights: [0.3174721896648407, 0.347159743309021, 0.3353681266307831]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 456\n",
            "  Loss for client 0: 1.5112\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 456\n",
            "  Loss for client 1: 1.9286\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 456\n",
            "  Loss for client 2: 1.5780\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 457\n",
            "  Client 0: total grad norm = 80.229254 (253 params)\n",
            "  Client 1: total grad norm = 84.365459 (253 params)\n",
            "  Client 2: total grad norm = 78.579214 (253 params)\n",
            "\n",
            "=== Optimizer Step 457 ===\n",
            "\n",
            "=== Updating Client Weights (Step 457) ===\n",
            "Gradient norms: [8.18786334991455, 8.654735565185547, 8.364084243774414]\n",
            "Target weights: [0.3418818414211273, 0.3234393298625946, 0.3346787989139557]\n",
            "Updated weights: [0.32479506731033325, 0.3400436043739319, 0.33516132831573486]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 457\n",
            "  Loss for client 0: 1.7716\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 457\n",
            "  Loss for client 1: 2.5367\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 457\n",
            "  Loss for client 2: 1.2467\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 458\n",
            "  Client 0: total grad norm = 80.266772 (253 params)\n",
            "  Client 1: total grad norm = 118.559538 (253 params)\n",
            "  Client 2: total grad norm = 67.961324 (253 params)\n",
            "\n",
            "=== Optimizer Step 458 ===\n",
            "\n",
            "=== Updating Client Weights (Step 458) ===\n",
            "Gradient norms: [8.354230880737305, 15.000993728637695, 6.597419738769531]\n",
            "Target weights: [0.3542080521583557, 0.19726265966892242, 0.4485292434692383]\n",
            "Updated weights: [0.33361896872520447, 0.29720932245254517, 0.36917170882225037]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 458\n",
            "  Loss for client 0: 1.4683\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 458\n",
            "  Loss for client 1: 1.6579\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 458\n",
            "  Loss for client 2: 1.8698\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 459\n",
            "  Client 0: total grad norm = 79.084717 (253 params)\n",
            "  Client 1: total grad norm = 83.366125 (253 params)\n",
            "  Client 2: total grad norm = 83.056456 (253 params)\n",
            "\n",
            "=== Optimizer Step 459 ===\n",
            "\n",
            "=== Updating Client Weights (Step 459) ===\n",
            "Gradient norms: [9.529595375061035, 8.57790756225586, 9.310908317565918]\n",
            "Target weights: [0.31903690099716187, 0.3544328808784485, 0.32653018832206726]\n",
            "Updated weights: [0.32924434542655945, 0.3143763840198517, 0.35637927055358887]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 459\n",
            "  Loss for client 0: 2.1122\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 459\n",
            "  Loss for client 1: 2.0587\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 459\n",
            "  Loss for client 2: 2.1504\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 460\n",
            "  Client 0: total grad norm = 87.434617 (253 params)\n",
            "  Client 1: total grad norm = 103.872858 (253 params)\n",
            "  Client 2: total grad norm = 99.592715 (253 params)\n",
            "\n",
            "=== Optimizer Step 460 ===\n",
            "\n",
            "=== Updating Client Weights (Step 460) ===\n",
            "Gradient norms: [10.35457706451416, 9.482068061828613, 9.427661895751953]\n",
            "Target weights: [0.31344667077064514, 0.342289000749588, 0.34426432847976685]\n",
            "Updated weights: [0.3245050609111786, 0.32275015115737915, 0.35274478793144226]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 460\n",
            "  Loss for client 0: 1.9934\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 460\n",
            "  Loss for client 1: 0.8963\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 460\n",
            "  Loss for client 2: 1.1692\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 461\n",
            "  Client 0: total grad norm = 82.088554 (253 params)\n",
            "  Client 1: total grad norm = 64.683222 (253 params)\n",
            "  Client 2: total grad norm = 57.148114 (253 params)\n",
            "\n",
            "=== Optimizer Step 461 ===\n",
            "\n",
            "=== Updating Client Weights (Step 461) ===\n",
            "Gradient norms: [7.782939910888672, 7.002467155456543, 5.615955352783203]\n",
            "Target weights: [0.2859332859516144, 0.3178025186061859, 0.39626410603523254]\n",
            "Updated weights: [0.3129335343837738, 0.3212658762931824, 0.3658005893230438]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 461\n",
            "  Loss for client 0: 1.8543\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 461\n",
            "  Loss for client 1: 2.0215\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 461\n",
            "  Loss for client 2: 1.9722\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 462\n",
            "  Client 0: total grad norm = 102.682616 (253 params)\n",
            "  Client 1: total grad norm = 83.534991 (253 params)\n",
            "  Client 2: total grad norm = 86.414508 (253 params)\n",
            "\n",
            "=== Optimizer Step 462 ===\n",
            "\n",
            "=== Updating Client Weights (Step 462) ===\n",
            "Gradient norms: [12.243965148925781, 7.761185646057129, 9.45761489868164]\n",
            "Target weights: [0.2582508623600006, 0.4074138402938843, 0.3343353271484375]\n",
            "Updated weights: [0.29652875661849976, 0.3471102714538574, 0.3563610315322876]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 462\n",
            "  Loss for client 0: 1.3499\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 462\n",
            "  Loss for client 1: 1.5612\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 462\n",
            "  Loss for client 2: 1.3153\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 463\n",
            "  Client 0: total grad norm = 83.681815 (253 params)\n",
            "  Client 1: total grad norm = 70.824204 (253 params)\n",
            "  Client 2: total grad norm = 69.079753 (253 params)\n",
            "\n",
            "=== Optimizer Step 463 ===\n",
            "\n",
            "=== Updating Client Weights (Step 463) ===\n",
            "Gradient norms: [8.863842964172363, 7.69599723815918, 7.024990558624268]\n",
            "Target weights: [0.2929539382457733, 0.3374088704586029, 0.3696371912956238]\n",
            "Updated weights: [0.29545629024505615, 0.3441998362541199, 0.360343873500824]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 463\n",
            "  Loss for client 0: 0.7172\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 463\n",
            "  Loss for client 1: 1.7945\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 463\n",
            "  Loss for client 2: 1.5575\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 464\n",
            "  Client 0: total grad norm = 52.118293 (253 params)\n",
            "  Client 1: total grad norm = 69.448333 (253 params)\n",
            "  Client 2: total grad norm = 82.362686 (253 params)\n",
            "\n",
            "=== Optimizer Step 464 ===\n",
            "\n",
            "=== Updating Client Weights (Step 464) ===\n",
            "Gradient norms: [6.136736869812012, 8.28653621673584, 8.223949432373047]\n",
            "Target weights: [0.4021280109882355, 0.2978028357028961, 0.3000691831111908]\n",
            "Updated weights: [0.32745781540870667, 0.33028072118759155, 0.34226149320602417]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 464\n",
            "  Loss for client 0: 1.7083\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 464\n",
            "  Loss for client 1: 1.8133\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 464\n",
            "  Loss for client 2: 1.5744\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 465\n",
            "  Client 0: total grad norm = 88.102803 (253 params)\n",
            "  Client 1: total grad norm = 91.538802 (253 params)\n",
            "  Client 2: total grad norm = 73.163550 (253 params)\n",
            "\n",
            "=== Optimizer Step 465 ===\n",
            "\n",
            "=== Updating Client Weights (Step 465) ===\n",
            "Gradient norms: [10.236136436462402, 8.008357048034668, 7.95155668258667]\n",
            "Target weights: [0.2804660201072693, 0.3584865927696228, 0.3610473573207855]\n",
            "Updated weights: [0.3133602738380432, 0.3387424945831299, 0.3478972613811493]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 465\n",
            "  Loss for client 0: 1.8803\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 465\n",
            "  Loss for client 1: 1.8096\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 465\n",
            "  Loss for client 2: 2.0872\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 466\n",
            "  Client 0: total grad norm = 89.191975 (253 params)\n",
            "  Client 1: total grad norm = 70.607074 (253 params)\n",
            "  Client 2: total grad norm = 82.598107 (253 params)\n",
            "\n",
            "=== Optimizer Step 466 ===\n",
            "\n",
            "=== Updating Client Weights (Step 466) ===\n",
            "Gradient norms: [10.00653076171875, 7.602465629577637, 8.479764938354492]\n",
            "Target weights: [0.28601911664009094, 0.3764646053314209, 0.33751633763313293]\n",
            "Updated weights: [0.30515792965888977, 0.3500591516494751, 0.3447829782962799]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 466\n",
            "  Loss for client 0: 1.2228\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 466\n",
            "  Loss for client 1: 2.0513\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 466\n",
            "  Loss for client 2: 1.6216\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 467\n",
            "  Client 0: total grad norm = 65.643118 (253 params)\n",
            "  Client 1: total grad norm = 80.325362 (253 params)\n",
            "  Client 2: total grad norm = 67.818544 (253 params)\n",
            "\n",
            "=== Optimizer Step 467 ===\n",
            "\n",
            "=== Updating Client Weights (Step 467) ===\n",
            "Gradient norms: [7.246053695678711, 10.095446586608887, 8.076452255249023]\n",
            "Target weights: [0.38241833448410034, 0.2744825482368469, 0.3430991470813751]\n",
            "Updated weights: [0.32833606004714966, 0.32738617062568665, 0.34427782893180847]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 467\n",
            "  Loss for client 0: 1.9079\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 467\n",
            "  Loss for client 1: 2.0093\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 467\n",
            "  Loss for client 2: 2.1198\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 468\n",
            "  Client 0: total grad norm = 90.951514 (253 params)\n",
            "  Client 1: total grad norm = 87.168372 (253 params)\n",
            "  Client 2: total grad norm = 87.959455 (253 params)\n",
            "\n",
            "=== Optimizer Step 468 ===\n",
            "\n",
            "=== Updating Client Weights (Step 468) ===\n",
            "Gradient norms: [10.419565200805664, 9.263896942138672, 11.197789192199707]\n",
            "Target weights: [0.32730528712272644, 0.3681364953517914, 0.3045582175254822]\n",
            "Updated weights: [0.32802683115005493, 0.339611291885376, 0.33236193656921387]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 468\n",
            "  Loss for client 0: 0.6524\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 468\n",
            "  Loss for client 1: 1.5040\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 468\n",
            "  Loss for client 2: 1.8185\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 469\n",
            "  Client 0: total grad norm = 52.722372 (253 params)\n",
            "  Client 1: total grad norm = 64.603376 (253 params)\n",
            "  Client 2: total grad norm = 84.316078 (253 params)\n",
            "\n",
            "=== Optimizer Step 469 ===\n",
            "\n",
            "=== Updating Client Weights (Step 469) ===\n",
            "Gradient norms: [5.795046329498291, 7.264137268066406, 8.90607738494873]\n",
            "Target weights: [0.4084223806858063, 0.32582351565361023, 0.2657541334629059]\n",
            "Updated weights: [0.3521454930305481, 0.33547496795654297, 0.3123795986175537]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 469\n",
            "  Loss for client 0: 0.8511\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 469\n",
            "  Loss for client 1: 3.2840\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 469\n",
            "  Loss for client 2: 2.1708\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 470\n",
            "  Client 0: total grad norm = 54.648112 (253 params)\n",
            "  Client 1: total grad norm = 133.514338 (253 params)\n",
            "  Client 2: total grad norm = 84.631804 (253 params)\n",
            "\n",
            "=== Optimizer Step 470 ===\n",
            "\n",
            "=== Updating Client Weights (Step 470) ===\n",
            "Gradient norms: [6.538390636444092, 13.197735786437988, 10.225198745727539]\n",
            "Target weights: [0.4684155285358429, 0.23206129670143127, 0.29952314496040344]\n",
            "Updated weights: [0.38702651858329773, 0.3044508695602417, 0.30852267146110535]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 470\n",
            "  Loss for client 0: 2.2305\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 470\n",
            "  Loss for client 1: 1.4854\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 470\n",
            "  Loss for client 2: 1.9163\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 471\n",
            "  Client 0: total grad norm = 96.398756 (253 params)\n",
            "  Client 1: total grad norm = 82.474341 (253 params)\n",
            "  Client 2: total grad norm = 77.463229 (253 params)\n",
            "\n",
            "=== Optimizer Step 471 ===\n",
            "\n",
            "=== Updating Client Weights (Step 471) ===\n",
            "Gradient norms: [9.66336727142334, 8.128984451293945, 9.51549243927002]\n",
            "Target weights: [0.31208112835884094, 0.3709878623485565, 0.31693100929260254]\n",
            "Updated weights: [0.3645429015159607, 0.32441195845603943, 0.31104516983032227]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 471\n",
            "  Loss for client 0: 1.4361\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 471\n",
            "  Loss for client 1: 1.5006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 471\n",
            "  Loss for client 2: 1.8443\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 472\n",
            "  Client 0: total grad norm = 70.600987 (253 params)\n",
            "  Client 1: total grad norm = 65.522255 (253 params)\n",
            "  Client 2: total grad norm = 121.158419 (253 params)\n",
            "\n",
            "=== Optimizer Step 472 ===\n",
            "\n",
            "=== Updating Client Weights (Step 472) ===\n",
            "Gradient norms: [8.26379108428955, 7.0653581619262695, 9.44501781463623]\n",
            "Target weights: [0.3284549415111542, 0.38416779041290283, 0.2873772382736206]\n",
            "Updated weights: [0.35371652245521545, 0.3423387110233307, 0.30394476652145386]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 472\n",
            "  Loss for client 0: 1.7837\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 472\n",
            "  Loss for client 1: 1.8492\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 472\n",
            "  Loss for client 2: 1.6828\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 473\n",
            "  Client 0: total grad norm = 66.471165 (253 params)\n",
            "  Client 1: total grad norm = 83.412089 (253 params)\n",
            "  Client 2: total grad norm = 79.406003 (253 params)\n",
            "\n",
            "=== Optimizer Step 473 ===\n",
            "\n",
            "=== Updating Client Weights (Step 473) ===\n",
            "Gradient norms: [7.832581520080566, 8.798190116882324, 8.571582794189453]\n",
            "Target weights: [0.35662904381752014, 0.31748872995376587, 0.3258821964263916]\n",
            "Updated weights: [0.3545902967453003, 0.3348837196826935, 0.3105259835720062]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 473\n",
            "  Loss for client 0: 1.5556\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 473\n",
            "  Loss for client 1: 1.7495\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 473\n",
            "  Loss for client 2: 2.0948\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 474\n",
            "  Client 0: total grad norm = 68.518727 (253 params)\n",
            "  Client 1: total grad norm = 77.300763 (253 params)\n",
            "  Client 2: total grad norm = 73.219994 (253 params)\n",
            "\n",
            "=== Optimizer Step 474 ===\n",
            "\n",
            "=== Updating Client Weights (Step 474) ===\n",
            "Gradient norms: [8.26148796081543, 7.423168182373047, 7.330479145050049]\n",
            "Target weights: [0.30864790081977844, 0.3435043692588806, 0.3478477895259857]\n",
            "Updated weights: [0.34080755710601807, 0.3374699056148529, 0.32172253727912903]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 474\n",
            "  Loss for client 0: 1.1938\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 474\n",
            "  Loss for client 1: 1.2008\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 474\n",
            "  Loss for client 2: 2.3829\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 475\n",
            "  Client 0: total grad norm = 70.876180 (253 params)\n",
            "  Client 1: total grad norm = 56.911059 (253 params)\n",
            "  Client 2: total grad norm = 123.876066 (253 params)\n",
            "\n",
            "=== Optimizer Step 475 ===\n",
            "\n",
            "=== Updating Client Weights (Step 475) ===\n",
            "Gradient norms: [7.808355331420898, 6.211423397064209, 11.203300476074219]\n",
            "Target weights: [0.33851656317710876, 0.4255478084087372, 0.23593561351299286]\n",
            "Updated weights: [0.34012025594711304, 0.3638932704925537, 0.29598647356033325]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 475\n",
            "  Loss for client 0: 1.7827\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 475\n",
            "  Loss for client 1: 1.4104\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 475\n",
            "  Loss for client 2: 1.4746\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 476\n",
            "  Client 0: total grad norm = 88.599391 (253 params)\n",
            "  Client 1: total grad norm = 67.242411 (253 params)\n",
            "  Client 2: total grad norm = 99.320530 (253 params)\n",
            "\n",
            "=== Optimizer Step 476 ===\n",
            "\n",
            "=== Updating Client Weights (Step 476) ===\n",
            "Gradient norms: [9.17547607421875, 7.851443767547607, 10.890915870666504]\n",
            "Target weights: [0.33210185170173645, 0.38810601830482483, 0.2797921299934387]\n",
            "Updated weights: [0.3377147316932678, 0.37115707993507385, 0.29112815856933594]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 476\n",
            "  Loss for client 0: 1.3748\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 476\n",
            "  Loss for client 1: 1.4231\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 476\n",
            "  Loss for client 2: 1.6064\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 477\n",
            "  Client 0: total grad norm = 66.974770 (253 params)\n",
            "  Client 1: total grad norm = 68.887014 (253 params)\n",
            "  Client 2: total grad norm = 79.472635 (253 params)\n",
            "\n",
            "=== Optimizer Step 477 ===\n",
            "\n",
            "=== Updating Client Weights (Step 477) ===\n",
            "Gradient norms: [7.52457332611084, 7.893575668334961, 9.883747100830078]\n",
            "Target weights: [0.3683837354183197, 0.351162850856781, 0.2804534137248993]\n",
            "Updated weights: [0.34691545367240906, 0.3651588261127472, 0.28792575001716614]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 477\n",
            "  Loss for client 0: 1.5291\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 477\n",
            "  Loss for client 1: 1.5967\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 477\n",
            "  Loss for client 2: 1.6610\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 478\n",
            "  Client 0: total grad norm = 82.340429 (253 params)\n",
            "  Client 1: total grad norm = 64.295160 (253 params)\n",
            "  Client 2: total grad norm = 76.520364 (253 params)\n",
            "\n",
            "=== Optimizer Step 478 ===\n",
            "\n",
            "=== Updating Client Weights (Step 478) ===\n",
            "Gradient norms: [8.272793769836426, 7.343807697296143, 7.225025653839111]\n",
            "Target weights: [0.3056684732437134, 0.34433525800704956, 0.34999626874923706]\n",
            "Updated weights: [0.33454135060310364, 0.35891175270080566, 0.3065469264984131]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 478\n",
            "  Loss for client 0: 1.7783\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 478\n",
            "  Loss for client 1: 1.4786\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 478\n",
            "  Loss for client 2: 1.4159\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 479\n",
            "  Client 0: total grad norm = 74.120189 (253 params)\n",
            "  Client 1: total grad norm = 66.633920 (253 params)\n",
            "  Client 2: total grad norm = 62.526277 (253 params)\n",
            "\n",
            "=== Optimizer Step 479 ===\n",
            "\n",
            "=== Updating Client Weights (Step 479) ===\n",
            "Gradient norms: [8.123618125915527, 7.864159107208252, 7.183424949645996]\n",
            "Target weights: [0.31606796383857727, 0.326495885848999, 0.3574361205101013]\n",
            "Updated weights: [0.3289993405342102, 0.3491869866847992, 0.321813702583313]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 479\n",
            "  Loss for client 0: 1.7311\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 479\n",
            "  Loss for client 1: 1.8368\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 479\n",
            "  Loss for client 2: 2.1474\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 480\n",
            "  Client 0: total grad norm = 78.644388 (253 params)\n",
            "  Client 1: total grad norm = 70.478597 (253 params)\n",
            "  Client 2: total grad norm = 105.983307 (253 params)\n",
            "\n",
            "=== Optimizer Step 480 ===\n",
            "\n",
            "=== Updating Client Weights (Step 480) ===\n",
            "Gradient norms: [9.35107421875, 7.7014875411987305, 9.412470817565918]\n",
            "Target weights: [0.3117530047893524, 0.37852755188941956, 0.3097194731235504]\n",
            "Updated weights: [0.3238254189491272, 0.3579891622066498, 0.3181854486465454]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 480\n",
            "  Loss for client 0: 1.1353\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 480\n",
            "  Loss for client 1: 1.2665\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 480\n",
            "  Loss for client 2: 2.1851\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 481\n",
            "  Client 0: total grad norm = 70.723698 (253 params)\n",
            "  Client 1: total grad norm = 84.387315 (253 params)\n",
            "  Client 2: total grad norm = 84.349204 (253 params)\n",
            "\n",
            "=== Optimizer Step 481 ===\n",
            "\n",
            "=== Updating Client Weights (Step 481) ===\n",
            "Gradient norms: [6.729236602783203, 11.807884216308594, 9.547929763793945]\n",
            "Target weights: [0.4396226108074188, 0.25053808093070984, 0.3098393678665161]\n",
            "Updated weights: [0.3585645854473114, 0.3257538378238678, 0.3156816363334656]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 481\n",
            "  Loss for client 0: 1.2837\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 481\n",
            "  Loss for client 1: 1.9697\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 481\n",
            "  Loss for client 2: 1.8198\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 482\n",
            "  Client 0: total grad norm = 70.567602 (253 params)\n",
            "  Client 1: total grad norm = 71.437917 (253 params)\n",
            "  Client 2: total grad norm = 91.409486 (253 params)\n",
            "\n",
            "=== Optimizer Step 482 ===\n",
            "\n",
            "=== Updating Client Weights (Step 482) ===\n",
            "Gradient norms: [8.575843811035156, 8.765380859375, 9.285277366638184]\n",
            "Target weights: [0.34459319710731506, 0.3371419310569763, 0.31826484203338623]\n",
            "Updated weights: [0.35437318682670593, 0.329170286655426, 0.3164566159248352]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 482\n",
            "  Loss for client 0: 1.2617\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 482\n",
            "  Loss for client 1: 1.4530\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 482\n",
            "  Loss for client 2: 1.8518\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 483\n",
            "  Client 0: total grad norm = 61.189622 (253 params)\n",
            "  Client 1: total grad norm = 78.787623 (253 params)\n",
            "  Client 2: total grad norm = 85.017564 (253 params)\n",
            "\n",
            "=== Optimizer Step 483 ===\n",
            "\n",
            "=== Updating Client Weights (Step 483) ===\n",
            "Gradient norms: [6.511940956115723, 7.71702241897583, 8.389476776123047]\n",
            "Target weights: [0.3816729187965393, 0.32207128405570984, 0.29625582695007324]\n",
            "Updated weights: [0.3625631034374237, 0.32704058289527893, 0.31039637327194214]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 483\n",
            "  Loss for client 0: 0.9876\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 483\n",
            "  Loss for client 1: 1.6467\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 483\n",
            "  Loss for client 2: 1.8902\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 484\n",
            "  Client 0: total grad norm = 57.839997 (253 params)\n",
            "  Client 1: total grad norm = 66.212047 (253 params)\n",
            "  Client 2: total grad norm = 90.130381 (253 params)\n",
            "\n",
            "=== Optimizer Step 484 ===\n",
            "\n",
            "=== Updating Client Weights (Step 484) ===\n",
            "Gradient norms: [6.892163276672363, 7.400640964508057, 10.579631805419922]\n",
            "Target weights: [0.38718441128730774, 0.36058205366134644, 0.2522335648536682]\n",
            "Updated weights: [0.36994948983192444, 0.337103009223938, 0.29294753074645996]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 484\n",
            "  Loss for client 0: 1.4755\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 484\n",
            "  Loss for client 1: 1.8583\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 484\n",
            "  Loss for client 2: 1.4192\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 485\n",
            "  Client 0: total grad norm = 74.725447 (253 params)\n",
            "  Client 1: total grad norm = 81.226887 (253 params)\n",
            "  Client 2: total grad norm = 66.567879 (253 params)\n",
            "\n",
            "=== Optimizer Step 485 ===\n",
            "\n",
            "=== Updating Client Weights (Step 485) ===\n",
            "Gradient norms: [8.341734886169434, 7.768060684204102, 6.925816535949707]\n",
            "Target weights: [0.3050369918346405, 0.3275640904903412, 0.3673989474773407]\n",
            "Updated weights: [0.3504757285118103, 0.3342413306236267, 0.315282940864563]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 485\n",
            "  Loss for client 0: 1.5334\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 485\n",
            "  Loss for client 1: 1.8029\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 485\n",
            "  Loss for client 2: 1.5362\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 486\n",
            "  Client 0: total grad norm = 71.175287 (253 params)\n",
            "  Client 1: total grad norm = 76.698263 (253 params)\n",
            "  Client 2: total grad norm = 78.313166 (253 params)\n",
            "\n",
            "=== Optimizer Step 486 ===\n",
            "\n",
            "=== Updating Client Weights (Step 486) ===\n",
            "Gradient norms: [7.517806529998779, 9.421972274780273, 9.130895614624023]\n",
            "Target weights: [0.3814990222454071, 0.30439865589141846, 0.3141023516654968]\n",
            "Updated weights: [0.35978269577026367, 0.3252885341644287, 0.3149287700653076]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 486\n",
            "  Loss for client 0: 1.2107\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 486\n",
            "  Loss for client 1: 1.6022\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 486\n",
            "  Loss for client 2: 1.6050\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 487\n",
            "  Client 0: total grad norm = 71.175469 (253 params)\n",
            "  Client 1: total grad norm = 85.854746 (253 params)\n",
            "  Client 2: total grad norm = 76.113059 (253 params)\n",
            "\n",
            "=== Optimizer Step 487 ===\n",
            "\n",
            "=== Updating Client Weights (Step 487) ===\n",
            "Gradient norms: [9.011295318603516, 8.658815383911133, 7.926983833312988]\n",
            "Target weights: [0.31471332907676697, 0.3275245428085327, 0.3577621281147003]\n",
            "Updated weights: [0.3462618887424469, 0.32595935463905334, 0.32777878642082214]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 487\n",
            "  Loss for client 0: 2.3657\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 487\n",
            "  Loss for client 1: 1.3451\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 487\n",
            "  Loss for client 2: 2.0745\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 488\n",
            "  Client 0: total grad norm = 135.568791 (253 params)\n",
            "  Client 1: total grad norm = 65.064907 (253 params)\n",
            "  Client 2: total grad norm = 82.611688 (253 params)\n",
            "\n",
            "=== Optimizer Step 488 ===\n",
            "\n",
            "=== Updating Client Weights (Step 488) ===\n",
            "Gradient norms: [13.468182563781738, 6.804633140563965, 8.106277465820312]\n",
            "Target weights: [0.2154839038848877, 0.4265000820159912, 0.3580159544944763]\n",
            "Updated weights: [0.30702847242355347, 0.35612156987190247, 0.3368499279022217]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 488\n",
            "  Loss for client 0: 1.7196\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 488\n",
            "  Loss for client 1: 2.2759\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 488\n",
            "  Loss for client 2: 1.6615\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 489\n",
            "  Client 0: total grad norm = 68.465010 (253 params)\n",
            "  Client 1: total grad norm = 93.361816 (253 params)\n",
            "  Client 2: total grad norm = 75.716417 (253 params)\n",
            "\n",
            "=== Optimizer Step 489 ===\n",
            "\n",
            "=== Updating Client Weights (Step 489) ===\n",
            "Gradient norms: [7.474043369293213, 9.702930450439453, 9.22297191619873]\n",
            "Target weights: [0.38749781250953674, 0.29848459362983704, 0.3140175938606262]\n",
            "Updated weights: [0.3311692774295807, 0.33883047103881836, 0.33000022172927856]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 489\n",
            "  Loss for client 0: 1.5902\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 489\n",
            "  Loss for client 1: 1.4458\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 489\n",
            "  Loss for client 2: 1.5885\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 490\n",
            "  Client 0: total grad norm = 78.222626 (253 params)\n",
            "  Client 1: total grad norm = 71.133102 (253 params)\n",
            "  Client 2: total grad norm = 61.698629 (253 params)\n",
            "\n",
            "=== Optimizer Step 490 ===\n",
            "\n",
            "=== Updating Client Weights (Step 490) ===\n",
            "Gradient norms: [7.9643096923828125, 7.544835567474365, 7.107784271240234]\n",
            "Target weights: [0.3148512840270996, 0.33235621452331543, 0.35279250144958496]\n",
            "Updated weights: [0.3262738883495331, 0.3368881940841675, 0.33683791756629944]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 490\n",
            "  Loss for client 0: 1.2638\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 490\n",
            "  Loss for client 1: 1.9078\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 490\n",
            "  Loss for client 2: 1.8261\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 491\n",
            "  Client 0: total grad norm = 95.381413 (253 params)\n",
            "  Client 1: total grad norm = 81.416169 (253 params)\n",
            "  Client 2: total grad norm = 73.323407 (253 params)\n",
            "\n",
            "=== Optimizer Step 491 ===\n",
            "\n",
            "=== Updating Client Weights (Step 491) ===\n",
            "Gradient norms: [8.472546577453613, 9.871593475341797, 7.706463813781738]\n",
            "Target weights: [0.33810245990753174, 0.29018503427505493, 0.37171247601509094]\n",
            "Updated weights: [0.32982248067855835, 0.32287725806236267, 0.34730029106140137]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 491\n",
            "  Loss for client 0: 1.4322\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 491\n",
            "  Loss for client 1: 1.3093\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 491\n",
            "  Loss for client 2: 1.9777\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 492\n",
            "  Client 0: total grad norm = 65.724449 (253 params)\n",
            "  Client 1: total grad norm = 58.572101 (253 params)\n",
            "  Client 2: total grad norm = 92.330218 (253 params)\n",
            "\n",
            "=== Optimizer Step 492 ===\n",
            "\n",
            "=== Updating Client Weights (Step 492) ===\n",
            "Gradient norms: [7.223799705505371, 6.71127986907959, 9.497101783752441]\n",
            "Target weights: [0.35248512029647827, 0.37940335273742676, 0.2681114971637726]\n",
            "Updated weights: [0.3366212844848633, 0.3398350775241852, 0.32354363799095154]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 492\n",
            "  Loss for client 0: 1.1897\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 492\n",
            "  Loss for client 1: 1.5157\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 492\n",
            "  Loss for client 2: 1.4878\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 493\n",
            "  Client 0: total grad norm = 67.751333 (253 params)\n",
            "  Client 1: total grad norm = 70.603644 (253 params)\n",
            "  Client 2: total grad norm = 73.511502 (253 params)\n",
            "\n",
            "=== Optimizer Step 493 ===\n",
            "\n",
            "=== Updating Client Weights (Step 493) ===\n",
            "Gradient norms: [7.080013751983643, 8.472965240478516, 8.816943168640137]\n",
            "Target weights: [0.378988653421402, 0.3166830837726593, 0.30432823300361633]\n",
            "Updated weights: [0.34933149814605713, 0.33288949728012085, 0.317779004573822]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 493\n",
            "  Loss for client 0: 1.7023\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 493\n",
            "  Loss for client 1: 1.3192\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 493\n",
            "  Loss for client 2: 1.6991\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 494\n",
            "  Client 0: total grad norm = 84.945416 (253 params)\n",
            "  Client 1: total grad norm = 101.252544 (253 params)\n",
            "  Client 2: total grad norm = 80.605068 (253 params)\n",
            "\n",
            "=== Optimizer Step 494 ===\n",
            "\n",
            "=== Updating Client Weights (Step 494) ===\n",
            "Gradient norms: [8.69874095916748, 11.4378080368042, 9.128812789916992]\n",
            "Target weights: [0.368539422750473, 0.28028351068496704, 0.35117700695991516]\n",
            "Updated weights: [0.35509389638900757, 0.3171077072620392, 0.32779839634895325]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 494\n",
            "  Loss for client 0: 1.4925\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 494\n",
            "  Loss for client 1: 1.8617\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 494\n",
            "  Loss for client 2: 1.6042\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 495\n",
            "  Client 0: total grad norm = 78.214007 (253 params)\n",
            "  Client 1: total grad norm = 80.310390 (253 params)\n",
            "  Client 2: total grad norm = 64.704842 (253 params)\n",
            "\n",
            "=== Optimizer Step 495 ===\n",
            "\n",
            "=== Updating Client Weights (Step 495) ===\n",
            "Gradient norms: [8.891358375549316, 8.750295639038086, 7.745870113372803]\n",
            "Target weights: [0.31605514883995056, 0.3211502432823181, 0.36279454827308655]\n",
            "Updated weights: [0.3433822989463806, 0.31832048296928406, 0.3382972776889801]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 495\n",
            "  Loss for client 0: 1.3211\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 495\n",
            "  Loss for client 1: 1.9574\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 495\n",
            "  Loss for client 2: 1.3609\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 496\n",
            "  Client 0: total grad norm = 62.766388 (253 params)\n",
            "  Client 1: total grad norm = 82.038240 (253 params)\n",
            "  Client 2: total grad norm = 70.737304 (253 params)\n",
            "\n",
            "=== Optimizer Step 496 ===\n",
            "\n",
            "=== Updating Client Weights (Step 496) ===\n",
            "Gradient norms: [7.7101593017578125, 8.577324867248535, 6.620562553405762]\n",
            "Target weights: [0.3264263868331909, 0.2934247553348541, 0.38014885783195496]\n",
            "Updated weights: [0.33829551935195923, 0.3108517527580261, 0.35085275769233704]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 496\n",
            "  Loss for client 0: 1.8744\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 496\n",
            "  Loss for client 1: 1.0777\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 496\n",
            "  Loss for client 2: 1.5109\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 497\n",
            "  Client 0: total grad norm = 93.135698 (253 params)\n",
            "  Client 1: total grad norm = 57.092946 (253 params)\n",
            "  Client 2: total grad norm = 81.846722 (253 params)\n",
            "\n",
            "=== Optimizer Step 497 ===\n",
            "\n",
            "=== Updating Client Weights (Step 497) ===\n",
            "Gradient norms: [9.25813102722168, 6.242719650268555, 8.533828735351562]\n",
            "Target weights: [0.28027671575546265, 0.4156583249568939, 0.3040649890899658]\n",
            "Updated weights: [0.3208898901939392, 0.34229370951652527, 0.3368164300918579]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 497\n",
            "  Loss for client 0: 0.8969\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 497\n",
            "  Loss for client 1: 1.8591\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 497\n",
            "  Loss for client 2: 1.6412\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 498\n",
            "  Client 0: total grad norm = 58.218987 (253 params)\n",
            "  Client 1: total grad norm = 88.905684 (253 params)\n",
            "  Client 2: total grad norm = 79.655912 (253 params)\n",
            "\n",
            "=== Optimizer Step 498 ===\n",
            "\n",
            "=== Updating Client Weights (Step 498) ===\n",
            "Gradient norms: [6.2656569480896, 9.386228561401367, 8.7701416015625]\n",
            "Target weights: [0.4198209345340729, 0.2802461087703705, 0.29993289709091187]\n",
            "Updated weights: [0.3505691885948181, 0.3236794173717499, 0.3257513642311096]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 498\n",
            "  Loss for client 0: 1.4336\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 498\n",
            "  Loss for client 1: 1.7633\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 498\n",
            "  Loss for client 2: 1.6461\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 499\n",
            "  Client 0: total grad norm = 79.893222 (253 params)\n",
            "  Client 1: total grad norm = 75.633022 (253 params)\n",
            "  Client 2: total grad norm = 79.565430 (253 params)\n",
            "\n",
            "=== Optimizer Step 499 ===\n",
            "\n",
            "=== Updating Client Weights (Step 499) ===\n",
            "Gradient norms: [9.324854850769043, 8.154159545898438, 8.556342124938965]\n",
            "Target weights: [0.3092731237411499, 0.3536755442619324, 0.3370513617992401]\n",
            "Updated weights: [0.3381803631782532, 0.33267825841903687, 0.32914137840270996]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 499\n",
            "  Loss for client 0: 1.0097\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 499\n",
            "  Loss for client 1: 1.6548\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 499\n",
            "  Loss for client 2: 1.3543\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 500\n",
            "  Client 0: total grad norm = 78.134210 (253 params)\n",
            "  Client 1: total grad norm = 87.906565 (253 params)\n",
            "  Client 2: total grad norm = 74.884376 (253 params)\n",
            "\n",
            "=== Optimizer Step 500 ===\n",
            "\n",
            "=== Updating Client Weights (Step 500) ===\n",
            "Gradient norms: [7.717598915100098, 8.228737831115723, 8.166133880615234]\n",
            "Target weights: [0.34686604142189026, 0.32531997561454773, 0.3278139531612396]\n",
            "Updated weights: [0.34078606963157654, 0.3304707705974579, 0.3287431597709656]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 500\n",
            "  Val Loss = 1.5286 (5 batches)\n",
            "–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ /content/drive/MyDrive/meritopt_checkpoints/step_500\n",
            "\n",
            "Reached max steps (500), stopping...\n",
            "\n",
            "Epoch 1 completed:\n",
            "   Train Loss = 0.2682\n",
            "   Val Loss = 1.5951 (8 batches)\n",
            "–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ /content/drive/MyDrive/meritopt_checkpoints/epoch_1\n",
            "\n",
            "============================================================\n",
            "Quick test:\n",
            "============================================================\n",
            "Client 0: Tud ºi î…îxon kad ºaabia î... -> –í–æ—Ç —Ç–∞–∫ —Å—Ç–∞—Ä–∏–∫....\n",
            "Client 1: matu d ºa kona…Åa... -> –ù—É, –≤–æ—Ç –∫–æ–Ω–µ—á–Ω–æ....\n",
            "Client 2: ƒåeda m ºakdant ≈ãad ºiw î... -> –í–æ—Ç —Ç–∞–∫ –±—ã–ª–æ....\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1600x800 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "try:\n",
        "    model, tokenizer, logger = main_simple()\n",
        "except Exception as e:\n",
        "    print(f\"–û—à–∏–±–∫–∞: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_o8ebi1aHtg"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZn2XSTmK3Kp"
      },
      "source": [
        "# –£—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "QIV9dnWtLmzy"
      },
      "outputs": [],
      "source": [
        "def create_language_clients(config, tokenizer):\n",
        "    \"\"\"\n",
        "    –°–æ–∑–¥–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –†–ê–í–ù–´–ú –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞\n",
        "    \"\"\"\n",
        "    all_data = load_your_dataset()\n",
        "\n",
        "    if not all_data:\n",
        "        print(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö!\")\n",
        "        return {}, None, {}\n",
        "\n",
        "    # –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —è–∑—ã–∫–∞–º\n",
        "    data_by_language = defaultdict(list)\n",
        "    for item in all_data:\n",
        "        lang = item.get('lang', 'unknown')\n",
        "        data_by_language[lang].append(item)\n",
        "\n",
        "    # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤\n",
        "    min_samples = min(len(items) for items in data_by_language.values())\n",
        "    print(f\"\\n–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {min_samples}\")\n",
        "    print(f\"–ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ {min_samples} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞\")\n",
        "\n",
        "    languages = list(data_by_language.keys())\n",
        "    if len(languages) > config.npeers:\n",
        "        languages = languages[:config.npeers]\n",
        "\n",
        "    # –±–µ—Ä–µ–º –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞\n",
        "    balanced_data_by_language = {}\n",
        "    for i, lang in enumerate(languages):\n",
        "        if i >= config.npeers:\n",
        "            break\n",
        "\n",
        "        lang_data = data_by_language[lang]\n",
        "\n",
        "        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ min_samples –ø—Ä–∏–º–µ—Ä–æ–≤ –æ—Ç –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞\n",
        "        if len(lang_data) > min_samples:\n",
        "            # –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º min_samples –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "            balanced_data = random.sample(lang_data, min_samples)\n",
        "        else:\n",
        "            # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—å—à–µ min_samples, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ\n",
        "            balanced_data = lang_data.copy()\n",
        "            print(f\"  –í–ù–ò–ú–ê–ù–ò–ï: –£ —è–∑—ã–∫–∞ {lang} —Ç–æ–ª—å–∫–æ {len(lang_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "\n",
        "        balanced_data_by_language[lang] = balanced_data\n",
        "\n",
        "    # —Å–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞\n",
        "    train_loaders = {}\n",
        "    val_loaders = {}\n",
        "    test_loaders = {}\n",
        "\n",
        "    for i, lang in enumerate(languages):\n",
        "        if i >= config.npeers:\n",
        "            break\n",
        "\n",
        "        lang_data = balanced_data_by_language[lang]\n",
        "        random.shuffle(lang_data)  # –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "\n",
        "        # —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val/test (70/15/15)\n",
        "        n_total = len(lang_data)\n",
        "        n_train = int(n_total * 0.7)\n",
        "        n_val = int(n_total * 0.15)\n",
        "\n",
        "        train_data = lang_data[:n_train]\n",
        "        val_data = lang_data[n_train:n_train + n_val]\n",
        "        test_data = lang_data[n_train + n_val:]\n",
        "\n",
        "        print(f\"\\n–ö–ª–∏–µ–Ω—Ç {i} ({lang.upper()}):\")\n",
        "        print(f\"  Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "        print(f\"  Val: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "        print(f\"  Test: {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "\n",
        "        # —Å–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
        "        train_dataset = TranslationDataset(train_data, split=\"train\")\n",
        "        val_dataset = TranslationDataset(val_data, split=\"val\")\n",
        "        test_dataset = TranslationDataset(test_data, split=\"test\")\n",
        "\n",
        "        # —Å–æ–∑–¥–∞–µ–º DataLoader—ã\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=config.batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=lambda x: x,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=config.batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=lambda x: x,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=config.batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=lambda x: x,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        train_loaders[i] = train_loader\n",
        "        val_loaders[i] = val_loader\n",
        "        test_loaders[i] = test_loader\n",
        "\n",
        "    # –æ–±—â–∏–π –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –∏–∑ –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤ (—Ç–∞–∫–∂–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)\n",
        "    all_val_data = []\n",
        "    for lang, lang_data in balanced_data_by_language.items():\n",
        "        samples_to_take = min(5, len(lang_data))\n",
        "        all_val_data.extend(lang_data[:samples_to_take])\n",
        "\n",
        "    random.shuffle(all_val_data)\n",
        "    combined_val_dataset = TranslationDataset(all_val_data, split=\"val\")\n",
        "    combined_val_loader = DataLoader(\n",
        "        combined_val_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda x: x,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    print(f\"\\n–û–±—â–∏–π –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä: {len(all_val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "    print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –≤ val_loader: {len(combined_val_loader)}\")\n",
        "\n",
        "    return train_loaders, combined_val_loader, test_loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2f5e01e8f2d44a2cb1d70643bfd82dc0",
            "d5cbb80ba7e042c68f90d515da234918",
            "6f257b5666de4a109bfffe65ac5bd25f",
            "896d22d83f17470d8bf9ef4f69284c3b",
            "e365c08cfa714bd4837669a7db4fe476",
            "2582bd71c593480d8a1f11e9b63783cf",
            "0d666bcaad9346f0a16d9ae26e7af600",
            "e88f702f612643c8b295acc6640e56c6",
            "ca8b50cf17754d4694bfe4bc7ce02bf1",
            "3deb48617cc64a05b9199f67bbdfeb54",
            "b575564fbe3348c19b695fcc9b62df01"
          ]
        },
        "id": "OPPuT0OoL8WE",
        "outputId": "fae3d2e5-a1dc-4f9d-f73e-5d124fc3cec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å Hugging Face...\n",
            "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 97434 –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
            "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 96608 –ø—Ä–∏–º–µ—Ä–æ–≤, –ø—Ä–æ–ø—É—â–µ–Ω–æ 826\n",
            "\n",
            "–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: 10222\n",
            "–ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ 10222 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞\n",
            "  –í–ù–ò–ú–ê–ù–ò–ï: –£ —è–∑—ã–∫–∞ nen —Ç–æ–ª—å–∫–æ 10222 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "\n",
            "–ö–ª–∏–µ–Ω—Ç 0 (ENF):\n",
            "  Train: 7155 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "  Val: 1533 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "  Test: 1534 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "\n",
            "–ö–ª–∏–µ–Ω—Ç 1 (NGA):\n",
            "  Train: 7155 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "  Val: 1533 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "  Test: 1534 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "\n",
            "–ö–ª–∏–µ–Ω—Ç 2 (NEN):\n",
            "  Train: 7155 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "  Val: 1533 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "  Test: 1534 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "\n",
            "–û–±—â–∏–π –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä: 15 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –≤ val_loader: 8\n",
            "\n",
            "============================================================\n",
            "Starting FIXED training (max_steps=500)\n",
            "============================================================\n",
            "\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f5e01e8f2d44a2cb1d70643bfd82dc0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FL Epoch 1:   0%|          | 0/3578 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Processing] Client 0, batch 0\n",
            "  Loss for client 0: 6.5508\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 0\n",
            "  Loss for client 1: 5.8902\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 0\n",
            "  Loss for client 2: 6.2271\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 1\n",
            "  Client 0: total grad norm = 413.229016 (253 params)\n",
            "  Client 1: total grad norm = 232.836915 (253 params)\n",
            "  Client 2: total grad norm = 256.319565 (253 params)\n",
            "\n",
            "=== Optimizer Step 1 ===\n",
            "\n",
            "=== Updating Client Weights (Step 1) ===\n",
            "Gradient norms: [31.8551082611084, 16.13771629333496, 18.907814025878906]\n",
            "Target weights: [0.21465155482292175, 0.42371225357055664, 0.36163613200187683]\n",
            "Updated weights: [0.2977288067340851, 0.3604470193386078, 0.34182417392730713]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 1\n",
            "  Loss for client 0: 5.7095\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 1\n",
            "  Loss for client 1: 4.9081\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 1\n",
            "  Loss for client 2: 5.0625\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 2\n",
            "  Client 0: total grad norm = 252.506709 (253 params)\n",
            "  Client 1: total grad norm = 183.844787 (253 params)\n",
            "  Client 2: total grad norm = 199.166802 (253 params)\n",
            "\n",
            "=== Optimizer Step 2 ===\n",
            "\n",
            "=== Updating Client Weights (Step 2) ===\n",
            "Gradient norms: [22.18312644958496, 16.224348068237305, 16.266639709472656]\n",
            "Target weights: [0.26802515983581543, 0.3664637804031372, 0.365511029958725]\n",
            "Updated weights: [0.28881770372390747, 0.36225202679634094, 0.3489302396774292]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 2\n",
            "  Loss for client 0: 4.9572\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 2\n",
            "  Loss for client 1: 5.2150\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 2\n",
            "  Loss for client 2: 4.7606\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 3\n",
            "  Client 0: total grad norm = 239.321038 (253 params)\n",
            "  Client 1: total grad norm = 199.584352 (253 params)\n",
            "  Client 2: total grad norm = 138.938156 (253 params)\n",
            "\n",
            "=== Optimizer Step 3 ===\n",
            "\n",
            "=== Updating Client Weights (Step 3) ===\n",
            "Gradient norms: [22.340089797973633, 18.341102600097656, 12.625630378723145]\n",
            "Target weights: [0.25078633427619934, 0.30546635389328003, 0.44374728202819824]\n",
            "Updated weights: [0.27740830183029175, 0.3452163338661194, 0.37737536430358887]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 3\n",
            "  Loss for client 0: 3.9647\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 3\n",
            "  Loss for client 1: 4.0827\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 3\n",
            "  Loss for client 2: 4.2777\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 4\n",
            "  Client 0: total grad norm = 234.262096 (253 params)\n",
            "  Client 1: total grad norm = 107.856955 (253 params)\n",
            "  Client 2: total grad norm = 102.864026 (253 params)\n",
            "\n",
            "=== Optimizer Step 4 ===\n",
            "\n",
            "=== Updating Client Weights (Step 4) ===\n",
            "Gradient norms: [20.531898498535156, 13.403661727905273, 10.07612133026123]\n",
            "Target weights: [0.2188427448272705, 0.3352260887622833, 0.44593119621276855]\n",
            "Updated weights: [0.25983864068984985, 0.3422192633152008, 0.39794212579727173]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 4\n",
            "  Loss for client 0: 3.6947\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 4\n",
            "  Loss for client 1: 3.7458\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 4\n",
            "  Loss for client 2: 3.9876\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 5\n",
            "  Client 0: total grad norm = 423.452076 (253 params)\n",
            "  Client 1: total grad norm = 109.124468 (253 params)\n",
            "  Client 2: total grad norm = 94.780955 (253 params)\n",
            "\n",
            "=== Optimizer Step 5 ===\n",
            "\n",
            "=== Updating Client Weights (Step 5) ===\n",
            "Gradient norms: [42.899688720703125, 10.466055870056152, 13.175889015197754]\n",
            "Target weights: [0.11969082057476044, 0.490604966878891, 0.389704167842865]\n",
            "Updated weights: [0.2177942991256714, 0.3867349624633789, 0.3954707384109497]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 5\n",
            "  Loss for client 0: 3.4374\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 5\n",
            "  Loss for client 1: 3.8590\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 5\n",
            "  Loss for client 2: 3.7161\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 6\n",
            "  Client 0: total grad norm = 236.232012 (253 params)\n",
            "  Client 1: total grad norm = 113.873609 (253 params)\n",
            "  Client 2: total grad norm = 117.121227 (253 params)\n",
            "\n",
            "=== Optimizer Step 6 ===\n",
            "\n",
            "=== Updating Client Weights (Step 6) ===\n",
            "Gradient norms: [23.913326263427734, 12.630749702453613, 12.148872375488281]\n",
            "Target weights: [0.20569272339344025, 0.38943037390708923, 0.40487685799598694]\n",
            "Updated weights: [0.21416382491588593, 0.38754358887672424, 0.39829257130622864]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 6\n",
            "  Loss for client 0: 3.4779\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 6\n",
            "  Loss for client 1: 2.8550\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 6\n",
            "  Loss for client 2: 3.4573\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 7\n",
            "  Client 0: total grad norm = 95.719180 (253 params)\n",
            "  Client 1: total grad norm = 157.970531 (253 params)\n",
            "  Client 2: total grad norm = 100.689193 (253 params)\n",
            "\n",
            "=== Optimizer Step 7 ===\n",
            "\n",
            "=== Updating Client Weights (Step 7) ===\n",
            "Gradient norms: [10.403165817260742, 17.141605377197266, 10.16486644744873]\n",
            "Target weights: [0.38017910718917847, 0.23072905838489532, 0.3890918493270874]\n",
            "Updated weights: [0.26396840810775757, 0.34049922227859497, 0.39553236961364746]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 7\n",
            "  Loss for client 0: 3.2541\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 7\n",
            "  Loss for client 1: 3.2016\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 7\n",
            "  Loss for client 2: 3.5592\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 8\n",
            "  Client 0: total grad norm = 103.565006 (253 params)\n",
            "  Client 1: total grad norm = 91.228931 (253 params)\n",
            "  Client 2: total grad norm = 91.783640 (253 params)\n",
            "\n",
            "=== Optimizer Step 8 ===\n",
            "\n",
            "=== Updating Client Weights (Step 8) ===\n",
            "Gradient norms: [11.127635955810547, 11.185932159423828, 10.826054573059082]\n",
            "Target weights: [0.3308360278606415, 0.32911184430122375, 0.34005212783813477]\n",
            "Updated weights: [0.28402870893478394, 0.33708301186561584, 0.3788883090019226]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 8\n",
            "  Loss for client 0: 3.6679\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 8\n",
            "  Loss for client 1: 3.2521\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 8\n",
            "  Loss for client 2: 3.0711\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 9\n",
            "  Client 0: total grad norm = 111.174937 (253 params)\n",
            "  Client 1: total grad norm = 100.713380 (253 params)\n",
            "  Client 2: total grad norm = 89.217489 (253 params)\n",
            "\n",
            "=== Optimizer Step 9 ===\n",
            "\n",
            "=== Updating Client Weights (Step 9) ===\n",
            "Gradient norms: [13.299179077148438, 10.545169830322266, 9.815727233886719]\n",
            "Target weights: [0.276544988155365, 0.34876835346221924, 0.3746866285800934]\n",
            "Updated weights: [0.2817835807800293, 0.34058859944343567, 0.37762778997421265]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 9\n",
            "  Loss for client 0: 3.4381\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 9\n",
            "  Loss for client 1: 2.9849\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 9\n",
            "  Loss for client 2: 3.3582\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 10\n",
            "  Client 0: total grad norm = 82.677116 (253 params)\n",
            "  Client 1: total grad norm = 69.296016 (253 params)\n",
            "  Client 2: total grad norm = 79.369567 (253 params)\n",
            "\n",
            "=== Optimizer Step 10 ===\n",
            "\n",
            "=== Updating Client Weights (Step 10) ===\n",
            "Gradient norms: [11.20598316192627, 7.608242034912109, 12.139357566833496]\n",
            "Target weights: [0.2944653034210205, 0.43371036648750305, 0.2718243896961212]\n",
            "Updated weights: [0.2855880856513977, 0.3685251474380493, 0.345886766910553]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 10\n",
            "  Loss for client 0: 2.8709\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 10\n",
            "  Loss for client 1: 3.4031\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 10\n",
            "  Loss for client 2: 3.4391\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 11\n",
            "  Client 0: total grad norm = 79.495495 (253 params)\n",
            "  Client 1: total grad norm = 77.381986 (253 params)\n",
            "  Client 2: total grad norm = 78.146717 (253 params)\n",
            "\n",
            "=== Optimizer Step 11 ===\n",
            "\n",
            "=== Updating Client Weights (Step 11) ===\n",
            "Gradient norms: [9.11255931854248, 11.862614631652832, 11.22787857055664]\n",
            "Target weights: [0.38763055205345154, 0.2977679371833801, 0.3146013915538788]\n",
            "Updated weights: [0.316200852394104, 0.3472979962825775, 0.33650118112564087]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 11\n",
            "  Loss for client 0: 1.8979\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 11\n",
            "  Loss for client 1: 3.1519\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 11\n",
            "  Loss for client 2: 3.8436\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 12\n",
            "  Client 0: total grad norm = 80.932747 (253 params)\n",
            "  Client 1: total grad norm = 88.843138 (253 params)\n",
            "  Client 2: total grad norm = 63.697246 (253 params)\n",
            "\n",
            "=== Optimizer Step 12 ===\n",
            "\n",
            "=== Updating Client Weights (Step 12) ===\n",
            "Gradient norms: [8.946757316589355, 11.085447311401367, 9.654154777526855]\n",
            "Target weights: [0.3657914400100708, 0.29522013664245605, 0.3389884829521179]\n",
            "Updated weights: [0.33107802271842957, 0.33167463541030884, 0.337247371673584]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 12\n",
            "  Loss for client 0: 3.3241\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 12\n",
            "  Loss for client 1: 4.1053\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 12\n",
            "  Loss for client 2: 2.0479\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 13\n",
            "  Client 0: total grad norm = 125.700090 (253 params)\n",
            "  Client 1: total grad norm = 90.224629 (253 params)\n",
            "  Client 2: total grad norm = 53.749404 (253 params)\n",
            "\n",
            "=== Optimizer Step 13 ===\n",
            "\n",
            "=== Updating Client Weights (Step 13) ===\n",
            "Gradient norms: [14.422940254211426, 14.541090965270996, 6.216957092285156]\n",
            "Target weights: [0.23192115128040314, 0.23003672063350677, 0.5380421280860901]\n",
            "Updated weights: [0.30133095383644104, 0.3011832535266876, 0.39748579263687134]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 13\n",
            "  Loss for client 0: 3.3161\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 13\n",
            "  Loss for client 1: 3.6306\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 13\n",
            "  Loss for client 2: 2.5436\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 14\n",
            "  Client 0: total grad norm = 83.610258 (253 params)\n",
            "  Client 1: total grad norm = 77.046207 (253 params)\n",
            "  Client 2: total grad norm = 63.104988 (253 params)\n",
            "\n",
            "=== Optimizer Step 14 ===\n",
            "\n",
            "=== Updating Client Weights (Step 14) ===\n",
            "Gradient norms: [12.765203475952148, 10.925374984741211, 9.482198715209961]\n",
            "Target weights: [0.28452515602111816, 0.3324390947818756, 0.383035808801651]\n",
            "Updated weights: [0.29628920555114746, 0.310560017824173, 0.39315080642700195]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 14\n",
            "  Loss for client 0: 2.6632\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 14\n",
            "  Loss for client 1: 3.5093\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 14\n",
            "  Loss for client 2: 3.7603\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 15\n",
            "  Client 0: total grad norm = 90.809765 (253 params)\n",
            "  Client 1: total grad norm = 65.624714 (253 params)\n",
            "  Client 2: total grad norm = 78.342382 (253 params)\n",
            "\n",
            "=== Optimizer Step 15 ===\n",
            "\n",
            "=== Updating Client Weights (Step 15) ===\n",
            "Gradient norms: [10.106656074523926, 10.050496101379395, 10.274364471435547]\n",
            "Target weights: [0.33453041315078735, 0.33639970421791077, 0.3290698826313019]\n",
            "Updated weights: [0.307761549949646, 0.3183119297027588, 0.3739265203475952]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 15\n",
            "  Loss for client 0: 2.8535\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 15\n",
            "  Loss for client 1: 2.0450\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 15\n",
            "  Loss for client 2: 3.1519\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 16\n",
            "  Client 0: total grad norm = 66.684147 (253 params)\n",
            "  Client 1: total grad norm = 71.483271 (253 params)\n",
            "  Client 2: total grad norm = 70.413273 (253 params)\n",
            "\n",
            "=== Optimizer Step 16 ===\n",
            "\n",
            "=== Updating Client Weights (Step 16) ===\n",
            "Gradient norms: [9.488078117370605, 7.658015727996826, 9.653670310974121]\n",
            "Target weights: [0.31038355827331543, 0.38455697894096375, 0.3050594627857208]\n",
            "Updated weights: [0.3085481524467468, 0.3381854295730591, 0.3532663881778717]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 16\n",
            "  Loss for client 0: 1.9086\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 16\n",
            "  Loss for client 1: 3.4254\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 16\n",
            "  Loss for client 2: 3.6126\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 17\n",
            "  Client 0: total grad norm = 51.274247 (253 params)\n",
            "  Client 1: total grad norm = 55.979779 (253 params)\n",
            "  Client 2: total grad norm = 79.812261 (253 params)\n",
            "\n",
            "=== Optimizer Step 17 ===\n",
            "\n",
            "=== Updating Client Weights (Step 17) ===\n",
            "Gradient norms: [7.940734386444092, 9.103983879089355, 11.326168060302734]\n",
            "Target weights: [0.3886026442050934, 0.3389494717121124, 0.2724478840827942]\n",
            "Updated weights: [0.33256450295448303, 0.33841463923454285, 0.32902082800865173]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 17\n",
            "  Loss for client 0: 2.4756\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 17\n",
            "  Loss for client 1: 2.4685\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 17\n",
            "  Loss for client 2: 3.1043\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 18\n",
            "  Client 0: total grad norm = 114.451131 (253 params)\n",
            "  Client 1: total grad norm = 57.575274 (253 params)\n",
            "  Client 2: total grad norm = 55.901392 (253 params)\n",
            "\n",
            "=== Optimizer Step 18 ===\n",
            "\n",
            "=== Updating Client Weights (Step 18) ===\n",
            "Gradient norms: [15.741223335266113, 7.654356956481934, 8.712562561035156]\n",
            "Target weights: [0.2056245654821396, 0.42286792397499084, 0.3715074956417084]\n",
            "Updated weights: [0.2944825291633606, 0.3637506365776062, 0.3417668342590332]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 18\n",
            "  Loss for client 0: 1.7982\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 18\n",
            "  Loss for client 1: 2.3512\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 18\n",
            "  Loss for client 2: 3.2507\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 19\n",
            "  Client 0: total grad norm = 54.805336 (253 params)\n",
            "  Client 1: total grad norm = 56.645904 (253 params)\n",
            "  Client 2: total grad norm = 52.483028 (253 params)\n",
            "\n",
            "=== Optimizer Step 19 ===\n",
            "\n",
            "=== Updating Client Weights (Step 19) ===\n",
            "Gradient norms: [7.453073501586914, 8.756866455078125, 7.877592086791992]\n",
            "Target weights: [0.3574974834918976, 0.30427035689353943, 0.3382321298122406]\n",
            "Updated weights: [0.313387006521225, 0.3459065556526184, 0.3407064378261566]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 19\n",
            "  Loss for client 0: 2.0964\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 19\n",
            "  Loss for client 1: 2.4555\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 19\n",
            "  Loss for client 2: 3.0383\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 20\n",
            "  Client 0: total grad norm = 53.785964 (253 params)\n",
            "  Client 1: total grad norm = 60.864553 (253 params)\n",
            "  Client 2: total grad norm = 56.483727 (253 params)\n",
            "\n",
            "=== Optimizer Step 20 ===\n",
            "\n",
            "=== Updating Client Weights (Step 20) ===\n",
            "Gradient norms: [7.142587184906006, 8.782865524291992, 7.779160499572754]\n",
            "Target weights: [0.36611121892929077, 0.2977367043495178, 0.3361521363258362]\n",
            "Updated weights: [0.329204261302948, 0.3314555883407593, 0.3393401503562927]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 20\n",
            "  Loss for client 0: 2.3008\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 20\n",
            "  Loss for client 1: 3.0340\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 20\n",
            "  Loss for client 2: 1.9214\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 21\n",
            "  Client 0: total grad norm = 60.187601 (253 params)\n",
            "  Client 1: total grad norm = 69.816643 (253 params)\n",
            "  Client 2: total grad norm = 63.278639 (253 params)\n",
            "\n",
            "=== Optimizer Step 21 ===\n",
            "\n",
            "=== Updating Client Weights (Step 21) ===\n",
            "Gradient norms: [8.582917213439941, 11.157018661499023, 8.08387279510498]\n",
            "Target weights: [0.3532298803329468, 0.2717341482639313, 0.37503594160079956]\n",
            "Updated weights: [0.3364119529724121, 0.31353914737701416, 0.35004889965057373]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 21\n",
            "  Loss for client 0: 3.0855\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 21\n",
            "  Loss for client 1: 2.6047\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 21\n",
            "  Loss for client 2: 2.2510\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 22\n",
            "  Client 0: total grad norm = 65.036730 (253 params)\n",
            "  Client 1: total grad norm = 72.576923 (253 params)\n",
            "  Client 2: total grad norm = 71.758931 (253 params)\n",
            "\n",
            "=== Optimizer Step 22 ===\n",
            "\n",
            "=== Updating Client Weights (Step 22) ===\n",
            "Gradient norms: [9.14398193359375, 10.712394714355469, 9.606276512145996]\n",
            "Target weights: [0.3564471900463104, 0.30425935983657837, 0.3392934203147888]\n",
            "Updated weights: [0.3424225449562073, 0.31075519323349, 0.34682226181030273]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 22\n",
            "  Loss for client 0: 2.1501\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 22\n",
            "  Loss for client 1: 3.1285\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 22\n",
            "  Loss for client 2: 3.0614\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 23\n",
            "  Client 0: total grad norm = 67.553183 (253 params)\n",
            "  Client 1: total grad norm = 58.000522 (253 params)\n",
            "  Client 2: total grad norm = 69.126721 (253 params)\n",
            "\n",
            "=== Optimizer Step 23 ===\n",
            "\n",
            "=== Updating Client Weights (Step 23) ===\n",
            "Gradient norms: [9.367467880249023, 9.714119911193848, 8.93697738647461]\n",
            "Target weights: [0.33195191621780396, 0.3201061189174652, 0.34794190526008606]\n",
            "Updated weights: [0.3392813503742218, 0.31356045603752136, 0.34715813398361206]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 23\n",
            "  Loss for client 0: 2.6786\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 23\n",
            "  Loss for client 1: 3.0761\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 23\n",
            "  Loss for client 2: 2.3203\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 24\n",
            "  Client 0: total grad norm = 60.146835 (253 params)\n",
            "  Client 1: total grad norm = 64.894305 (253 params)\n",
            "  Client 2: total grad norm = 56.297797 (253 params)\n",
            "\n",
            "=== Optimizer Step 24 ===\n",
            "\n",
            "=== Updating Client Weights (Step 24) ===\n",
            "Gradient norms: [8.743403434753418, 11.713347434997559, 7.898741245269775]\n",
            "Target weights: [0.35046103596687317, 0.26160091161727905, 0.3879380524158478]\n",
            "Updated weights: [0.34263527393341064, 0.29797258973121643, 0.35939210653305054]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 24\n",
            "  Loss for client 0: 2.2251\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 24\n",
            "  Loss for client 1: 2.8844\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 24\n",
            "  Loss for client 2: 2.6607\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 25\n",
            "  Client 0: total grad norm = 80.090999 (253 params)\n",
            "  Client 1: total grad norm = 67.522835 (253 params)\n",
            "  Client 2: total grad norm = 53.208246 (253 params)\n",
            "\n",
            "=== Optimizer Step 25 ===\n",
            "\n",
            "=== Updating Client Weights (Step 25) ===\n",
            "Gradient norms: [10.428526878356934, 10.671138763427734, 9.440652847290039]\n",
            "Target weights: [0.32447460293769836, 0.31709757447242737, 0.3584277629852295]\n",
            "Updated weights: [0.3371870815753937, 0.30371007323265076, 0.3591027855873108]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 25\n",
            "  Loss for client 0: 2.4189\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 25\n",
            "  Loss for client 1: 2.7096\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 25\n",
            "  Loss for client 2: 2.6349\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 26\n",
            "  Client 0: total grad norm = 73.630063 (253 params)\n",
            "  Client 1: total grad norm = 73.721006 (253 params)\n",
            "  Client 2: total grad norm = 58.717549 (253 params)\n",
            "\n",
            "=== Optimizer Step 26 ===\n",
            "\n",
            "=== Updating Client Weights (Step 26) ===\n",
            "Gradient norms: [8.835761070251465, 9.598188400268555, 8.164101600646973]\n",
            "Target weights: [0.33301860094070435, 0.30656543374061584, 0.3604159653186798]\n",
            "Updated weights: [0.335936576128006, 0.30456671118736267, 0.3594967722892761]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 26\n",
            "  Loss for client 0: 2.8258\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 26\n",
            "  Loss for client 1: 2.4383\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 26\n",
            "  Loss for client 2: 2.8308\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 27\n",
            "  Client 0: total grad norm = 57.092040 (253 params)\n",
            "  Client 1: total grad norm = 68.901900 (253 params)\n",
            "  Client 2: total grad norm = 67.704567 (253 params)\n",
            "\n",
            "=== Optimizer Step 27 ===\n",
            "\n",
            "=== Updating Client Weights (Step 27) ===\n",
            "Gradient norms: [7.543881416320801, 9.123638153076172, 9.99417495727539]\n",
            "Target weights: [0.38734495639801025, 0.32027626037597656, 0.2923787832260132]\n",
            "Updated weights: [0.351359099149704, 0.30927956104278564, 0.33936136960983276]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 27\n",
            "  Loss for client 0: 2.5780\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 27\n",
            "  Loss for client 1: 2.9804\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 27\n",
            "  Loss for client 2: 2.7801\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 28\n",
            "  Client 0: total grad norm = 63.837621 (253 params)\n",
            "  Client 1: total grad norm = 62.025806 (253 params)\n",
            "  Client 2: total grad norm = 53.070191 (253 params)\n",
            "\n",
            "=== Optimizer Step 28 ===\n",
            "\n",
            "=== Updating Client Weights (Step 28) ===\n",
            "Gradient norms: [9.031396865844727, 10.025473594665527, 8.009599685668945]\n",
            "Target weights: [0.3302055597305298, 0.2974640130996704, 0.3723303973674774]\n",
            "Updated weights: [0.3450130522251129, 0.30573490262031555, 0.3492520749568939]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 28\n",
            "  Loss for client 0: 2.5483\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 28\n",
            "  Loss for client 1: 3.1781\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 28\n",
            "  Loss for client 2: 2.9326\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 29\n",
            "  Client 0: total grad norm = 59.417480 (253 params)\n",
            "  Client 1: total grad norm = 80.292419 (253 params)\n",
            "  Client 2: total grad norm = 66.791289 (253 params)\n",
            "\n",
            "=== Optimizer Step 29 ===\n",
            "\n",
            "=== Updating Client Weights (Step 29) ===\n",
            "Gradient norms: [7.549070358276367, 10.133376121520996, 9.216275215148926]\n",
            "Target weights: [0.3900044858455658, 0.29054200649261475, 0.31945350766181946]\n",
            "Updated weights: [0.35851049423217773, 0.3011770248413086, 0.34031251072883606]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 29\n",
            "  Loss for client 0: 2.3892\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 29\n",
            "  Loss for client 1: 2.8292\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 29\n",
            "  Loss for client 2: 2.4762\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 30\n",
            "  Client 0: total grad norm = 146.869974 (253 params)\n",
            "  Client 1: total grad norm = 75.340691 (253 params)\n",
            "  Client 2: total grad norm = 72.330229 (253 params)\n",
            "\n",
            "=== Optimizer Step 30 ===\n",
            "\n",
            "=== Updating Client Weights (Step 30) ===\n",
            "Gradient norms: [17.493330001831055, 12.16797161102295, 7.808629989624023]\n",
            "Target weights: [0.21377068758010864, 0.3073282241821289, 0.47890105843544006]\n",
            "Updated weights: [0.31508854031562805, 0.3030223846435547, 0.38188907504081726]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 30\n",
            "  Loss for client 0: 2.2951\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 30\n",
            "  Loss for client 1: 2.4654\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 30\n",
            "  Loss for client 2: 2.6779\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 31\n",
            "  Client 0: total grad norm = 92.918126 (253 params)\n",
            "  Client 1: total grad norm = 71.779640 (253 params)\n",
            "  Client 2: total grad norm = 61.261454 (253 params)\n",
            "\n",
            "=== Optimizer Step 31 ===\n",
            "\n",
            "=== Updating Client Weights (Step 31) ===\n",
            "Gradient norms: [10.185667037963867, 9.026501655578613, 7.697329044342041]\n",
            "Target weights: [0.2897130846977234, 0.326917439699173, 0.38336947560310364]\n",
            "Updated weights: [0.3074759244918823, 0.310190886259079, 0.3823332190513611]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 31\n",
            "  Loss for client 0: 2.2166\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 31\n",
            "  Loss for client 1: 2.3728\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 31\n",
            "  Loss for client 2: 2.2393\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 32\n",
            "  Client 0: total grad norm = 62.711216 (253 params)\n",
            "  Client 1: total grad norm = 103.059072 (253 params)\n",
            "  Client 2: total grad norm = 80.926493 (253 params)\n",
            "\n",
            "=== Optimizer Step 32 ===\n",
            "\n",
            "=== Updating Client Weights (Step 32) ===\n",
            "Gradient norms: [7.801406383514404, 10.872767448425293, 10.797544479370117]\n",
            "Target weights: [0.4098302125930786, 0.2940605580806732, 0.2961091995239258]\n",
            "Updated weights: [0.3381822109222412, 0.30535179376602173, 0.35646602511405945]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 32\n",
            "  Loss for client 0: 2.6504\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 32\n",
            "  Loss for client 1: 2.4699\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 32\n",
            "  Loss for client 2: 2.2363\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 33\n",
            "  Client 0: total grad norm = 86.823992 (253 params)\n",
            "  Client 1: total grad norm = 75.540582 (253 params)\n",
            "  Client 2: total grad norm = 73.769411 (253 params)\n",
            "\n",
            "=== Optimizer Step 33 ===\n",
            "\n",
            "=== Updating Client Weights (Step 33) ===\n",
            "Gradient norms: [10.905284881591797, 8.981204986572266, 8.894474983215332]\n",
            "Target weights: [0.2906716465950012, 0.35294339060783386, 0.35638493299484253]\n",
            "Updated weights: [0.3239290416240692, 0.3196292817592621, 0.3564417064189911]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 33\n",
            "  Loss for client 0: 2.4094\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 33\n",
            "  Loss for client 1: 2.7266\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 33\n",
            "  Loss for client 2: 2.5512\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 34\n",
            "  Client 0: total grad norm = 73.193600 (253 params)\n",
            "  Client 1: total grad norm = 68.872012 (253 params)\n",
            "  Client 2: total grad norm = 72.221622 (253 params)\n",
            "\n",
            "=== Optimizer Step 34 ===\n",
            "\n",
            "=== Updating Client Weights (Step 34) ===\n",
            "Gradient norms: [9.727105140686035, 9.30013656616211, 9.426857948303223]\n",
            "Target weights: [0.32491159439086914, 0.3398282527923584, 0.3352600932121277]\n",
            "Updated weights: [0.3242238163948059, 0.3256889581680298, 0.3500872254371643]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 34\n",
            "  Loss for client 0: 2.3285\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 34\n",
            "  Loss for client 1: 2.6819\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 34\n",
            "  Loss for client 2: 2.8778\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 35\n",
            "  Client 0: total grad norm = 75.767846 (253 params)\n",
            "  Client 1: total grad norm = 78.042931 (253 params)\n",
            "  Client 2: total grad norm = 88.575093 (253 params)\n",
            "\n",
            "=== Optimizer Step 35 ===\n",
            "\n",
            "=== Updating Client Weights (Step 35) ===\n",
            "Gradient norms: [8.773722648620605, 9.059892654418945, 7.927919864654541]\n",
            "Target weights: [0.3251926302909851, 0.31492096185684204, 0.3598863482475281]\n",
            "Updated weights: [0.3245144784450531, 0.3224585950374603, 0.35302698612213135]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 35\n",
            "  Loss for client 0: 2.0804\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 35\n",
            "  Loss for client 1: 2.1365\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 35\n",
            "  Loss for client 2: 2.3137\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 36\n",
            "  Client 0: total grad norm = 91.118719 (253 params)\n",
            "  Client 1: total grad norm = 86.311328 (253 params)\n",
            "  Client 2: total grad norm = 79.082363 (253 params)\n",
            "\n",
            "=== Optimizer Step 36 ===\n",
            "\n",
            "=== Updating Client Weights (Step 36) ===\n",
            "Gradient norms: [9.840607643127441, 10.612326622009277, 9.82321548461914]\n",
            "Target weights: [0.3414074778556824, 0.316580593585968, 0.3420119285583496]\n",
            "Updated weights: [0.3295823633670807, 0.3206951916217804, 0.3497224748134613]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 36\n",
            "  Loss for client 0: 1.8663\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 36\n",
            "  Loss for client 1: 2.7861\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 36\n",
            "  Loss for client 2: 3.0167\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 37\n",
            "  Client 0: total grad norm = 74.957451 (253 params)\n",
            "  Client 1: total grad norm = 80.296218 (253 params)\n",
            "  Client 2: total grad norm = 84.423705 (253 params)\n",
            "\n",
            "=== Optimizer Step 37 ===\n",
            "\n",
            "=== Updating Client Weights (Step 37) ===\n",
            "Gradient norms: [9.113997459411621, 9.389297485351562, 9.252279281616211]\n",
            "Target weights: [0.33832547068595886, 0.32840555906295776, 0.3332689702510834]\n",
            "Updated weights: [0.33220529556274414, 0.32300829887390137, 0.3447864055633545]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 37\n",
            "  Loss for client 0: 1.7047\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 37\n",
            "  Loss for client 1: 2.8095\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 37\n",
            "  Loss for client 2: 2.3555\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 38\n",
            "  Client 0: total grad norm = 129.058423 (253 params)\n",
            "  Client 1: total grad norm = 80.186732 (253 params)\n",
            "  Client 2: total grad norm = 85.561028 (253 params)\n",
            "\n",
            "=== Optimizer Step 38 ===\n",
            "\n",
            "=== Updating Client Weights (Step 38) ===\n",
            "Gradient norms: [13.15645980834961, 8.956905364990234, 10.027661323547363]\n",
            "Target weights: [0.2644887864589691, 0.38849756121635437, 0.3470137119293213]\n",
            "Updated weights: [0.3118903636932373, 0.34265509247779846, 0.345454603433609]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 38\n",
            "  Loss for client 0: 2.6864\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 38\n",
            "  Loss for client 1: 2.3388\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 38\n",
            "  Loss for client 2: 2.8701\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 39\n",
            "  Client 0: total grad norm = 94.375500 (253 params)\n",
            "  Client 1: total grad norm = 168.881384 (253 params)\n",
            "  Client 2: total grad norm = 91.963448 (253 params)\n",
            "\n",
            "=== Optimizer Step 39 ===\n",
            "\n",
            "=== Updating Client Weights (Step 39) ===\n",
            "Gradient norms: [10.317598342895508, 23.854970932006836, 9.99136734008789]\n",
            "Target weights: [0.40565237402915955, 0.1754501461982727, 0.41889744997024536]\n",
            "Updated weights: [0.34001895785331726, 0.292493611574173, 0.36748746037483215]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 39\n",
            "  Loss for client 0: 1.8579\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 39\n",
            "  Loss for client 1: 2.5851\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 39\n",
            "  Loss for client 2: 1.9821\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 40\n",
            "  Client 0: total grad norm = 74.283466 (253 params)\n",
            "  Client 1: total grad norm = 92.410107 (253 params)\n",
            "  Client 2: total grad norm = 72.158140 (253 params)\n",
            "\n",
            "=== Optimizer Step 40 ===\n",
            "\n",
            "=== Updating Client Weights (Step 40) ===\n",
            "Gradient norms: [8.180721282958984, 10.423336029052734, 9.31374740600586]\n",
            "Target weights: [0.3754887580871582, 0.29470109939575195, 0.3298102021217346]\n",
            "Updated weights: [0.35065990686416626, 0.29315584897994995, 0.3561842739582062]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 40\n",
            "  Loss for client 0: 1.7397\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 40\n",
            "  Loss for client 1: 2.7375\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 40\n",
            "  Loss for client 2: 2.4115\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 41\n",
            "  Client 0: total grad norm = 72.026034 (253 params)\n",
            "  Client 1: total grad norm = 108.046672 (253 params)\n",
            "  Client 2: total grad norm = 76.646521 (253 params)\n",
            "\n",
            "=== Optimizer Step 41 ===\n",
            "\n",
            "=== Updating Client Weights (Step 41) ===\n",
            "Gradient norms: [7.29764986038208, 14.628071784973145, 8.52380657196045]\n",
            "Target weights: [0.4246232509613037, 0.21183595061302185, 0.36354079842567444]\n",
            "Updated weights: [0.3728489279747009, 0.2687598764896393, 0.3583912253379822]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 41\n",
            "  Loss for client 0: 2.3529\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 41\n",
            "  Loss for client 1: 2.5815\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 41\n",
            "  Loss for client 2: 2.0816\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 42\n",
            "  Client 0: total grad norm = 74.194418 (253 params)\n",
            "  Client 1: total grad norm = 93.171338 (253 params)\n",
            "  Client 2: total grad norm = 90.932857 (253 params)\n",
            "\n",
            "=== Optimizer Step 42 ===\n",
            "\n",
            "=== Updating Client Weights (Step 42) ===\n",
            "Gradient norms: [8.631464958190918, 11.642364501953125, 9.212817192077637]\n",
            "Target weights: [0.37337371706962585, 0.2768133580684662, 0.3498128652572632]\n",
            "Updated weights: [0.3730063736438751, 0.27117592096328735, 0.3558177351951599]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 42\n",
            "  Loss for client 0: 2.4178\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 42\n",
            "  Loss for client 1: 2.2926\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 42\n",
            "  Loss for client 2: 2.6646\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 43\n",
            "  Client 0: total grad norm = 70.727252 (253 params)\n",
            "  Client 1: total grad norm = 102.391693 (253 params)\n",
            "  Client 2: total grad norm = 67.087057 (253 params)\n",
            "\n",
            "=== Optimizer Step 43 ===\n",
            "\n",
            "=== Updating Client Weights (Step 43) ===\n",
            "Gradient norms: [7.719142913818359, 12.961089134216309, 7.795766353607178]\n",
            "Target weights: [0.3867373764514923, 0.23032641410827637, 0.3829362094402313]\n",
            "Updated weights: [0.37712568044662476, 0.2589210569858551, 0.36395329236984253]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 43\n",
            "  Loss for client 0: 1.7014\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 43\n",
            "  Loss for client 1: 2.5596\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 43\n",
            "  Loss for client 2: 2.5411\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 44\n",
            "  Client 0: total grad norm = 60.938932 (253 params)\n",
            "  Client 1: total grad norm = 103.733321 (253 params)\n",
            "  Client 2: total grad norm = 73.128045 (253 params)\n",
            "\n",
            "=== Optimizer Step 44 ===\n",
            "\n",
            "=== Updating Client Weights (Step 44) ===\n",
            "Gradient norms: [6.447353839874268, 11.524169921875, 8.348010063171387]\n",
            "Target weights: [0.42885586619377136, 0.23992925882339478, 0.33121493458747864]\n",
            "Updated weights: [0.3926447033882141, 0.2532235085964203, 0.3541317880153656]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 44\n",
            "  Loss for client 0: 2.4215\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 44\n",
            "  Loss for client 1: 2.0153\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 44\n",
            "  Loss for client 2: 2.5630\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 45\n",
            "  Client 0: total grad norm = 81.353841 (253 params)\n",
            "  Client 1: total grad norm = 92.626739 (253 params)\n",
            "  Client 2: total grad norm = 75.977023 (253 params)\n",
            "\n",
            "=== Optimizer Step 45 ===\n",
            "\n",
            "=== Updating Client Weights (Step 45) ===\n",
            "Gradient norms: [9.37399959564209, 10.559579849243164, 9.010336875915527]\n",
            "Target weights: [0.3415201008319855, 0.30317583680152893, 0.3553040623664856]\n",
            "Updated weights: [0.37730732560157776, 0.26820921897888184, 0.3544834852218628]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 45\n",
            "  Loss for client 0: 2.4891\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 45\n",
            "  Loss for client 1: 2.0020\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 45\n",
            "  Loss for client 2: 2.8751\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 46\n",
            "  Client 0: total grad norm = 86.717302 (253 params)\n",
            "  Client 1: total grad norm = 70.769320 (253 params)\n",
            "  Client 2: total grad norm = 82.122380 (253 params)\n",
            "\n",
            "=== Optimizer Step 46 ===\n",
            "\n",
            "=== Updating Client Weights (Step 46) ===\n",
            "Gradient norms: [11.533979415893555, 7.835639476776123, 9.898653984069824]\n",
            "Target weights: [0.2749370336532593, 0.4047044813632965, 0.3203585147857666]\n",
            "Updated weights: [0.34659624099731445, 0.3091577887535095, 0.3442460000514984]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 46\n",
            "  Loss for client 0: 1.7430\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 46\n",
            "  Loss for client 1: 3.0948\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 46\n",
            "  Loss for client 2: 2.4152\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 47\n",
            "  Client 0: total grad norm = 79.241337 (253 params)\n",
            "  Client 1: total grad norm = 74.446027 (253 params)\n",
            "  Client 2: total grad norm = 69.700197 (253 params)\n",
            "\n",
            "=== Optimizer Step 47 ===\n",
            "\n",
            "=== Updating Client Weights (Step 47) ===\n",
            "Gradient norms: [9.273303985595703, 8.584380149841309, 8.71709156036377]\n",
            "Target weights: [0.3180600702762604, 0.34358537197113037, 0.33835455775260925]\n",
            "Updated weights: [0.3380354046821594, 0.3194860517978668, 0.34247857332229614]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 47\n",
            "  Loss for client 0: 1.9223\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 47\n",
            "  Loss for client 1: 2.2308\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 47\n",
            "  Loss for client 2: 2.3371\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 48\n",
            "  Client 0: total grad norm = 72.695725 (253 params)\n",
            "  Client 1: total grad norm = 70.241680 (253 params)\n",
            "  Client 2: total grad norm = 70.938305 (253 params)\n",
            "\n",
            "=== Optimizer Step 48 ===\n",
            "\n",
            "=== Updating Client Weights (Step 48) ===\n",
            "Gradient norms: [8.274697303771973, 8.649069786071777, 8.098458290100098]\n",
            "Target weights: [0.3357420265674591, 0.32120952010154724, 0.34304845333099365]\n",
            "Updated weights: [0.3373473882675171, 0.32000309228897095, 0.34264951944351196]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 48\n",
            "  Loss for client 0: 2.0842\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 48\n",
            "  Loss for client 1: 2.5496\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 48\n",
            "  Loss for client 2: 1.8587\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 49\n",
            "  Client 0: total grad norm = 69.467753 (253 params)\n",
            "  Client 1: total grad norm = 76.124299 (253 params)\n",
            "  Client 2: total grad norm = 70.278457 (253 params)\n",
            "\n",
            "=== Optimizer Step 49 ===\n",
            "\n",
            "=== Updating Client Weights (Step 49) ===\n",
            "Gradient norms: [8.767802238464355, 9.60888385772705, 9.658709526062012]\n",
            "Target weights: [0.3545810580253601, 0.3235439658164978, 0.3218749463558197]\n",
            "Updated weights: [0.3425174951553345, 0.32106536626815796, 0.33641713857650757]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 49\n",
            "  Loss for client 0: 2.2621\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 49\n",
            "  Loss for client 1: 2.7000\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 49\n",
            "  Loss for client 2: 2.5133\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 50\n",
            "  Client 0: total grad norm = 64.702095 (253 params)\n",
            "  Client 1: total grad norm = 77.662577 (253 params)\n",
            "  Client 2: total grad norm = 74.698846 (253 params)\n",
            "\n",
            "=== Optimizer Step 50 ===\n",
            "\n",
            "=== Updating Client Weights (Step 50) ===\n",
            "Gradient norms: [7.765692234039307, 10.045124053955078, 9.471912384033203]\n",
            "Target weights: [0.38566169142723083, 0.29814764857292175, 0.3161906599998474]\n",
            "Updated weights: [0.3554607629776001, 0.3141900300979614, 0.3303492069244385]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 50\n",
            "  Val Loss = 2.3732 (5 batches)\n",
            "\n",
            "[Processing] Client 0, batch 50\n",
            "  Loss for client 0: 2.1243\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 50\n",
            "  Loss for client 1: 2.5498\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 50\n",
            "  Loss for client 2: 2.4226\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 51\n",
            "  Client 0: total grad norm = 70.525592 (253 params)\n",
            "  Client 1: total grad norm = 68.475170 (253 params)\n",
            "  Client 2: total grad norm = 68.901662 (253 params)\n",
            "\n",
            "=== Optimizer Step 51 ===\n",
            "\n",
            "=== Updating Client Weights (Step 51) ===\n",
            "Gradient norms: [7.9487690925598145, 8.386360168457031, 9.117137908935547]\n",
            "Target weights: [0.35465142130851746, 0.3361460864543915, 0.30920252203941345]\n",
            "Updated weights: [0.35521796345710754, 0.3207768499851227, 0.3240051865577698]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 51\n",
            "  Loss for client 0: 2.0106\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 51\n",
            "  Loss for client 1: 1.3757\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 51\n",
            "  Loss for client 2: 2.5564\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 52\n",
            "  Client 0: total grad norm = 84.554981 (253 params)\n",
            "  Client 1: total grad norm = 63.146204 (253 params)\n",
            "  Client 2: total grad norm = 81.781293 (253 params)\n",
            "\n",
            "=== Optimizer Step 52 ===\n",
            "\n",
            "=== Updating Client Weights (Step 52) ===\n",
            "Gradient norms: [9.805272102355957, 7.634384632110596, 9.778730392456055]\n",
            "Target weights: [0.3042222261428833, 0.3907298147678375, 0.3050479292869568]\n",
            "Updated weights: [0.33991923928260803, 0.3417627513408661, 0.3183180093765259]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 52\n",
            "  Loss for client 0: 1.9364\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 52\n",
            "  Loss for client 1: 1.9865\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 52\n",
            "  Loss for client 2: 2.6576\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 53\n",
            "  Client 0: total grad norm = 77.288675 (253 params)\n",
            "  Client 1: total grad norm = 61.264824 (253 params)\n",
            "  Client 2: total grad norm = 78.647103 (253 params)\n",
            "\n",
            "=== Optimizer Step 53 ===\n",
            "\n",
            "=== Updating Client Weights (Step 53) ===\n",
            "Gradient norms: [9.774444580078125, 7.35761022567749, 10.71054458618164]\n",
            "Target weights: [0.3085390031337738, 0.4098881781101227, 0.28157275915145874]\n",
            "Updated weights: [0.3305051922798157, 0.36220037937164307, 0.30729442834854126]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 53\n",
            "  Loss for client 0: 1.8702\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 53\n",
            "  Loss for client 1: 2.8338\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 53\n",
            "  Loss for client 2: 2.4263\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 54\n",
            "  Client 0: total grad norm = 64.881924 (253 params)\n",
            "  Client 1: total grad norm = 74.849804 (253 params)\n",
            "  Client 2: total grad norm = 72.037604 (253 params)\n",
            "\n",
            "=== Optimizer Step 54 ===\n",
            "\n",
            "=== Updating Client Weights (Step 54) ===\n",
            "Gradient norms: [6.5454421043396, 8.962790489196777, 7.642741680145264]\n",
            "Target weights: [0.38659048080444336, 0.2823233902454376, 0.33108609914779663]\n",
            "Updated weights: [0.347330778837204, 0.33823728561401367, 0.31443193554878235]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 54\n",
            "  Loss for client 0: 2.5268\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 54\n",
            "  Loss for client 1: 2.9768\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 54\n",
            "  Loss for client 2: 2.3242\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 55\n",
            "  Client 0: total grad norm = 74.647438 (253 params)\n",
            "  Client 1: total grad norm = 79.209081 (253 params)\n",
            "  Client 2: total grad norm = 80.705305 (253 params)\n",
            "\n",
            "=== Optimizer Step 55 ===\n",
            "\n",
            "=== Updating Client Weights (Step 55) ===\n",
            "Gradient norms: [9.851272583007812, 9.280407905578613, 10.580946922302246]\n",
            "Target weights: [0.33416298031806946, 0.35471832752227783, 0.3111187219619751]\n",
            "Updated weights: [0.3433804512023926, 0.3431816101074219, 0.31343796849250793]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 55\n",
            "  Loss for client 0: 1.9432\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 55\n",
            "  Loss for client 1: 2.8409\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 55\n",
            "  Loss for client 2: 2.3067\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 56\n",
            "  Client 0: total grad norm = 68.923746 (253 params)\n",
            "  Client 1: total grad norm = 83.543189 (253 params)\n",
            "  Client 2: total grad norm = 73.862368 (253 params)\n",
            "\n",
            "=== Optimizer Step 56 ===\n",
            "\n",
            "=== Updating Client Weights (Step 56) ===\n",
            "Gradient norms: [8.759086608886719, 10.370172500610352, 8.964428901672363]\n",
            "Target weights: [0.35439175367355347, 0.2993342876434326, 0.34627392888069153]\n",
            "Updated weights: [0.3466838300228119, 0.33002740144729614, 0.32328876852989197]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 56\n",
            "  Loss for client 0: 1.7216\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 56\n",
            "  Loss for client 1: 2.8574\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 56\n",
            "  Loss for client 2: 1.8207\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 57\n",
            "  Client 0: total grad norm = 95.427116 (253 params)\n",
            "  Client 1: total grad norm = 85.690191 (253 params)\n",
            "  Client 2: total grad norm = 73.381237 (253 params)\n",
            "\n",
            "=== Optimizer Step 57 ===\n",
            "\n",
            "=== Updating Client Weights (Step 57) ===\n",
            "Gradient norms: [12.971479415893555, 9.72671127319336, 7.920345783233643]\n",
            "Target weights: [0.25180450081825256, 0.33580484986305237, 0.41239064931869507]\n",
            "Updated weights: [0.31822001934051514, 0.3317606449127197, 0.35001933574676514]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 57\n",
            "  Loss for client 0: 2.3340\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 57\n",
            "  Loss for client 1: 2.0034\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 57\n",
            "  Loss for client 2: 2.4932\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 58\n",
            "  Client 0: total grad norm = 85.384127 (253 params)\n",
            "  Client 1: total grad norm = 68.079065 (253 params)\n",
            "  Client 2: total grad norm = 75.866899 (253 params)\n",
            "\n",
            "=== Optimizer Step 58 ===\n",
            "\n",
            "=== Updating Client Weights (Step 58) ===\n",
            "Gradient norms: [10.715359687805176, 7.248229503631592, 9.146061897277832]\n",
            "Target weights: [0.2739783525466919, 0.4050336182117462, 0.3209880590438843]\n",
            "Updated weights: [0.30494752526283264, 0.3537425398826599, 0.34130996465682983]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 58\n",
            "  Loss for client 0: 2.2129\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 58\n",
            "  Loss for client 1: 2.1539\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 58\n",
            "  Loss for client 2: 2.4798\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 59\n",
            "  Client 0: total grad norm = 80.459192 (253 params)\n",
            "  Client 1: total grad norm = 77.255725 (253 params)\n",
            "  Client 2: total grad norm = 72.703060 (253 params)\n",
            "\n",
            "=== Optimizer Step 59 ===\n",
            "\n",
            "=== Updating Client Weights (Step 59) ===\n",
            "Gradient norms: [9.222762107849121, 8.972441673278809, 9.251615524291992]\n",
            "Target weights: [0.3306025564670563, 0.33982595801353455, 0.3295714855194092]\n",
            "Updated weights: [0.31264403462409973, 0.34956756234169006, 0.3377884328365326]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 59\n",
            "  Loss for client 0: 1.3053\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 59\n",
            "  Loss for client 1: 2.5016\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 59\n",
            "  Loss for client 2: 2.4345\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 60\n",
            "  Client 0: total grad norm = 98.666035 (253 params)\n",
            "  Client 1: total grad norm = 93.828486 (253 params)\n",
            "  Client 2: total grad norm = 88.723791 (253 params)\n",
            "\n",
            "=== Optimizer Step 60 ===\n",
            "\n",
            "=== Updating Client Weights (Step 60) ===\n",
            "Gradient norms: [9.78395938873291, 9.663763046264648, 11.627313613891602]\n",
            "Target weights: [0.3503975570201874, 0.3547557294368744, 0.2948467433452606]\n",
            "Updated weights: [0.32397007942199707, 0.35112401843070984, 0.3249059319496155]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 60\n",
            "  Loss for client 0: 1.3398\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 60\n",
            "  Loss for client 1: 2.6181\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 60\n",
            "  Loss for client 2: 2.5360\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 61\n",
            "  Client 0: total grad norm = 57.025651 (253 params)\n",
            "  Client 1: total grad norm = 77.661097 (253 params)\n",
            "  Client 2: total grad norm = 73.208345 (253 params)\n",
            "\n",
            "=== Optimizer Step 61 ===\n",
            "\n",
            "=== Updating Client Weights (Step 61) ===\n",
            "Gradient norms: [6.419262886047363, 8.719408988952637, 7.752568244934082]\n",
            "Target weights: [0.3899818956851959, 0.2871062159538269, 0.3229118883609772]\n",
            "Updated weights: [0.34377363324165344, 0.3319186866283417, 0.32430770993232727]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 61\n",
            "  Loss for client 0: 1.4302\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 61\n",
            "  Loss for client 1: 2.0687\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 61\n",
            "  Loss for client 2: 2.6216\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 62\n",
            "  Client 0: total grad norm = 68.171871 (253 params)\n",
            "  Client 1: total grad norm = 72.235920 (253 params)\n",
            "  Client 2: total grad norm = 95.260712 (253 params)\n",
            "\n",
            "=== Optimizer Step 62 ===\n",
            "\n",
            "=== Updating Client Weights (Step 62) ===\n",
            "Gradient norms: [6.8590898513793945, 9.004690170288086, 11.73216438293457]\n",
            "Target weights: [0.4261913597583771, 0.3246402442455292, 0.24916842579841614]\n",
            "Updated weights: [0.36849895119667053, 0.3297351598739624, 0.30176591873168945]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 62\n",
            "  Loss for client 0: 2.3972\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 62\n",
            "  Loss for client 1: 2.5637\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 62\n",
            "  Loss for client 2: 2.6901\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 63\n",
            "  Client 0: total grad norm = 94.006913 (253 params)\n",
            "  Client 1: total grad norm = 93.020029 (253 params)\n",
            "  Client 2: total grad norm = 86.340667 (253 params)\n",
            "\n",
            "=== Optimizer Step 63 ===\n",
            "\n",
            "=== Updating Client Weights (Step 63) ===\n",
            "Gradient norms: [11.367250442504883, 12.21396541595459, 10.904996871948242]\n",
            "Target weights: [0.3363531231880188, 0.3130359351634979, 0.3506108522415161]\n",
            "Updated weights: [0.3588551878929138, 0.3247253894805908, 0.316419392824173]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 63\n",
            "  Loss for client 0: 1.2756\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 63\n",
            "  Loss for client 1: 2.3092\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 63\n",
            "  Loss for client 2: 2.6541\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 64\n",
            "  Client 0: total grad norm = 61.328349 (253 params)\n",
            "  Client 1: total grad norm = 77.851944 (253 params)\n",
            "  Client 2: total grad norm = 80.369701 (253 params)\n",
            "\n",
            "=== Optimizer Step 64 ===\n",
            "\n",
            "=== Updating Client Weights (Step 64) ===\n",
            "Gradient norms: [7.122384071350098, 8.702713012695312, 9.1315336227417]\n",
            "Target weights: [0.38485419750213623, 0.31496840715408325, 0.30017733573913574]\n",
            "Updated weights: [0.3666548728942871, 0.32179829478263855, 0.31154677271842957]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 64\n",
            "  Loss for client 0: 2.1425\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 64\n",
            "  Loss for client 1: 2.1989\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 64\n",
            "  Loss for client 2: 2.1450\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 65\n",
            "  Client 0: total grad norm = 74.817204 (253 params)\n",
            "  Client 1: total grad norm = 82.656639 (253 params)\n",
            "  Client 2: total grad norm = 88.852666 (253 params)\n",
            "\n",
            "=== Optimizer Step 65 ===\n",
            "\n",
            "=== Updating Client Weights (Step 65) ===\n",
            "Gradient norms: [8.375712394714355, 9.591056823730469, 11.492142677307129]\n",
            "Target weights: [0.38430434465408325, 0.3356066942214966, 0.28008899092674255]\n",
            "Updated weights: [0.371949702501297, 0.3259408175945282, 0.3021094501018524]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 65\n",
            "  Loss for client 0: 1.5425\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 65\n",
            "  Loss for client 1: 2.3825\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 65\n",
            "  Loss for client 2: 2.5308\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 66\n",
            "  Client 0: total grad norm = 76.031216 (253 params)\n",
            "  Client 1: total grad norm = 86.675420 (253 params)\n",
            "  Client 2: total grad norm = 69.873143 (253 params)\n",
            "\n",
            "=== Optimizer Step 66 ===\n",
            "\n",
            "=== Updating Client Weights (Step 66) ===\n",
            "Gradient norms: [9.02227783203125, 10.259106636047363, 8.545424461364746]\n",
            "Target weights: [0.34068727493286133, 0.2996143400669098, 0.3596983850002289]\n",
            "Updated weights: [0.36257100105285645, 0.3180428743362427, 0.3193861246109009]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 66\n",
            "  Loss for client 0: 2.5809\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 66\n",
            "  Loss for client 1: 3.0892\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 66\n",
            "  Loss for client 2: 2.4335\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 67\n",
            "  Client 0: total grad norm = 126.504667 (253 params)\n",
            "  Client 1: total grad norm = 88.894520 (253 params)\n",
            "  Client 2: total grad norm = 70.695581 (253 params)\n",
            "\n",
            "=== Optimizer Step 67 ===\n",
            "\n",
            "=== Updating Client Weights (Step 67) ===\n",
            "Gradient norms: [15.964838981628418, 9.123871803283691, 9.096689224243164]\n",
            "Target weights: [0.22198522090911865, 0.3884270191192627, 0.38958773016929626]\n",
            "Updated weights: [0.320395290851593, 0.3391581177711487, 0.3404466211795807]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 67\n",
            "  Loss for client 0: 1.3580\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 67\n",
            "  Loss for client 1: 2.2811\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 67\n",
            "  Loss for client 2: 2.2129\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 68\n",
            "  Client 0: total grad norm = 61.751265 (253 params)\n",
            "  Client 1: total grad norm = 86.095492 (253 params)\n",
            "  Client 2: total grad norm = 84.734478 (253 params)\n",
            "\n",
            "=== Optimizer Step 68 ===\n",
            "\n",
            "=== Updating Client Weights (Step 68) ===\n",
            "Gradient norms: [7.276670932769775, 9.290888786315918, 11.439685821533203]\n",
            "Target weights: [0.41334351897239685, 0.32373273372650146, 0.2629237174987793]\n",
            "Updated weights: [0.34827977418899536, 0.3345305025577545, 0.3171897530555725]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 68\n",
            "  Loss for client 0: 2.2780\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 68\n",
            "  Loss for client 1: 2.0277\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 68\n",
            "  Loss for client 2: 2.3185\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 69\n",
            "  Client 0: total grad norm = 76.216293 (253 params)\n",
            "  Client 1: total grad norm = 79.005925 (253 params)\n",
            "  Client 2: total grad norm = 75.286814 (253 params)\n",
            "\n",
            "=== Optimizer Step 69 ===\n",
            "\n",
            "=== Updating Client Weights (Step 69) ===\n",
            "Gradient norms: [8.93414306640625, 10.437289237976074, 9.395475387573242]\n",
            "Target weights: [0.3562672734260559, 0.3049587309360504, 0.3387739956378937]\n",
            "Updated weights: [0.350676029920578, 0.32565897703170776, 0.3236650228500366]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 69\n",
            "  Loss for client 0: 1.8741\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 69\n",
            "  Loss for client 1: 2.0122\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 69\n",
            "  Loss for client 2: 1.9352\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 70\n",
            "  Client 0: total grad norm = 64.328961 (253 params)\n",
            "  Client 1: total grad norm = 71.359961 (253 params)\n",
            "  Client 2: total grad norm = 66.934939 (253 params)\n",
            "\n",
            "=== Optimizer Step 70 ===\n",
            "\n",
            "=== Updating Client Weights (Step 70) ===\n",
            "Gradient norms: [7.055697917938232, 8.34373664855957, 8.996306419372559]\n",
            "Target weights: [0.38024020195007324, 0.32154178619384766, 0.29821792244911194]\n",
            "Updated weights: [0.3595452904701233, 0.32442381978034973, 0.316030889749527]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 70\n",
            "  Loss for client 0: 1.3372\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 70\n",
            "  Loss for client 1: 1.8768\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 70\n",
            "  Loss for client 2: 2.2785\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 71\n",
            "  Client 0: total grad norm = 58.858369 (253 params)\n",
            "  Client 1: total grad norm = 73.950413 (253 params)\n",
            "  Client 2: total grad norm = 91.632882 (253 params)\n",
            "\n",
            "=== Optimizer Step 71 ===\n",
            "\n",
            "=== Updating Client Weights (Step 71) ===\n",
            "Gradient norms: [6.471285820007324, 7.097196102142334, 12.296014785766602]\n",
            "Target weights: [0.41015544533729553, 0.37398332357406616, 0.2158612459897995]\n",
            "Updated weights: [0.37472832202911377, 0.33929169178009033, 0.2859799861907959]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 71\n",
            "  Loss for client 0: 2.3196\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 71\n",
            "  Loss for client 1: 1.6575\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 71\n",
            "  Loss for client 2: 2.8022\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 72\n",
            "  Client 0: total grad norm = 83.962072 (253 params)\n",
            "  Client 1: total grad norm = 65.071504 (253 params)\n",
            "  Client 2: total grad norm = 78.445590 (253 params)\n",
            "\n",
            "=== Optimizer Step 72 ===\n",
            "\n",
            "=== Updating Client Weights (Step 72) ===\n",
            "Gradient norms: [9.205986976623535, 6.795341491699219, 8.614896774291992]\n",
            "Target weights: [0.29211050271987915, 0.39573660492897034, 0.3121529519557953]\n",
            "Updated weights: [0.34994298219680786, 0.3562251627445221, 0.29383188486099243]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 72\n",
            "  Loss for client 0: 2.4455\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 72\n",
            "  Loss for client 1: 2.5295\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 72\n",
            "  Loss for client 2: 2.4867\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 73\n",
            "  Client 0: total grad norm = 88.163628 (253 params)\n",
            "  Client 1: total grad norm = 82.858982 (253 params)\n",
            "  Client 2: total grad norm = 80.252855 (253 params)\n",
            "\n",
            "=== Optimizer Step 73 ===\n",
            "\n",
            "=== Updating Client Weights (Step 73) ===\n",
            "Gradient norms: [9.709212303161621, 9.050436019897461, 10.122846603393555]\n",
            "Target weights: [0.3298231363296509, 0.3538307845592499, 0.31634607911109924]\n",
            "Updated weights: [0.34390702843666077, 0.3555068373680115, 0.30058616399765015]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 73\n",
            "  Loss for client 0: 1.7730\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 73\n",
            "  Loss for client 1: 2.3750\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 73\n",
            "  Loss for client 2: 2.9058\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 74\n",
            "  Client 0: total grad norm = 92.808710 (253 params)\n",
            "  Client 1: total grad norm = 86.635269 (253 params)\n",
            "  Client 2: total grad norm = 92.607895 (253 params)\n",
            "\n",
            "=== Optimizer Step 74 ===\n",
            "\n",
            "=== Updating Client Weights (Step 74) ===\n",
            "Gradient norms: [10.326170921325684, 9.746862411499023, 10.69393253326416]\n",
            "Target weights: [0.33057352900505066, 0.35022127628326416, 0.3192051947116852]\n",
            "Updated weights: [0.3399069905281067, 0.35392117500305176, 0.30617186427116394]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 74\n",
            "  Loss for client 0: 2.2759\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 74\n",
            "  Loss for client 1: 2.1864\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 74\n",
            "  Loss for client 2: 2.2945\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 75\n",
            "  Client 0: total grad norm = 80.111334 (253 params)\n",
            "  Client 1: total grad norm = 74.189563 (253 params)\n",
            "  Client 2: total grad norm = 91.563107 (253 params)\n",
            "\n",
            "=== Optimizer Step 75 ===\n",
            "\n",
            "=== Updating Client Weights (Step 75) ===\n",
            "Gradient norms: [8.662561416625977, 8.656981468200684, 11.058423042297363]\n",
            "Target weights: [0.3591967225074768, 0.3594282567501068, 0.281374990940094]\n",
            "Updated weights: [0.3456939160823822, 0.35557329654693604, 0.29873281717300415]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 75\n",
            "  Loss for client 0: 2.7011\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 75\n",
            "  Loss for client 1: 2.2150\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 75\n",
            "  Loss for client 2: 1.9067\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 76\n",
            "  Client 0: total grad norm = 79.703877 (253 params)\n",
            "  Client 1: total grad norm = 70.499890 (253 params)\n",
            "  Client 2: total grad norm = 76.770893 (253 params)\n",
            "\n",
            "=== Optimizer Step 76 ===\n",
            "\n",
            "=== Updating Client Weights (Step 76) ===\n",
            "Gradient norms: [8.669071197509766, 7.588704586029053, 9.329621315002441]\n",
            "Target weights: [0.3255670666694641, 0.3719164729118347, 0.30251646041870117]\n",
            "Updated weights: [0.33965587615966797, 0.3604762554168701, 0.2998679280281067]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 76\n",
            "  Loss for client 0: 2.0019\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 76\n",
            "  Loss for client 1: 2.4566\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 76\n",
            "  Loss for client 2: 2.6549\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 77\n",
            "  Client 0: total grad norm = 76.001980 (253 params)\n",
            "  Client 1: total grad norm = 86.472412 (253 params)\n",
            "  Client 2: total grad norm = 85.431904 (253 params)\n",
            "\n",
            "=== Optimizer Step 77 ===\n",
            "\n",
            "=== Updating Client Weights (Step 77) ===\n",
            "Gradient norms: [8.422094345092773, 9.100170135498047, 10.333734512329102]\n",
            "Target weights: [0.3648972809314728, 0.33770790696144104, 0.29739484190940857]\n",
            "Updated weights: [0.3472282886505127, 0.3536457419395447, 0.299125999212265]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 77\n",
            "  Loss for client 0: 1.2793\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 77\n",
            "  Loss for client 1: 2.1142\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 77\n",
            "  Loss for client 2: 2.2539\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 78\n",
            "  Client 0: total grad norm = 59.112889 (253 params)\n",
            "  Client 1: total grad norm = 81.612831 (253 params)\n",
            "  Client 2: total grad norm = 90.060860 (253 params)\n",
            "\n",
            "=== Optimizer Step 78 ===\n",
            "\n",
            "=== Updating Client Weights (Step 78) ===\n",
            "Gradient norms: [7.044455528259277, 9.235997200012207, 10.620516777038574]\n",
            "Target weights: [0.41220033168792725, 0.3143923580646515, 0.27340731024742126]\n",
            "Updated weights: [0.36671990156173706, 0.3418697416782379, 0.2914103865623474]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 78\n",
            "  Loss for client 0: 2.6970\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 78\n",
            "  Loss for client 1: 1.9374\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 78\n",
            "  Loss for client 2: 2.2984\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 79\n",
            "  Client 0: total grad norm = 114.600740 (253 params)\n",
            "  Client 1: total grad norm = 89.618781 (253 params)\n",
            "  Client 2: total grad norm = 76.723593 (253 params)\n",
            "\n",
            "=== Optimizer Step 79 ===\n",
            "\n",
            "=== Updating Client Weights (Step 79) ===\n",
            "Gradient norms: [11.997381210327148, 10.743200302124023, 8.411111831665039]\n",
            "Target weights: [0.28223755955696106, 0.315186470746994, 0.4025759696960449]\n",
            "Updated weights: [0.3413751721382141, 0.3338647484779358, 0.3247600793838501]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 79\n",
            "  Loss for client 0: 2.0526\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 79\n",
            "  Loss for client 1: 2.8665\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 79\n",
            "  Loss for client 2: 2.2415\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 80\n",
            "  Client 0: total grad norm = 83.584134 (253 params)\n",
            "  Client 1: total grad norm = 79.152241 (253 params)\n",
            "  Client 2: total grad norm = 85.709417 (253 params)\n",
            "\n",
            "=== Optimizer Step 80 ===\n",
            "\n",
            "=== Updating Client Weights (Step 80) ===\n",
            "Gradient norms: [8.916272163391113, 8.472299575805664, 10.731622695922852]\n",
            "Target weights: [0.3468315601348877, 0.36500653624534607, 0.28816190361976624]\n",
            "Updated weights: [0.34301209449768066, 0.34320729970932007, 0.31378063559532166]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 80\n",
            "  Loss for client 0: 1.5060\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 80\n",
            "  Loss for client 1: 2.5938\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 80\n",
            "  Loss for client 2: 1.8863\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 81\n",
            "  Client 0: total grad norm = 64.472659 (253 params)\n",
            "  Client 1: total grad norm = 78.241880 (253 params)\n",
            "  Client 2: total grad norm = 74.942037 (253 params)\n",
            "\n",
            "=== Optimizer Step 81 ===\n",
            "\n",
            "=== Updating Client Weights (Step 81) ===\n",
            "Gradient norms: [7.320289611816406, 9.130755424499512, 8.85816764831543]\n",
            "Target weights: [0.3805020749568939, 0.30505532026290894, 0.31444260478019714]\n",
            "Updated weights: [0.35425907373428345, 0.3317617177963257, 0.31397920846939087]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 81\n",
            "  Loss for client 0: 1.6569\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 81\n",
            "  Loss for client 1: 1.7757\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 81\n",
            "  Loss for client 2: 1.8054\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 82\n",
            "  Client 0: total grad norm = 79.267728 (253 params)\n",
            "  Client 1: total grad norm = 72.855216 (253 params)\n",
            "  Client 2: total grad norm = 70.931278 (253 params)\n",
            "\n",
            "=== Optimizer Step 82 ===\n",
            "\n",
            "=== Updating Client Weights (Step 82) ===\n",
            "Gradient norms: [8.60412883758545, 7.8703413009643555, 8.184727668762207]\n",
            "Target weights: [0.31801795959472656, 0.34766823053359985, 0.3343138098716736]\n",
            "Updated weights: [0.3433867394924164, 0.33653366565704346, 0.32007959485054016]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 82\n",
            "  Loss for client 0: 2.2021\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 82\n",
            "  Loss for client 1: 1.9460\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 82\n",
            "  Loss for client 2: 2.3760\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 83\n",
            "  Client 0: total grad norm = 83.433468 (253 params)\n",
            "  Client 1: total grad norm = 81.132977 (253 params)\n",
            "  Client 2: total grad norm = 95.290589 (253 params)\n",
            "\n",
            "=== Optimizer Step 83 ===\n",
            "\n",
            "=== Updating Client Weights (Step 83) ===\n",
            "Gradient norms: [8.916618347167969, 9.903475761413574, 9.916449546813965]\n",
            "Target weights: [0.3572031855583191, 0.3216087520122528, 0.3211880028247833]\n",
            "Updated weights: [0.34753167629241943, 0.3320561945438385, 0.3204120993614197]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 83\n",
            "  Loss for client 0: 1.4410\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 83\n",
            "  Loss for client 1: 2.7095\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 83\n",
            "  Loss for client 2: 2.3881\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 84\n",
            "  Client 0: total grad norm = 55.935572 (253 params)\n",
            "  Client 1: total grad norm = 97.801841 (253 params)\n",
            "  Client 2: total grad norm = 102.560542 (253 params)\n",
            "\n",
            "=== Optimizer Step 84 ===\n",
            "\n",
            "=== Updating Client Weights (Step 84) ===\n",
            "Gradient norms: [5.913628578186035, 10.571235656738281, 11.401802062988281]\n",
            "Target weights: [0.4812169075012207, 0.26919636130332947, 0.24958670139312744]\n",
            "Updated weights: [0.38763725757598877, 0.3131982684135437, 0.29916447401046753]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 84\n",
            "  Loss for client 0: 1.6284\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 84\n",
            "  Loss for client 1: 2.2346\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 84\n",
            "  Loss for client 2: 2.3701\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 85\n",
            "  Client 0: total grad norm = 63.822221 (253 params)\n",
            "  Client 1: total grad norm = 86.663025 (253 params)\n",
            "  Client 2: total grad norm = 87.665967 (253 params)\n",
            "\n",
            "=== Optimizer Step 85 ===\n",
            "\n",
            "=== Updating Client Weights (Step 85) ===\n",
            "Gradient norms: [6.309189796447754, 11.073466300964355, 10.261979103088379]\n",
            "Target weights: [0.45775607228279114, 0.26080992817878723, 0.281434029340744]\n",
            "Updated weights: [0.40867286920547485, 0.2974817752838135, 0.2938453257083893]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 85\n",
            "  Loss for client 0: 1.2670\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 85\n",
            "  Loss for client 1: 2.0974\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 85\n",
            "  Loss for client 2: 1.5548\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 86\n",
            "  Client 0: total grad norm = 57.321413 (253 params)\n",
            "  Client 1: total grad norm = 84.283303 (253 params)\n",
            "  Client 2: total grad norm = 76.233841 (253 params)\n",
            "\n",
            "=== Optimizer Step 86 ===\n",
            "\n",
            "=== Updating Client Weights (Step 86) ===\n",
            "Gradient norms: [6.458252429962158, 8.989310264587402, 8.849784851074219]\n",
            "Target weights: [0.4084632098674774, 0.2934550642967224, 0.2980816662311554]\n",
            "Updated weights: [0.4086099863052368, 0.29627376794815063, 0.29511624574661255]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 86\n",
            "  Loss for client 0: 1.7233\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 86\n",
            "  Loss for client 1: 1.8317\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 86\n",
            "  Loss for client 2: 2.6659\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 87\n",
            "  Client 0: total grad norm = 77.344120 (253 params)\n",
            "  Client 1: total grad norm = 76.684790 (253 params)\n",
            "  Client 2: total grad norm = 86.317831 (253 params)\n",
            "\n",
            "=== Optimizer Step 87 ===\n",
            "\n",
            "=== Updating Client Weights (Step 87) ===\n",
            "Gradient norms: [7.65515661239624, 9.694089889526367, 8.32373046875]\n",
            "Target weights: [0.36909204721450806, 0.29146185517311096, 0.3394460678100586]\n",
            "Updated weights: [0.39675459265708923, 0.29483020305633545, 0.3084152042865753]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 87\n",
            "  Loss for client 0: 2.1696\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 87\n",
            "  Loss for client 1: 2.4012\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 87\n",
            "  Loss for client 2: 2.1156\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 88\n",
            "  Client 0: total grad norm = 76.966781 (253 params)\n",
            "  Client 1: total grad norm = 86.385698 (253 params)\n",
            "  Client 2: total grad norm = 80.983055 (253 params)\n",
            "\n",
            "=== Optimizer Step 88 ===\n",
            "\n",
            "=== Updating Client Weights (Step 88) ===\n",
            "Gradient norms: [8.477352142333984, 10.790868759155273, 7.781824111938477]\n",
            "Target weights: [0.3478281497955322, 0.27325528860092163, 0.37891653180122375]\n",
            "Updated weights: [0.3820766508579254, 0.2883577346801758, 0.3295656144618988]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 88\n",
            "  Loss for client 0: 1.6633\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 88\n",
            "  Loss for client 1: 1.7832\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 88\n",
            "  Loss for client 2: 2.3980\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 89\n",
            "  Client 0: total grad norm = 75.823117 (253 params)\n",
            "  Client 1: total grad norm = 76.592327 (253 params)\n",
            "  Client 2: total grad norm = 90.447129 (253 params)\n",
            "\n",
            "=== Optimizer Step 89 ===\n",
            "\n",
            "=== Updating Client Weights (Step 89) ===\n",
            "Gradient norms: [8.11299991607666, 8.997572898864746, 9.218856811523438]\n",
            "Target weights: [0.359488308429718, 0.3241461515426636, 0.3163655400276184]\n",
            "Updated weights: [0.3753001391887665, 0.2990942597389221, 0.3256056010723114]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 89\n",
            "  Loss for client 0: 2.2719\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 89\n",
            "  Loss for client 1: 1.8508\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 89\n",
            "  Loss for client 2: 2.1272\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 90\n",
            "  Client 0: total grad norm = 89.390285 (253 params)\n",
            "  Client 1: total grad norm = 72.562464 (253 params)\n",
            "  Client 2: total grad norm = 91.181104 (253 params)\n",
            "\n",
            "=== Optimizer Step 90 ===\n",
            "\n",
            "=== Updating Client Weights (Step 90) ===\n",
            "Gradient norms: [9.46843147277832, 8.774635314941406, 9.452787399291992]\n",
            "Target weights: [0.32459908723831177, 0.35026460886001587, 0.32513630390167236]\n",
            "Updated weights: [0.36008983850479126, 0.3144453763961792, 0.32546481490135193]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 90\n",
            "  Loss for client 0: 1.1908\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 90\n",
            "  Loss for client 1: 1.9469\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 90\n",
            "  Loss for client 2: 2.0252\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 91\n",
            "  Client 0: total grad norm = 70.275013 (253 params)\n",
            "  Client 1: total grad norm = 66.671926 (253 params)\n",
            "  Client 2: total grad norm = 87.508562 (253 params)\n",
            "\n",
            "=== Optimizer Step 91 ===\n",
            "\n",
            "=== Updating Client Weights (Step 91) ===\n",
            "Gradient norms: [6.5107903480529785, 7.071414947509766, 9.685303688049316]\n",
            "Target weights: [0.3856605887413025, 0.3550852835178375, 0.2592541575431824]\n",
            "Updated weights: [0.3677610456943512, 0.326637327671051, 0.3056015968322754]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 91\n",
            "  Loss for client 0: 1.5367\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 91\n",
            "  Loss for client 1: 2.4681\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 91\n",
            "  Loss for client 2: 2.4054\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 92\n",
            "  Client 0: total grad norm = 78.893638 (253 params)\n",
            "  Client 1: total grad norm = 83.593232 (253 params)\n",
            "  Client 2: total grad norm = 91.072655 (253 params)\n",
            "\n",
            "=== Optimizer Step 92 ===\n",
            "\n",
            "=== Updating Client Weights (Step 92) ===\n",
            "Gradient norms: [7.887450695037842, 9.28701114654541, 9.569150924682617]\n",
            "Target weights: [0.37403351068496704, 0.31766629219055176, 0.3083001375198364]\n",
            "Updated weights: [0.36964279413223267, 0.3239460289478302, 0.30641114711761475]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 92\n",
            "  Loss for client 0: 1.9208\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 92\n",
            "  Loss for client 1: 1.6521\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 92\n",
            "  Loss for client 2: 2.2192\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 93\n",
            "  Client 0: total grad norm = 76.528001 (253 params)\n",
            "  Client 1: total grad norm = 74.331001 (253 params)\n",
            "  Client 2: total grad norm = 81.479299 (253 params)\n",
            "\n",
            "=== Optimizer Step 93 ===\n",
            "\n",
            "=== Updating Client Weights (Step 93) ===\n",
            "Gradient norms: [9.56283950805664, 7.838348865509033, 9.493836402893066]\n",
            "Target weights: [0.30985894799232483, 0.378030002117157, 0.3121110498905182]\n",
            "Updated weights: [0.3517076373100281, 0.340171217918396, 0.30812111496925354]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 93\n",
            "  Loss for client 0: 2.0228\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 93\n",
            "  Loss for client 1: 1.8818\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 93\n",
            "  Loss for client 2: 1.8448\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 94\n",
            "  Client 0: total grad norm = 79.134125 (253 params)\n",
            "  Client 1: total grad norm = 89.233299 (253 params)\n",
            "  Client 2: total grad norm = 78.803624 (253 params)\n",
            "\n",
            "=== Optimizer Step 94 ===\n",
            "\n",
            "=== Updating Client Weights (Step 94) ===\n",
            "Gradient norms: [8.658512115478516, 9.523602485656738, 8.900562286376953]\n",
            "Target weights: [0.3469850420951843, 0.31546616554260254, 0.3375488221645355]\n",
            "Updated weights: [0.35029086470603943, 0.33275970816612244, 0.31694942712783813]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 94\n",
            "  Loss for client 0: 2.1533\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 94\n",
            "  Loss for client 1: 2.3340\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 94\n",
            "  Loss for client 2: 1.7949\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 95\n",
            "  Client 0: total grad norm = 95.661373 (253 params)\n",
            "  Client 1: total grad norm = 86.393823 (253 params)\n",
            "  Client 2: total grad norm = 73.956077 (253 params)\n",
            "\n",
            "=== Optimizer Step 95 ===\n",
            "\n",
            "=== Updating Client Weights (Step 95) ===\n",
            "Gradient norms: [9.014952659606934, 10.41350269317627, 7.809787273406982]\n",
            "Target weights: [0.3311243951320648, 0.28665387630462646, 0.3822217583656311]\n",
            "Updated weights: [0.34454092383384705, 0.31892794370651245, 0.3365311324596405]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 95\n",
            "  Loss for client 0: 1.4334\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 95\n",
            "  Loss for client 1: 2.0215\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 95\n",
            "  Loss for client 2: 2.3730\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 96\n",
            "  Client 0: total grad norm = 109.991935 (253 params)\n",
            "  Client 1: total grad norm = 72.399838 (253 params)\n",
            "  Client 2: total grad norm = 85.761463 (253 params)\n",
            "\n",
            "=== Optimizer Step 96 ===\n",
            "\n",
            "=== Updating Client Weights (Step 96) ===\n",
            "Gradient norms: [11.758208274841309, 7.613197326660156, 9.13653564453125]\n",
            "Target weights: [0.2610015571117401, 0.4031040668487549, 0.3358943462371826]\n",
            "Updated weights: [0.3194791376590729, 0.34418079257011414, 0.33634012937545776]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 96\n",
            "  Loss for client 0: 2.0282\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 96\n",
            "  Loss for client 1: 2.4863\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 96\n",
            "  Loss for client 2: 2.2859\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 97\n",
            "  Client 0: total grad norm = 84.935218 (253 params)\n",
            "  Client 1: total grad norm = 90.707828 (253 params)\n",
            "  Client 2: total grad norm = 92.786399 (253 params)\n",
            "\n",
            "=== Optimizer Step 97 ===\n",
            "\n",
            "=== Updating Client Weights (Step 97) ===\n",
            "Gradient norms: [7.788980484008789, 10.312856674194336, 8.76876449584961]\n",
            "Target weights: [0.3782816231250763, 0.2857043445110321, 0.3360140323638916]\n",
            "Updated weights: [0.3371198773384094, 0.326637864112854, 0.33624231815338135]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 97\n",
            "  Loss for client 0: 1.3382\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 97\n",
            "  Loss for client 1: 2.2472\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 97\n",
            "  Loss for client 2: 1.6386\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 98\n",
            "  Client 0: total grad norm = 61.082641 (253 params)\n",
            "  Client 1: total grad norm = 88.730536 (253 params)\n",
            "  Client 2: total grad norm = 79.827041 (253 params)\n",
            "\n",
            "=== Optimizer Step 98 ===\n",
            "\n",
            "=== Updating Client Weights (Step 98) ===\n",
            "Gradient norms: [6.774479866027832, 10.398073196411133, 7.983617305755615]\n",
            "Target weights: [0.3999903202056885, 0.2605988681316376, 0.33941084146499634]\n",
            "Updated weights: [0.3559809923171997, 0.3068261742591858, 0.3371928930282593]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 98\n",
            "  Loss for client 0: 1.9709\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 98\n",
            "  Loss for client 1: 2.0883\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 98\n",
            "  Loss for client 2: 1.9874\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 99\n",
            "  Client 0: total grad norm = 66.714842 (253 params)\n",
            "  Client 1: total grad norm = 83.594890 (253 params)\n",
            "  Client 2: total grad norm = 82.845169 (253 params)\n",
            "\n",
            "=== Optimizer Step 99 ===\n",
            "\n",
            "=== Updating Client Weights (Step 99) ===\n",
            "Gradient norms: [8.021845817565918, 10.468939781188965, 11.2405424118042]\n",
            "Target weights: [0.40324127674102783, 0.3089843988418579, 0.28777429461479187]\n",
            "Updated weights: [0.3701590895652771, 0.30747365951538086, 0.3223673105239868]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 99\n",
            "  Loss for client 0: 0.9914\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 99\n",
            "  Loss for client 1: 2.7582\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 99\n",
            "  Loss for client 2: 2.3791\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 100\n",
            "  Client 0: total grad norm = 63.737855 (253 params)\n",
            "  Client 1: total grad norm = 90.672721 (253 params)\n",
            "  Client 2: total grad norm = 87.693092 (253 params)\n",
            "\n",
            "=== Optimizer Step 100 ===\n",
            "\n",
            "=== Updating Client Weights (Step 100) ===\n",
            "Gradient norms: [6.870147228240967, 9.382966995239258, 10.907228469848633]\n",
            "Target weights: [0.4233584702014923, 0.3099803328514099, 0.26666125655174255]\n",
            "Updated weights: [0.38611888885498047, 0.3082256615161896, 0.30565550923347473]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 100\n",
            "  Val Loss = 2.2503 (5 batches)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62517]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ –æ–±—Ä–µ–∑–∞–Ω—ã –¥–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç—Ä–æ–∫ (5000).\u001b[0m\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 301\n",
            "  Loss for client 1: 1.5789\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 301\n",
            "  Loss for client 2: 1.5698\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 302\n",
            "  Client 0: total grad norm = 91.330610 (253 params)\n",
            "  Client 1: total grad norm = 74.259309 (253 params)\n",
            "  Client 2: total grad norm = 65.610002 (253 params)\n",
            "\n",
            "=== Optimizer Step 302 ===\n",
            "\n",
            "=== Updating Client Weights (Step 302) ===\n",
            "Gradient norms: [9.923552513122559, 8.342061996459961, 6.974485874176025]\n",
            "Target weights: [0.2768229842185974, 0.32930317521095276, 0.39387384057044983]\n",
            "Updated weights: [0.3193438649177551, 0.3433245122432709, 0.3373316526412964]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 302\n",
            "  Loss for client 0: 1.7203\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 302\n",
            "  Loss for client 1: 1.6709\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 302\n",
            "  Loss for client 2: 2.1032\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 303\n",
            "  Client 0: total grad norm = 79.169264 (253 params)\n",
            "  Client 1: total grad norm = 70.058516 (253 params)\n",
            "  Client 2: total grad norm = 95.724434 (253 params)\n",
            "\n",
            "=== Optimizer Step 303 ===\n",
            "\n",
            "=== Updating Client Weights (Step 303) ===\n",
            "Gradient norms: [9.820550918579102, 8.730224609375, 9.345328330993652]\n",
            "Target weights: [0.31488698720932007, 0.35421353578567505, 0.3308994174003601]\n",
            "Updated weights: [0.31800681352615356, 0.34659120440483093, 0.3354019820690155]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 303\n",
            "  Loss for client 0: 1.8458\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 303\n",
            "  Loss for client 1: 1.7060\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 303\n",
            "  Loss for client 2: 2.3183\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 304\n",
            "  Client 0: total grad norm = 90.152634 (253 params)\n",
            "  Client 1: total grad norm = 79.114947 (253 params)\n",
            "  Client 2: total grad norm = 118.286396 (253 params)\n",
            "\n",
            "=== Optimizer Step 304 ===\n",
            "\n",
            "=== Updating Client Weights (Step 304) ===\n",
            "Gradient norms: [10.891048431396484, 9.383871078491211, 10.025228500366211]\n",
            "Target weights: [0.30797868967056274, 0.35744425654411316, 0.3345769941806793]\n",
            "Updated weights: [0.3149983882904053, 0.34984710812568665, 0.3351544737815857]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 304\n",
            "  Loss for client 0: 1.5274\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 304\n",
            "  Loss for client 1: 1.9197\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 304\n",
            "  Loss for client 2: 2.4621\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 305\n",
            "  Client 0: total grad norm = 69.642497 (253 params)\n",
            "  Client 1: total grad norm = 80.900009 (253 params)\n",
            "  Client 2: total grad norm = 92.375702 (253 params)\n",
            "\n",
            "=== Optimizer Step 305 ===\n",
            "\n",
            "=== Updating Client Weights (Step 305) ===\n",
            "Gradient norms: [8.044885635375977, 8.92444133758545, 11.699708938598633]\n",
            "Target weights: [0.38624078035354614, 0.348174512386322, 0.26558464765548706]\n",
            "Updated weights: [0.33637112379074097, 0.349345326423645, 0.3142835199832916]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 305\n",
            "  Loss for client 0: 1.8764\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 305\n",
            "  Loss for client 1: 1.9152\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 305\n",
            "  Loss for client 2: 1.7318\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 306\n",
            "  Client 0: total grad norm = 78.731093 (253 params)\n",
            "  Client 1: total grad norm = 79.582144 (253 params)\n",
            "  Client 2: total grad norm = 88.819103 (253 params)\n",
            "\n",
            "=== Optimizer Step 306 ===\n",
            "\n",
            "=== Updating Client Weights (Step 306) ===\n",
            "Gradient norms: [9.824090957641602, 8.689004898071289, 9.049695014953613]\n",
            "Target weights: [0.31092533469200134, 0.35154300928115845, 0.3375316858291626]\n",
            "Updated weights: [0.32873740792274475, 0.350004643201828, 0.321258008480072]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 306\n",
            "  Loss for client 0: 1.7839\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 306\n",
            "  Loss for client 1: 1.6487\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 306\n",
            "  Loss for client 2: 1.4170\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 307\n",
            "  Client 0: total grad norm = 90.910754 (253 params)\n",
            "  Client 1: total grad norm = 79.190330 (253 params)\n",
            "  Client 2: total grad norm = 66.488749 (253 params)\n",
            "\n",
            "=== Optimizer Step 307 ===\n",
            "\n",
            "=== Updating Client Weights (Step 307) ===\n",
            "Gradient norms: [9.519543647766113, 8.57677936553955, 8.267691612243652]\n",
            "Target weights: [0.30662286281585693, 0.34032702445983887, 0.3530501425266266]\n",
            "Updated weights: [0.3221030533313751, 0.3471013605594635, 0.33079564571380615]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 307\n",
            "  Loss for client 0: 1.6572\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 307\n",
            "  Loss for client 1: 1.6075\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 307\n",
            "  Loss for client 2: 1.5479\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 308\n",
            "  Client 0: total grad norm = 88.854293 (253 params)\n",
            "  Client 1: total grad norm = 78.981913 (253 params)\n",
            "  Client 2: total grad norm = 84.950224 (253 params)\n",
            "\n",
            "=== Optimizer Step 308 ===\n",
            "\n",
            "=== Updating Client Weights (Step 308) ===\n",
            "Gradient norms: [9.742225646972656, 8.556684494018555, 9.31132698059082]\n",
            "Target weights: [0.31398865580558777, 0.35749226808547974, 0.3285190761089325]\n",
            "Updated weights: [0.319668710231781, 0.35021859407424927, 0.33011266589164734]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 308\n",
            "  Loss for client 0: 2.0776\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 308\n",
            "  Loss for client 1: 1.9282\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 308\n",
            "  Loss for client 2: 2.0401\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 309\n",
            "  Client 0: total grad norm = 92.718890 (253 params)\n",
            "  Client 1: total grad norm = 86.649601 (253 params)\n",
            "  Client 2: total grad norm = 88.649367 (253 params)\n",
            "\n",
            "=== Optimizer Step 309 ===\n",
            "\n",
            "=== Updating Client Weights (Step 309) ===\n",
            "Gradient norms: [10.095955848693848, 10.395454406738281, 10.208219528198242]\n",
            "Target weights: [0.33781591057777405, 0.32808324694633484, 0.3341008126735687]\n",
            "Updated weights: [0.32511287927627563, 0.3435779809951782, 0.33130910992622375]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 309\n",
            "  Loss for client 0: 1.4235\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 309\n",
            "  Loss for client 1: 2.0062\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 309\n",
            "  Loss for client 2: 1.9197\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 310\n",
            "  Client 0: total grad norm = 66.155706 (253 params)\n",
            "  Client 1: total grad norm = 75.089373 (253 params)\n",
            "  Client 2: total grad norm = 75.904848 (253 params)\n",
            "\n",
            "=== Optimizer Step 310 ===\n",
            "\n",
            "=== Updating Client Weights (Step 310) ===\n",
            "Gradient norms: [7.2810282707214355, 9.252204895019531, 7.263550281524658]\n",
            "Target weights: [0.35850557684898376, 0.28212618827819824, 0.359368234872818]\n",
            "Updated weights: [0.3351307213306427, 0.3251424729824066, 0.33972686529159546]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 310\n",
            "  Loss for client 0: 1.3329\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 310\n",
            "  Loss for client 1: 1.5717\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 310\n",
            "  Loss for client 2: 2.1142\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 311\n",
            "  Client 0: total grad norm = 73.776545 (253 params)\n",
            "  Client 1: total grad norm = 77.421157 (253 params)\n",
            "  Client 2: total grad norm = 89.881354 (253 params)\n",
            "\n",
            "=== Optimizer Step 311 ===\n",
            "\n",
            "=== Updating Client Weights (Step 311) ===\n",
            "Gradient norms: [8.476533889770508, 8.429774284362793, 10.1404447555542]\n",
            "Target weights: [0.35193172097206116, 0.35388386249542236, 0.29418444633483887]\n",
            "Updated weights: [0.3401710093021393, 0.3337648808956146, 0.3260641396045685]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 311\n",
            "  Loss for client 0: 1.4297\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 311\n",
            "  Loss for client 1: 1.9216\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 311\n",
            "  Loss for client 2: 2.3451\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 312\n",
            "  Client 0: total grad norm = 82.289143 (253 params)\n",
            "  Client 1: total grad norm = 92.768492 (253 params)\n",
            "  Client 2: total grad norm = 94.162480 (253 params)\n",
            "\n",
            "=== Optimizer Step 312 ===\n",
            "\n",
            "=== Updating Client Weights (Step 312) ===\n",
            "Gradient norms: [9.86428165435791, 8.34440803527832, 9.438277244567871]\n",
            "Target weights: [0.3098585903644562, 0.3662970960140228, 0.323844313621521]\n",
            "Updated weights: [0.3310772776603699, 0.3435245454311371, 0.32539820671081543]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 312\n",
            "  Loss for client 0: 1.3236\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 312\n",
            "  Loss for client 1: 2.1852\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 312\n",
            "  Loss for client 2: 1.4426\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 313\n",
            "  Client 0: total grad norm = 65.753192 (253 params)\n",
            "  Client 1: total grad norm = 94.887475 (253 params)\n",
            "  Client 2: total grad norm = 75.209263 (253 params)\n",
            "\n",
            "=== Optimizer Step 313 ===\n",
            "\n",
            "=== Updating Client Weights (Step 313) ===\n",
            "Gradient norms: [6.798141002655029, 9.092205047607422, 8.132959365844727]\n",
            "Target weights: [0.3870621621608734, 0.2894020974636078, 0.3235357701778412]\n",
            "Updated weights: [0.3478727340698242, 0.32728779315948486, 0.3248394727706909]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 313\n",
            "  Loss for client 0: 1.3515\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 313\n",
            "  Loss for client 1: 1.7866\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 313\n",
            "  Loss for client 2: 1.6129\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 314\n",
            "  Client 0: total grad norm = 81.784995 (253 params)\n",
            "  Client 1: total grad norm = 64.126755 (253 params)\n",
            "  Client 2: total grad norm = 73.496355 (253 params)\n",
            "\n",
            "=== Optimizer Step 314 ===\n",
            "\n",
            "=== Updating Client Weights (Step 314) ===\n",
            "Gradient norms: [8.121962547302246, 7.495722770690918, 7.860910892486572]\n",
            "Target weights: [0.3208464980125427, 0.34765201807022095, 0.33150145411491394]\n",
            "Updated weights: [0.33976486325263977, 0.3333970606327057, 0.32683807611465454]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 314\n",
            "  Loss for client 0: 1.8327\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 314\n",
            "  Loss for client 1: 1.5295\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 314\n",
            "  Loss for client 2: 2.0564\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 315\n",
            "  Client 0: total grad norm = 116.076495 (253 params)\n",
            "  Client 1: total grad norm = 76.463076 (253 params)\n",
            "  Client 2: total grad norm = 111.230195 (253 params)\n",
            "\n",
            "=== Optimizer Step 315 ===\n",
            "\n",
            "=== Updating Client Weights (Step 315) ===\n",
            "Gradient norms: [13.185234069824219, 9.063615798950195, 11.182050704956055]\n",
            "Target weights: [0.27518749237060547, 0.4003271460533142, 0.32448533177375793]\n",
            "Updated weights: [0.3203916549682617, 0.35347607731819153, 0.32613223791122437]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 315\n",
            "  Loss for client 0: 1.4690\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 315\n",
            "  Loss for client 1: 2.2817\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 315\n",
            "  Loss for client 2: 1.5993\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 316\n",
            "  Client 0: total grad norm = 71.503413 (253 params)\n",
            "  Client 1: total grad norm = 88.959852 (253 params)\n",
            "  Client 2: total grad norm = 76.276157 (253 params)\n",
            "\n",
            "=== Optimizer Step 316 ===\n",
            "\n",
            "=== Updating Client Weights (Step 316) ===\n",
            "Gradient norms: [8.086654663085938, 10.593098640441895, 7.6288065910339355]\n",
            "Target weights: [0.35418230295181274, 0.27037888765335083, 0.3754388093948364]\n",
            "Updated weights: [0.3305288553237915, 0.3285469114780426, 0.3409242033958435]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 316\n",
            "  Loss for client 0: 1.1559\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 316\n",
            "  Loss for client 1: 1.7334\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 316\n",
            "  Loss for client 2: 1.8445\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 317\n",
            "  Client 0: total grad norm = 65.229729 (253 params)\n",
            "  Client 1: total grad norm = 80.984518 (253 params)\n",
            "  Client 2: total grad norm = 78.483674 (253 params)\n",
            "\n",
            "=== Optimizer Step 317 ===\n",
            "\n",
            "=== Updating Client Weights (Step 317) ===\n",
            "Gradient norms: [6.869431495666504, 9.025036811828613, 7.938028812408447]\n",
            "Target weights: [0.38072967529296875, 0.2897934317588806, 0.32947680354118347]\n",
            "Updated weights: [0.3455891013145447, 0.3169208765029907, 0.3374899923801422]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 317\n",
            "  Loss for client 0: 0.8360\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 317\n",
            "  Loss for client 1: 1.8438\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 317\n",
            "  Loss for client 2: 1.6556\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 318\n",
            "  Client 0: total grad norm = 55.488259 (253 params)\n",
            "  Client 1: total grad norm = 104.656933 (253 params)\n",
            "  Client 2: total grad norm = 81.354304 (253 params)\n",
            "\n",
            "=== Optimizer Step 318 ===\n",
            "\n",
            "=== Updating Client Weights (Step 318) ===\n",
            "Gradient norms: [6.229516983032227, 10.617910385131836, 8.61997127532959]\n",
            "Target weights: [0.4330160915851593, 0.254050076007843, 0.31293386220932007]\n",
            "Updated weights: [0.3718172013759613, 0.2980596423149109, 0.3301231563091278]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 318\n",
            "  Loss for client 0: 1.7456\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 318\n",
            "  Loss for client 1: 1.4458\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 318\n",
            "  Loss for client 2: 2.0518\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 319\n",
            "  Client 0: total grad norm = 102.397286 (253 params)\n",
            "  Client 1: total grad norm = 78.176291 (253 params)\n",
            "  Client 2: total grad norm = 90.195158 (253 params)\n",
            "\n",
            "=== Optimizer Step 319 ===\n",
            "\n",
            "=== Updating Client Weights (Step 319) ===\n",
            "Gradient norms: [12.138484001159668, 7.006886959075928, 8.540904998779297]\n",
            "Target weights: [0.24075603485107422, 0.41707727313041687, 0.3421666920185089]\n",
            "Updated weights: [0.33249884843826294, 0.3337649405002594, 0.33373621106147766]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 319\n",
            "  Loss for client 0: 1.4972\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 319\n",
            "  Loss for client 1: 2.0138\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 319\n",
            "  Loss for client 2: 1.7862\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 320\n",
            "  Client 0: total grad norm = 69.575440 (253 params)\n",
            "  Client 1: total grad norm = 79.346837 (253 params)\n",
            "  Client 2: total grad norm = 75.028965 (253 params)\n",
            "\n",
            "=== Optimizer Step 320 ===\n",
            "\n",
            "=== Updating Client Weights (Step 320) ===\n",
            "Gradient norms: [7.480072021484375, 8.421821594238281, 8.393082618713379]\n",
            "Target weights: [0.3597903847694397, 0.31955769658088684, 0.32065191864967346]\n",
            "Updated weights: [0.3406863212585449, 0.32950276136398315, 0.3298109173774719]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 320\n",
            "  Loss for client 0: 1.7503\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 320\n",
            "  Loss for client 1: 2.2627\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 320\n",
            "  Loss for client 2: 1.4747\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 321\n",
            "  Client 0: total grad norm = 83.376915 (253 params)\n",
            "  Client 1: total grad norm = 80.769750 (253 params)\n",
            "  Client 2: total grad norm = 66.805851 (253 params)\n",
            "\n",
            "=== Optimizer Step 321 ===\n",
            "\n",
            "=== Updating Client Weights (Step 321) ===\n",
            "Gradient norms: [9.7425537109375, 8.156471252441406, 8.076109886169434]\n",
            "Target weights: [0.2940485179424286, 0.3512282967567444, 0.354723185300827]\n",
            "Updated weights: [0.3266949951648712, 0.33602043986320496, 0.3372846245765686]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 321\n",
            "  Loss for client 0: 2.2820\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 321\n",
            "  Loss for client 1: 1.2317\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 321\n",
            "  Loss for client 2: 2.3233\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 322\n",
            "  Client 0: total grad norm = 121.189042 (253 params)\n",
            "  Client 1: total grad norm = 92.105043 (253 params)\n",
            "  Client 2: total grad norm = 95.695858 (253 params)\n",
            "\n",
            "=== Optimizer Step 322 ===\n",
            "\n",
            "=== Updating Client Weights (Step 322) ===\n",
            "Gradient norms: [10.7496976852417, 8.489554405212402, 9.566751480102539]\n",
            "Target weights: [0.29499587416648865, 0.3735315203666687, 0.33147263526916504]\n",
            "Updated weights: [0.31718525290489197, 0.3472737669944763, 0.3355410099029541]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 322\n",
            "  Loss for client 0: 1.5531\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 322\n",
            "  Loss for client 1: 0.9396\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 322\n",
            "  Loss for client 2: 1.5157\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 323\n",
            "  Client 0: total grad norm = 114.394914 (253 params)\n",
            "  Client 1: total grad norm = 59.109407 (253 params)\n",
            "  Client 2: total grad norm = 81.727586 (253 params)\n",
            "\n",
            "=== Optimizer Step 323 ===\n",
            "\n",
            "=== Updating Client Weights (Step 323) ===\n",
            "Gradient norms: [12.894447326660156, 5.743024826049805, 8.071685791015625]\n",
            "Target weights: [0.20649519562721252, 0.4636304974555969, 0.32987427711486816]\n",
            "Updated weights: [0.2839782238006592, 0.382180780172348, 0.3338409960269928]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 323\n",
            "  Loss for client 0: 2.3446\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 323\n",
            "  Loss for client 1: 2.0999\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 323\n",
            "  Loss for client 2: 2.1767\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 324\n",
            "  Client 0: total grad norm = 92.254649 (253 params)\n",
            "  Client 1: total grad norm = 81.612224 (253 params)\n",
            "  Client 2: total grad norm = 87.599116 (253 params)\n",
            "\n",
            "=== Optimizer Step 324 ===\n",
            "\n",
            "=== Updating Client Weights (Step 324) ===\n",
            "Gradient norms: [10.148839950561523, 8.806784629821777, 8.40364933013916]\n",
            "Target weights: [0.29761379957199097, 0.34296679496765137, 0.35941940546035767]\n",
            "Updated weights: [0.28806889057159424, 0.3704165816307068, 0.341514527797699]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 324\n",
            "  Loss for client 0: 1.7036\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 324\n",
            "  Loss for client 1: 1.0309\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 324\n",
            "  Loss for client 2: 1.6876\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 325\n",
            "  Client 0: total grad norm = 79.313762 (253 params)\n",
            "  Client 1: total grad norm = 71.072149 (253 params)\n",
            "  Client 2: total grad norm = 76.859128 (253 params)\n",
            "\n",
            "=== Optimizer Step 325 ===\n",
            "\n",
            "=== Updating Client Weights (Step 325) ===\n",
            "Gradient norms: [8.2746000289917, 8.044062614440918, 7.719346523284912]\n",
            "Target weights: [0.32251933217048645, 0.33176252245903015, 0.3457181751728058]\n",
            "Updated weights: [0.2984040379524231, 0.3588203489780426, 0.3427756130695343]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 325\n",
            "  Loss for client 0: 1.6391\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 325\n",
            "  Loss for client 1: 1.8623\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 325\n",
            "  Loss for client 2: 1.6525\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 326\n",
            "  Client 0: total grad norm = 95.375529 (253 params)\n",
            "  Client 1: total grad norm = 86.736505 (253 params)\n",
            "  Client 2: total grad norm = 69.095955 (253 params)\n",
            "\n",
            "=== Optimizer Step 326 ===\n",
            "\n",
            "=== Updating Client Weights (Step 326) ===\n",
            "Gradient norms: [10.692534446716309, 9.629097938537598, 7.714745044708252]\n",
            "Target weights: [0.28600630164146423, 0.3175927996635437, 0.39640092849731445]\n",
            "Updated weights: [0.2946847081184387, 0.34645208716392517, 0.3588632047176361]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 326\n",
            "  Loss for client 0: 1.9112\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 326\n",
            "  Loss for client 1: 1.9331\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 326\n",
            "  Loss for client 2: 1.9006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 327\n",
            "  Client 0: total grad norm = 97.277748 (253 params)\n",
            "  Client 1: total grad norm = 153.178051 (253 params)\n",
            "  Client 2: total grad norm = 90.397707 (253 params)\n",
            "\n",
            "=== Optimizer Step 327 ===\n",
            "\n",
            "=== Updating Client Weights (Step 327) ===\n",
            "Gradient norms: [12.368428230285645, 15.00946044921875, 9.497550010681152]\n",
            "Target weights: [0.3198651075363159, 0.26358234882354736, 0.41655251383781433]\n",
            "Updated weights: [0.3022388219833374, 0.32159116864204407, 0.37617000937461853]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 327\n",
            "  Loss for client 0: 1.1389\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 327\n",
            "  Loss for client 1: 1.6887\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 327\n",
            "  Loss for client 2: 2.1433\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 328\n",
            "  Client 0: total grad norm = 67.938145 (253 params)\n",
            "  Client 1: total grad norm = 78.219622 (253 params)\n",
            "  Client 2: total grad norm = 85.595096 (253 params)\n",
            "\n",
            "=== Optimizer Step 328 ===\n",
            "\n",
            "=== Updating Client Weights (Step 328) ===\n",
            "Gradient norms: [7.059077262878418, 7.9149603843688965, 8.837925910949707]\n",
            "Target weights: [0.3716656267642975, 0.3314756453037262, 0.2968588173389435]\n",
            "Updated weights: [0.3230668604373932, 0.32455649971961975, 0.35237666964530945]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 328\n",
            "  Loss for client 0: 1.3537\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 328\n",
            "  Loss for client 1: 1.8100\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 328\n",
            "  Loss for client 2: 2.4608\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 329\n",
            "  Client 0: total grad norm = 80.683867 (253 params)\n",
            "  Client 1: total grad norm = 75.599424 (253 params)\n",
            "  Client 2: total grad norm = 76.790470 (253 params)\n",
            "\n",
            "=== Optimizer Step 329 ===\n",
            "\n",
            "=== Updating Client Weights (Step 329) ===\n",
            "Gradient norms: [9.141796112060547, 8.455267906188965, 8.463519096374512]\n",
            "Target weights: [0.31632187962532043, 0.34200575947761536, 0.3416723310947418]\n",
            "Updated weights: [0.32104337215423584, 0.32979127764701843, 0.3491653800010681]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 329\n",
            "  Loss for client 0: 1.8460\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 329\n",
            "  Loss for client 1: 1.8151\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 329\n",
            "  Loss for client 2: 1.8188\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 330\n",
            "  Client 0: total grad norm = 140.335071 (253 params)\n",
            "  Client 1: total grad norm = 76.282963 (253 params)\n",
            "  Client 2: total grad norm = 71.198051 (253 params)\n",
            "\n",
            "=== Optimizer Step 330 ===\n",
            "\n",
            "=== Updating Client Weights (Step 330) ===\n",
            "Gradient norms: [15.876790046691895, 8.272571563720703, 8.257426261901855]\n",
            "Target weights: [0.206528902053833, 0.3963720202445984, 0.39709901809692383]\n",
            "Updated weights: [0.28668904304504395, 0.34976547956466675, 0.3635454773902893]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 330\n",
            "  Loss for client 0: 1.8105\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 330\n",
            "  Loss for client 1: 2.2264\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 330\n",
            "  Loss for client 2: 1.5580\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 331\n",
            "  Client 0: total grad norm = 67.533029 (253 params)\n",
            "  Client 1: total grad norm = 78.248651 (253 params)\n",
            "  Client 2: total grad norm = 73.027506 (253 params)\n",
            "\n",
            "=== Optimizer Step 331 ===\n",
            "\n",
            "=== Updating Client Weights (Step 331) ===\n",
            "Gradient norms: [8.186190605163574, 9.524077415466309, 7.605813026428223]\n",
            "Target weights: [0.3406188488006592, 0.29277071356773376, 0.36661049723625183]\n",
            "Updated weights: [0.30286797881126404, 0.3326670527458191, 0.36446496844291687]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 331\n",
            "  Loss for client 0: 1.3651\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 331\n",
            "  Loss for client 1: 1.4126\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 331\n",
            "  Loss for client 2: 2.2270\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 332\n",
            "  Client 0: total grad norm = 107.293864 (253 params)\n",
            "  Client 1: total grad norm = 66.885543 (253 params)\n",
            "  Client 2: total grad norm = 77.643853 (253 params)\n",
            "\n",
            "=== Optimizer Step 332 ===\n",
            "\n",
            "=== Updating Client Weights (Step 332) ===\n",
            "Gradient norms: [9.889026641845703, 7.412272930145264, 8.887555122375488]\n",
            "Target weights: [0.2901220917701721, 0.3870641887187958, 0.3228137791156769]\n",
            "Updated weights: [0.2990442216396332, 0.3489861786365509, 0.3519695997238159]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 332\n",
            "  Loss for client 0: 1.3209\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 332\n",
            "  Loss for client 1: 2.2662\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 332\n",
            "  Loss for client 2: 1.4800\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 333\n",
            "  Client 0: total grad norm = 99.150052 (253 params)\n",
            "  Client 1: total grad norm = 78.693197 (253 params)\n",
            "  Client 2: total grad norm = 58.795925 (253 params)\n",
            "\n",
            "=== Optimizer Step 333 ===\n",
            "\n",
            "=== Updating Client Weights (Step 333) ===\n",
            "Gradient norms: [9.905008316040039, 8.770861625671387, 6.864079475402832]\n",
            "Target weights: [0.27992942929267883, 0.31612667441368103, 0.4039439260959625]\n",
            "Updated weights: [0.2933097779750824, 0.339128315448761, 0.3675619065761566]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 333\n",
            "  Loss for client 0: 1.8275\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 333\n",
            "  Loss for client 1: 1.6083\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 333\n",
            "  Loss for client 2: 2.1577\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 334\n",
            "  Client 0: total grad norm = 69.112016 (253 params)\n",
            "  Client 1: total grad norm = 67.673761 (253 params)\n",
            "  Client 2: total grad norm = 73.913852 (253 params)\n",
            "\n",
            "=== Optimizer Step 334 ===\n",
            "\n",
            "=== Updating Client Weights (Step 334) ===\n",
            "Gradient norms: [8.020580291748047, 7.552332878112793, 7.684486389160156]\n",
            "Target weights: [0.32198473811149597, 0.3419479429721832, 0.3360672891139984]\n",
            "Updated weights: [0.3019122779369354, 0.33997419476509094, 0.35811352729797363]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 334\n",
            "  Loss for client 0: 1.3684\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 334\n",
            "  Loss for client 1: 2.1730\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 334\n",
            "  Loss for client 2: 1.8168\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 335\n",
            "  Client 0: total grad norm = 65.777522 (253 params)\n",
            "  Client 1: total grad norm = 84.828244 (253 params)\n",
            "  Client 2: total grad norm = 86.433198 (253 params)\n",
            "\n",
            "=== Optimizer Step 335 ===\n",
            "\n",
            "=== Updating Client Weights (Step 335) ===\n",
            "Gradient norms: [6.837957859039307, 9.540265083312988, 8.823060035705566]\n",
            "Target weights: [0.40132325887680054, 0.2876473069190979, 0.31102946400642395]\n",
            "Updated weights: [0.3317355811595917, 0.3242761194705963, 0.343988299369812]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 335\n",
            "  Loss for client 0: 1.8355\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 335\n",
            "  Loss for client 1: 1.7445\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 335\n",
            "  Loss for client 2: 1.7290\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 336\n",
            "  Client 0: total grad norm = 77.242056 (253 params)\n",
            "  Client 1: total grad norm = 77.600867 (253 params)\n",
            "  Client 2: total grad norm = 66.576438 (253 params)\n",
            "\n",
            "=== Optimizer Step 336 ===\n",
            "\n",
            "=== Updating Client Weights (Step 336) ===\n",
            "Gradient norms: [8.411235809326172, 9.468961715698242, 7.723746299743652]\n",
            "Target weights: [0.33587419986724854, 0.2983555197715759, 0.3657703101634979]\n",
            "Updated weights: [0.33297717571258545, 0.3164999485015869, 0.35052290558815]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 336\n",
            "  Loss for client 0: 1.9232\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 336\n",
            "  Loss for client 1: 1.8066\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 336\n",
            "  Loss for client 2: 1.6940\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 337\n",
            "  Client 0: total grad norm = 83.924705 (253 params)\n",
            "  Client 1: total grad norm = 87.927168 (253 params)\n",
            "  Client 2: total grad norm = 64.486324 (253 params)\n",
            "\n",
            "=== Optimizer Step 337 ===\n",
            "\n",
            "=== Updating Client Weights (Step 337) ===\n",
            "Gradient norms: [9.543069839477539, 7.364718914031982, 6.944082260131836]\n",
            "Target weights: [0.2724754214286804, 0.3530687391757965, 0.3744558095932007]\n",
            "Updated weights: [0.3148266673088074, 0.327470600605011, 0.3577027916908264]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 337\n",
            "  Loss for client 0: 2.0142\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 337\n",
            "  Loss for client 1: 2.1852\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 337\n",
            "  Loss for client 2: 1.9673\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 338\n",
            "  Client 0: total grad norm = 85.013423 (253 params)\n",
            "  Client 1: total grad norm = 89.931403 (253 params)\n",
            "  Client 2: total grad norm = 67.867645 (253 params)\n",
            "\n",
            "=== Optimizer Step 338 ===\n",
            "\n",
            "=== Updating Client Weights (Step 338) ===\n",
            "Gradient norms: [9.350004196166992, 9.104954719543457, 8.236087799072266]\n",
            "Target weights: [0.31623929738998413, 0.32475054264068604, 0.35901010036468506]\n",
            "Updated weights: [0.3152504563331604, 0.3266545832157135, 0.3580949902534485]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 338\n",
            "  Loss for client 0: 1.5989\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 338\n",
            "  Loss for client 1: 1.8767\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 338\n",
            "  Loss for client 2: 1.6144\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 339\n",
            "  Client 0: total grad norm = 77.299817 (253 params)\n",
            "  Client 1: total grad norm = 68.396747 (253 params)\n",
            "  Client 2: total grad norm = 73.153079 (253 params)\n",
            "\n",
            "=== Optimizer Step 339 ===\n",
            "\n",
            "=== Updating Client Weights (Step 339) ===\n",
            "Gradient norms: [8.977160453796387, 7.34451150894165, 8.703192710876465]\n",
            "Target weights: [0.30733543634414673, 0.37565457820892334, 0.3170100450515747]\n",
            "Updated weights: [0.3128759264945984, 0.3413545489311218, 0.3457694947719574]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 339\n",
            "  Loss for client 0: 1.4011\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 339\n",
            "  Loss for client 1: 1.8398\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 339\n",
            "  Loss for client 2: 1.9882\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 340\n",
            "  Client 0: total grad norm = 69.669190 (253 params)\n",
            "  Client 1: total grad norm = 78.738281 (253 params)\n",
            "  Client 2: total grad norm = 75.787090 (253 params)\n",
            "\n",
            "=== Optimizer Step 340 ===\n",
            "\n",
            "=== Updating Client Weights (Step 340) ===\n",
            "Gradient norms: [7.816434860229492, 9.787100791931152, 8.521265029907227]\n",
            "Target weights: [0.3681976795196533, 0.2940598428249359, 0.33774250745773315]\n",
            "Updated weights: [0.32947248220443726, 0.3271661698818207, 0.34336140751838684]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 340\n",
            "  Loss for client 0: 1.9265\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 340\n",
            "  Loss for client 1: 1.9940\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 340\n",
            "  Loss for client 2: 1.4687\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 341\n",
            "  Client 0: total grad norm = 87.949233 (253 params)\n",
            "  Client 1: total grad norm = 86.539633 (253 params)\n",
            "  Client 2: total grad norm = 83.775144 (253 params)\n",
            "\n",
            "=== Optimizer Step 341 ===\n",
            "\n",
            "=== Updating Client Weights (Step 341) ===\n",
            "Gradient norms: [8.882925987243652, 10.738531112670898, 8.60008430480957]\n",
            "Target weights: [0.3496393859386444, 0.2892221510410309, 0.3611384332180023]\n",
            "Updated weights: [0.3355225622653961, 0.31578296422958374, 0.3486945331096649]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 341\n",
            "  Loss for client 0: 1.4664\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 341\n",
            "  Loss for client 1: 1.7631\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 341\n",
            "  Loss for client 2: 2.1434\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 342\n",
            "  Client 0: total grad norm = 85.015556 (253 params)\n",
            "  Client 1: total grad norm = 93.116712 (253 params)\n",
            "  Client 2: total grad norm = 81.278702 (253 params)\n",
            "\n",
            "=== Optimizer Step 342 ===\n",
            "\n",
            "=== Updating Client Weights (Step 342) ===\n",
            "Gradient norms: [9.548484802246094, 10.751194953918457, 8.545354843139648]\n",
            "Target weights: [0.3327209949493408, 0.2955003082752228, 0.3717787563800812]\n",
            "Updated weights: [0.33468207716941833, 0.3096981644630432, 0.3556198179721832]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 342\n",
            "  Loss for client 0: 1.7315\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 342\n",
            "  Loss for client 1: 1.9693\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 342\n",
            "  Loss for client 2: 2.0167\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 343\n",
            "  Client 0: total grad norm = 114.358299 (253 params)\n",
            "  Client 1: total grad norm = 90.678053 (253 params)\n",
            "  Client 2: total grad norm = 81.765858 (253 params)\n",
            "\n",
            "=== Optimizer Step 343 ===\n",
            "\n",
            "=== Updating Client Weights (Step 343) ===\n",
            "Gradient norms: [10.533113479614258, 9.078372955322266, 9.675806999206543]\n",
            "Target weights: [0.30780163407325745, 0.35712453722953796, 0.3350738286972046]\n",
            "Updated weights: [0.326617956161499, 0.32392609119415283, 0.3494560122489929]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 343\n",
            "  Loss for client 0: 1.8060\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 343\n",
            "  Loss for client 1: 1.5820\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 343\n",
            "  Loss for client 2: 2.1921\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 344\n",
            "  Client 0: total grad norm = 84.898926 (253 params)\n",
            "  Client 1: total grad norm = 74.512480 (253 params)\n",
            "  Client 2: total grad norm = 78.426994 (253 params)\n",
            "\n",
            "=== Optimizer Step 344 ===\n",
            "\n",
            "=== Updating Client Weights (Step 344) ===\n",
            "Gradient norms: [8.843152046203613, 7.852284908294678, 7.572849750518799]\n",
            "Target weights: [0.30358847975730896, 0.3418978154659271, 0.3545137047767639]\n",
            "Updated weights: [0.3197091221809387, 0.3293175995349884, 0.35097330808639526]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 344\n",
            "  Loss for client 0: 1.6736\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 344\n",
            "  Loss for client 1: 2.1706\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 344\n",
            "  Loss for client 2: 1.7502\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 345\n",
            "  Client 0: total grad norm = 84.954111 (253 params)\n",
            "  Client 1: total grad norm = 95.640713 (253 params)\n",
            "  Client 2: total grad norm = 73.832909 (253 params)\n",
            "\n",
            "=== Optimizer Step 345 ===\n",
            "\n",
            "=== Updating Client Weights (Step 345) ===\n",
            "Gradient norms: [8.672650337219238, 10.514261245727539, 7.926860332489014]\n",
            "Target weights: [0.342591255903244, 0.28258514404296875, 0.37482357025146484]\n",
            "Updated weights: [0.32657375931739807, 0.3152978718280792, 0.3581283986568451]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 345\n",
            "  Loss for client 0: 1.3137\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 345\n",
            "  Loss for client 1: 2.1247\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 345\n",
            "  Loss for client 2: 1.2635\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 346\n",
            "  Client 0: total grad norm = 104.627236 (253 params)\n",
            "  Client 1: total grad norm = 75.565534 (253 params)\n",
            "  Client 2: total grad norm = 65.495358 (253 params)\n",
            "\n",
            "=== Optimizer Step 346 ===\n",
            "\n",
            "=== Updating Client Weights (Step 346) ===\n",
            "Gradient norms: [12.838626861572266, 8.181324005126953, 7.634838104248047]\n",
            "Target weights: [0.23524735867977142, 0.3691643476486206, 0.39558833837509155]\n",
            "Updated weights: [0.29917582869529724, 0.33145779371261597, 0.3693663477897644]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 346\n",
            "  Loss for client 0: 1.2370\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 346\n",
            "  Loss for client 1: 1.7764\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 346\n",
            "  Loss for client 2: 1.9761\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 347\n",
            "  Client 0: total grad norm = 70.428610 (253 params)\n",
            "  Client 1: total grad norm = 74.144534 (253 params)\n",
            "  Client 2: total grad norm = 76.221316 (253 params)\n",
            "\n",
            "=== Optimizer Step 347 ===\n",
            "\n",
            "=== Updating Client Weights (Step 347) ===\n",
            "Gradient norms: [6.995139122009277, 8.150245666503906, 9.71518325805664]\n",
            "Target weights: [0.3878532648086548, 0.33288413286209106, 0.27926260232925415]\n",
            "Updated weights: [0.3257790803909302, 0.3318856954574585, 0.34233522415161133]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 347\n",
            "  Loss for client 0: 1.0748\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 347\n",
            "  Loss for client 1: 1.8274\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 347\n",
            "  Loss for client 2: 1.9017\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 348\n",
            "  Client 0: total grad norm = 83.674389 (253 params)\n",
            "  Client 1: total grad norm = 106.611191 (253 params)\n",
            "  Client 2: total grad norm = 80.335602 (253 params)\n",
            "\n",
            "=== Optimizer Step 348 ===\n",
            "\n",
            "=== Updating Client Weights (Step 348) ===\n",
            "Gradient norms: [10.745619773864746, 10.911757469177246, 9.035257339477539]\n",
            "Target weights: [0.3150525987148285, 0.3102557361125946, 0.3746916353702545]\n",
            "Updated weights: [0.3225611448287964, 0.32539671659469604, 0.35204213857650757]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 348\n",
            "  Loss for client 0: 1.2576\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 348\n",
            "  Loss for client 1: 2.8393\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 348\n",
            "  Loss for client 2: 1.7631\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 349\n",
            "  Client 0: total grad norm = 77.149917 (253 params)\n",
            "  Client 1: total grad norm = 87.710489 (253 params)\n",
            "  Client 2: total grad norm = 93.434511 (253 params)\n",
            "\n",
            "=== Optimizer Step 349 ===\n",
            "\n",
            "=== Updating Client Weights (Step 349) ===\n",
            "Gradient norms: [8.960881233215332, 9.277952194213867, 8.306827545166016]\n",
            "Target weights: [0.3284544348716736, 0.31722962856292725, 0.3543159067630768]\n",
            "Updated weights: [0.324329137802124, 0.32294660806655884, 0.35272425413131714]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 349\n",
            "  Loss for client 0: 0.9732\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 349\n",
            "  Loss for client 1: 1.8383\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 349\n",
            "  Loss for client 2: 1.5722\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 350\n",
            "  Client 0: total grad norm = 55.362927 (253 params)\n",
            "  Client 1: total grad norm = 75.922081 (253 params)\n",
            "  Client 2: total grad norm = 71.144027 (253 params)\n",
            "\n",
            "=== Optimizer Step 350 ===\n",
            "\n",
            "=== Updating Client Weights (Step 350) ===\n",
            "Gradient norms: [6.426115036010742, 8.32325553894043, 7.280857563018799]\n",
            "Target weights: [0.3766944110393524, 0.2908335328102112, 0.3324720561504364]\n",
            "Updated weights: [0.3400387167930603, 0.31331267952919006, 0.34664860367774963]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 350\n",
            "  Val Loss = 1.9801 (5 batches)\n",
            "\n",
            "[Processing] Client 0, batch 350\n",
            "  Loss for client 0: 1.6269\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 350\n",
            "  Loss for client 1: 1.6870\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 350\n",
            "  Loss for client 2: 1.7792\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 351\n",
            "  Client 0: total grad norm = 89.238545 (253 params)\n",
            "  Client 1: total grad norm = 83.822921 (253 params)\n",
            "  Client 2: total grad norm = 75.342850 (253 params)\n",
            "\n",
            "=== Optimizer Step 351 ===\n",
            "\n",
            "=== Updating Client Weights (Step 351) ===\n",
            "Gradient norms: [9.673603057861328, 8.313812255859375, 8.115938186645508]\n",
            "Target weights: [0.29801952838897705, 0.346763014793396, 0.35521742701530457]\n",
            "Updated weights: [0.3274329602718353, 0.3233477771282196, 0.34921926259994507]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 351\n",
            "  Loss for client 0: 1.3825\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 351\n",
            "  Loss for client 1: 1.2170\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 351\n",
            "  Loss for client 2: 1.4227\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 352\n",
            "  Client 0: total grad norm = 69.486822 (253 params)\n",
            "  Client 1: total grad norm = 82.003898 (253 params)\n",
            "  Client 2: total grad norm = 73.075548 (253 params)\n",
            "\n",
            "=== Optimizer Step 352 ===\n",
            "\n",
            "=== Updating Client Weights (Step 352) ===\n",
            "Gradient norms: [7.735918998718262, 8.502363204956055, 9.066287994384766]\n",
            "Target weights: [0.3619101047515869, 0.32928577065467834, 0.30880412459373474]\n",
            "Updated weights: [0.3377761244773865, 0.3251291811466217, 0.3370947241783142]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 352\n",
            "  Loss for client 0: 1.0855\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 352\n",
            "  Loss for client 1: 1.6901\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 352\n",
            "  Loss for client 2: 1.6495\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 353\n",
            "  Client 0: total grad norm = 69.005087 (253 params)\n",
            "  Client 1: total grad norm = 69.924446 (253 params)\n",
            "  Client 2: total grad norm = 88.263980 (253 params)\n",
            "\n",
            "=== Optimizer Step 353 ===\n",
            "\n",
            "=== Updating Client Weights (Step 353) ===\n",
            "Gradient norms: [8.473845481872559, 7.879029750823975, 9.680623054504395]\n",
            "Target weights: [0.33888721466064453, 0.3644709885120392, 0.29664182662963867]\n",
            "Updated weights: [0.33810943365097046, 0.3369317352771759, 0.324958860874176]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 353\n",
            "  Loss for client 0: 1.7334\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 353\n",
            "  Loss for client 1: 2.2039\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 353\n",
            "  Loss for client 2: 1.6789\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 354\n",
            "  Client 0: total grad norm = 86.912937 (253 params)\n",
            "  Client 1: total grad norm = 93.949309 (253 params)\n",
            "  Client 2: total grad norm = 74.382948 (253 params)\n",
            "\n",
            "=== Optimizer Step 354 ===\n",
            "\n",
            "=== Updating Client Weights (Step 354) ===\n",
            "Gradient norms: [9.789862632751465, 10.34980297088623, 9.644753456115723]\n",
            "Target weights: [0.3377301096916199, 0.31945839524269104, 0.3428114354610443]\n",
            "Updated weights: [0.33799564838409424, 0.3316897451877594, 0.33031463623046875]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 354\n",
            "  Loss for client 0: 1.6793\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 354\n",
            "  Loss for client 1: 1.9990\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 354\n",
            "  Loss for client 2: 1.7089\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 355\n",
            "  Client 0: total grad norm = 81.818251 (253 params)\n",
            "  Client 1: total grad norm = 91.762161 (253 params)\n",
            "  Client 2: total grad norm = 84.760539 (253 params)\n",
            "\n",
            "=== Optimizer Step 355 ===\n",
            "\n",
            "=== Updating Client Weights (Step 355) ===\n",
            "Gradient norms: [10.266347885131836, 10.388480186462402, 8.892094612121582]\n",
            "Target weights: [0.31818848848342896, 0.3144477307796478, 0.367363840341568]\n",
            "Updated weights: [0.3320535123348236, 0.32651713490486145, 0.3414294123649597]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 355\n",
            "  Loss for client 0: 1.8444\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 355\n",
            "  Loss for client 1: 2.0376\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 355\n",
            "  Loss for client 2: 1.8271\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 356\n",
            "  Client 0: total grad norm = 85.971834 (253 params)\n",
            "  Client 1: total grad norm = 85.410586 (253 params)\n",
            "  Client 2: total grad norm = 78.896502 (253 params)\n",
            "\n",
            "=== Optimizer Step 356 ===\n",
            "\n",
            "=== Updating Client Weights (Step 356) ===\n",
            "Gradient norms: [8.835904121398926, 8.699931144714355, 9.029609680175781]\n",
            "Target weights: [0.33398160338401794, 0.3392014503479004, 0.32681694626808167]\n",
            "Updated weights: [0.3326319456100464, 0.3303224444389343, 0.33704566955566406]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 356\n",
            "  Loss for client 0: 1.3360\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 356\n",
            "  Loss for client 1: 2.3387\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 356\n",
            "  Loss for client 2: 1.6264\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 357\n",
            "  Client 0: total grad norm = 83.489568 (253 params)\n",
            "  Client 1: total grad norm = 118.526255 (253 params)\n",
            "  Client 2: total grad norm = 78.459074 (253 params)\n",
            "\n",
            "=== Optimizer Step 357 ===\n",
            "\n",
            "=== Updating Client Weights (Step 357) ===\n",
            "Gradient norms: [9.11882495880127, 11.11378288269043, 7.87761116027832]\n",
            "Target weights: [0.33578917384147644, 0.27551400661468506, 0.3886968493461609]\n",
            "Updated weights: [0.3335791230201721, 0.31387990713119507, 0.3525410294532776]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 357\n",
            "  Loss for client 0: 1.6710\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 357\n",
            "  Loss for client 1: 2.0329\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 357\n",
            "  Loss for client 2: 1.9988\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 358\n",
            "  Client 0: total grad norm = 84.942843 (253 params)\n",
            "  Client 1: total grad norm = 76.541775 (253 params)\n",
            "  Client 2: total grad norm = 92.337466 (253 params)\n",
            "\n",
            "=== Optimizer Step 358 ===\n",
            "\n",
            "=== Updating Client Weights (Step 358) ===\n",
            "Gradient norms: [10.655624389648438, 8.880242347717285, 9.999025344848633]\n",
            "Target weights: [0.3062232434749603, 0.3674449622631073, 0.32633182406425476]\n",
            "Updated weights: [0.3253723680973053, 0.32994943857192993, 0.34467825293540955]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 358\n",
            "  Loss for client 0: 1.8951\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 358\n",
            "  Loss for client 1: 1.8931\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 358\n",
            "  Loss for client 2: 2.5828\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 359\n",
            "  Client 0: total grad norm = 83.280316 (253 params)\n",
            "  Client 1: total grad norm = 82.784340 (253 params)\n",
            "  Client 2: total grad norm = 122.481907 (253 params)\n",
            "\n",
            "=== Optimizer Step 359 ===\n",
            "\n",
            "=== Updating Client Weights (Step 359) ===\n",
            "Gradient norms: [9.027702331542969, 8.251628875732422, 13.517999649047852]\n",
            "Target weights: [0.36207231879234314, 0.39612555503845215, 0.2418021261692047]\n",
            "Updated weights: [0.33638235926628113, 0.34980225563049316, 0.3138154149055481]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 359\n",
            "  Loss for client 0: 0.9782\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 359\n",
            "  Loss for client 1: 2.2270\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 359\n",
            "  Loss for client 2: 2.0456\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 360\n",
            "  Client 0: total grad norm = 69.084506 (253 params)\n",
            "  Client 1: total grad norm = 84.156655 (253 params)\n",
            "  Client 2: total grad norm = 81.030066 (253 params)\n",
            "\n",
            "=== Optimizer Step 360 ===\n",
            "\n",
            "=== Updating Client Weights (Step 360) ===\n",
            "Gradient norms: [7.588026523590088, 10.412614822387695, 9.71822452545166]\n",
            "Target weights: [0.39847975969314575, 0.29038575291633606, 0.3111345171928406]\n",
            "Updated weights: [0.35501158237457275, 0.33197730779647827, 0.31301113963127136]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 360\n",
            "  Loss for client 0: 1.3567\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 360\n",
            "  Loss for client 1: 1.6043\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 360\n",
            "  Loss for client 2: 1.9723\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 361\n",
            "  Client 0: total grad norm = 68.539653 (253 params)\n",
            "  Client 1: total grad norm = 109.039861 (253 params)\n",
            "  Client 2: total grad norm = 105.766702 (253 params)\n",
            "\n",
            "=== Optimizer Step 361 ===\n",
            "\n",
            "=== Updating Client Weights (Step 361) ===\n",
            "Gradient norms: [7.460763454437256, 9.389748573303223, 9.921302795410156]\n",
            "Target weights: [0.39268672466278076, 0.31201502680778503, 0.29529818892478943]\n",
            "Updated weights: [0.3663141429424286, 0.32598862051963806, 0.30769723653793335]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 361\n",
            "  Loss for client 0: 1.6333\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 361\n",
            "  Loss for client 1: 1.6520\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 361\n",
            "  Loss for client 2: 1.5329\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 362\n",
            "  Client 0: total grad norm = 72.795801 (253 params)\n",
            "  Client 1: total grad norm = 68.750139 (253 params)\n",
            "  Client 2: total grad norm = 68.484639 (253 params)\n",
            "\n",
            "=== Optimizer Step 362 ===\n",
            "\n",
            "=== Updating Client Weights (Step 362) ===\n",
            "Gradient norms: [8.023250579833984, 7.843173503875732, 7.5336151123046875]\n",
            "Target weights: [0.32383909821510315, 0.3312743604183197, 0.34488651156425476]\n",
            "Updated weights: [0.3535716235637665, 0.32757434248924255, 0.31885403394699097]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 362\n",
            "  Loss for client 0: 1.1136\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 362\n",
            "  Loss for client 1: 2.2823\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 362\n",
            "  Loss for client 2: 1.9667\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 363\n",
            "  Client 0: total grad norm = 67.172588 (253 params)\n",
            "  Client 1: total grad norm = 86.998857 (253 params)\n",
            "  Client 2: total grad norm = 83.761622 (253 params)\n",
            "\n",
            "=== Optimizer Step 363 ===\n",
            "\n",
            "=== Updating Client Weights (Step 363) ===\n",
            "Gradient norms: [6.252971172332764, 9.32318115234375, 9.08891487121582]\n",
            "Target weights: [0.42396801710128784, 0.2843514084815979, 0.29168054461479187]\n",
            "Updated weights: [0.37469056248664856, 0.3146074712276459, 0.31070199608802795]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 363\n",
            "  Loss for client 0: 1.7249\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 363\n",
            "  Loss for client 1: 1.7594\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 363\n",
            "  Loss for client 2: 1.4454\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 364\n",
            "  Client 0: total grad norm = 115.088533 (253 params)\n",
            "  Client 1: total grad norm = 64.888313 (253 params)\n",
            "  Client 2: total grad norm = 73.336011 (253 params)\n",
            "\n",
            "=== Optimizer Step 364 ===\n",
            "\n",
            "=== Updating Client Weights (Step 364) ===\n",
            "Gradient norms: [13.730889320373535, 7.955194473266602, 8.456690788269043]\n",
            "Target weights: [0.22990089654922485, 0.3968154489994049, 0.37328359484672546]\n",
            "Updated weights: [0.33125364780426025, 0.33926987648010254, 0.3294764757156372]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 364\n",
            "  Loss for client 0: 1.0713\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 364\n",
            "  Loss for client 1: 1.4591\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 364\n",
            "  Loss for client 2: 1.0355\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 365\n",
            "  Client 0: total grad norm = 56.191731 (253 params)\n",
            "  Client 1: total grad norm = 71.110129 (253 params)\n",
            "  Client 2: total grad norm = 61.957365 (253 params)\n",
            "\n",
            "=== Optimizer Step 365 ===\n",
            "\n",
            "=== Updating Client Weights (Step 365) ===\n",
            "Gradient norms: [6.1764984130859375, 7.565887928009033, 6.7681121826171875]\n",
            "Target weights: [0.36644139885902405, 0.2991485893726349, 0.33441004157066345]\n",
            "Updated weights: [0.3418099880218506, 0.3272334933280945, 0.3309565484523773]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 365\n",
            "  Loss for client 0: 2.6300\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 365\n",
            "  Loss for client 1: 2.0525\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 365\n",
            "  Loss for client 2: 2.3910\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 366\n",
            "  Client 0: total grad norm = 96.855144 (253 params)\n",
            "  Client 1: total grad norm = 73.902472 (253 params)\n",
            "  Client 2: total grad norm = 96.771354 (253 params)\n",
            "\n",
            "=== Optimizer Step 366 ===\n",
            "\n",
            "=== Updating Client Weights (Step 366) ===\n",
            "Gradient norms: [9.755197525024414, 8.563074111938477, 9.941118240356445]\n",
            "Target weights: [0.3204599618911743, 0.36507338285446167, 0.3144666850566864]\n",
            "Updated weights: [0.33540499210357666, 0.3385854661464691, 0.3260095715522766]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 366\n",
            "  Loss for client 0: 1.4920\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 366\n",
            "  Loss for client 1: 1.0744\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 366\n",
            "  Loss for client 2: 1.3252\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 367\n",
            "  Client 0: total grad norm = 106.117678 (253 params)\n",
            "  Client 1: total grad norm = 65.890107 (253 params)\n",
            "  Client 2: total grad norm = 62.302146 (253 params)\n",
            "\n",
            "=== Optimizer Step 367 ===\n",
            "\n",
            "=== Updating Client Weights (Step 367) ===\n",
            "Gradient norms: [14.27135181427002, 6.533660888671875, 8.000188827514648]\n",
            "Target weights: [0.2012818455696106, 0.439656138420105, 0.3590620160102844]\n",
            "Updated weights: [0.29516804218292236, 0.3689066767692566, 0.33592531085014343]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 367\n",
            "  Loss for client 0: 1.1956\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 367\n",
            "  Loss for client 1: 2.2617\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 367\n",
            "  Loss for client 2: 2.0274\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 368\n",
            "  Client 0: total grad norm = 66.979372 (253 params)\n",
            "  Client 1: total grad norm = 81.272185 (253 params)\n",
            "  Client 2: total grad norm = 74.318701 (253 params)\n",
            "\n",
            "=== Optimizer Step 368 ===\n",
            "\n",
            "=== Updating Client Weights (Step 368) ===\n",
            "Gradient norms: [7.342648983001709, 9.711012840270996, 8.338716506958008]\n",
            "Target weights: [0.37926703691482544, 0.28676971793174744, 0.33396321535110474]\n",
            "Updated weights: [0.3203977346420288, 0.34426558017730713, 0.33533668518066406]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 368\n",
            "  Loss for client 0: 2.1877\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 368\n",
            "  Loss for client 1: 2.2066\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 368\n",
            "  Loss for client 2: 2.1819\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 369\n",
            "  Client 0: total grad norm = 95.765527 (253 params)\n",
            "  Client 1: total grad norm = 82.639517 (253 params)\n",
            "  Client 2: total grad norm = 86.253305 (253 params)\n",
            "\n",
            "=== Optimizer Step 369 ===\n",
            "\n",
            "=== Updating Client Weights (Step 369) ===\n",
            "Gradient norms: [9.805486679077148, 9.872370719909668, 8.547595024108887]\n",
            "Target weights: [0.31843194365501404, 0.31627461314201355, 0.3652934432029724]\n",
            "Updated weights: [0.3198080062866211, 0.33586829900741577, 0.3443237245082855]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 369\n",
            "  Loss for client 0: 1.1905\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 369\n",
            "  Loss for client 1: 1.8382\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 369\n",
            "  Loss for client 2: 1.4781\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 370\n",
            "  Client 0: total grad norm = 58.929259 (253 params)\n",
            "  Client 1: total grad norm = 77.312400 (253 params)\n",
            "  Client 2: total grad norm = 62.954443 (253 params)\n",
            "\n",
            "=== Optimizer Step 370 ===\n",
            "\n",
            "=== Updating Client Weights (Step 370) ===\n",
            "Gradient norms: [6.082335948944092, 10.03428840637207, 6.7200446128845215]\n",
            "Target weights: [0.3982066810131073, 0.24137504398822784, 0.36041826009750366]\n",
            "Updated weights: [0.3433276116847992, 0.30752032995224, 0.3491520881652832]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 370\n",
            "  Loss for client 0: 1.9482\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 370\n",
            "  Loss for client 1: 1.8266\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 370\n",
            "  Loss for client 2: 1.8887\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 371\n",
            "  Client 0: total grad norm = 83.258920 (253 params)\n",
            "  Client 1: total grad norm = 77.757721 (253 params)\n",
            "  Client 2: total grad norm = 76.946898 (253 params)\n",
            "\n",
            "=== Optimizer Step 371 ===\n",
            "\n",
            "=== Updating Client Weights (Step 371) ===\n",
            "Gradient norms: [9.079370498657227, 8.38321304321289, 8.629173278808594]\n",
            "Target weights: [0.3189575672149658, 0.34544438123703003, 0.33559805154800415]\n",
            "Updated weights: [0.33601659536361694, 0.318897545337677, 0.34508588910102844]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 371\n",
            "  Loss for client 0: 0.8722\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 371\n",
            "  Loss for client 1: 1.8884\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 371\n",
            "  Loss for client 2: 1.4925\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 372\n",
            "  Client 0: total grad norm = 63.041016 (253 params)\n",
            "  Client 1: total grad norm = 86.930677 (253 params)\n",
            "  Client 2: total grad norm = 70.926245 (253 params)\n",
            "\n",
            "=== Optimizer Step 372 ===\n",
            "\n",
            "=== Updating Client Weights (Step 372) ===\n",
            "Gradient norms: [7.430683135986328, 9.609579086303711, 6.79092264175415]\n",
            "Target weights: [0.34873995184898376, 0.2696659564971924, 0.38159412145614624]\n",
            "Updated weights: [0.3398336172103882, 0.3041280508041382, 0.356038361787796]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 372\n",
            "  Loss for client 0: 1.9777\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 372\n",
            "  Loss for client 1: 2.0141\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 372\n",
            "  Loss for client 2: 1.8148\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 373\n",
            "  Client 0: total grad norm = 82.171609 (253 params)\n",
            "  Client 1: total grad norm = 80.116279 (253 params)\n",
            "  Client 2: total grad norm = 69.578113 (253 params)\n",
            "\n",
            "=== Optimizer Step 373 ===\n",
            "\n",
            "=== Updating Client Weights (Step 373) ===\n",
            "Gradient norms: [10.037175178527832, 8.279111862182617, 7.611771583557129]\n",
            "Target weights: [0.28320688009262085, 0.34334561228752136, 0.3734474778175354]\n",
            "Updated weights: [0.32284557819366455, 0.3158933222293854, 0.3612610995769501]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 373\n",
            "  Loss for client 0: 1.4456\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 373\n",
            "  Loss for client 1: 1.8264\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 373\n",
            "  Loss for client 2: 1.5414\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 374\n",
            "  Client 0: total grad norm = 67.697585 (253 params)\n",
            "  Client 1: total grad norm = 78.852102 (253 params)\n",
            "  Client 2: total grad norm = 63.017505 (253 params)\n",
            "\n",
            "=== Optimizer Step 374 ===\n",
            "\n",
            "=== Updating Client Weights (Step 374) ===\n",
            "Gradient norms: [8.031542778015137, 9.897745132446289, 7.557356834411621]\n",
            "Target weights: [0.34792327880859375, 0.2823229730129242, 0.36975371837615967]\n",
            "Updated weights: [0.33036887645721436, 0.3058222234249115, 0.36380890011787415]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 374\n",
            "  Loss for client 0: 1.9892\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 374\n",
            "  Loss for client 1: 1.3646\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 374\n",
            "  Loss for client 2: 2.0818\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 375\n",
            "  Client 0: total grad norm = 72.002302 (253 params)\n",
            "  Client 1: total grad norm = 74.070390 (253 params)\n",
            "  Client 2: total grad norm = 603.763796 (253 params)\n",
            "\n",
            "=== Optimizer Step 375 ===\n",
            "\n",
            "=== Updating Client Weights (Step 375) ===\n",
            "Gradient norms: [8.569075584411621, 9.341300010681152, 49.649986267089844]\n",
            "Target weights: [0.4784867763519287, 0.43893134593963623, 0.08258188515901566]\n",
            "Updated weights: [0.3748042583465576, 0.3457549810409546, 0.27944082021713257]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 375\n",
            "  Loss for client 0: 1.9682\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 375\n",
            "  Loss for client 1: 1.7960\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 375\n",
            "  Loss for client 2: 1.6984\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 376\n",
            "  Client 0: total grad norm = 83.523556 (253 params)\n",
            "  Client 1: total grad norm = 81.762945 (253 params)\n",
            "  Client 2: total grad norm = 86.520019 (253 params)\n",
            "\n",
            "=== Optimizer Step 376 ===\n",
            "\n",
            "=== Updating Client Weights (Step 376) ===\n",
            "Gradient norms: [10.546381950378418, 9.397025108337402, 8.315299987792969]\n",
            "Target weights: [0.29493123292922974, 0.33100447058677673, 0.3740643560886383]\n",
            "Updated weights: [0.35084232687950134, 0.34132981300354004, 0.3078278601169586]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 376\n",
            "  Loss for client 0: 1.5314\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 376\n",
            "  Loss for client 1: 1.7616\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 376\n",
            "  Loss for client 2: 1.4867\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 377\n",
            "  Client 0: total grad norm = 62.450069 (253 params)\n",
            "  Client 1: total grad norm = 79.867171 (253 params)\n",
            "  Client 2: total grad norm = 68.574522 (253 params)\n",
            "\n",
            "=== Optimizer Step 377 ===\n",
            "\n",
            "=== Updating Client Weights (Step 377) ===\n",
            "Gradient norms: [7.127828598022461, 9.337729454040527, 8.254833221435547]\n",
            "Target weights: [0.38068991899490356, 0.2905944585800171, 0.32871559262275696]\n",
            "Updated weights: [0.3597966134548187, 0.3261092007160187, 0.3140941858291626]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 377\n",
            "  Loss for client 0: 0.6489\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 377\n",
            "  Loss for client 1: 1.6482\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 377\n",
            "  Loss for client 2: 1.7715\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 378\n",
            "  Client 0: total grad norm = 53.782423 (253 params)\n",
            "  Client 1: total grad norm = 73.364859 (253 params)\n",
            "  Client 2: total grad norm = 78.761253 (253 params)\n",
            "\n",
            "=== Optimizer Step 378 ===\n",
            "\n",
            "=== Updating Client Weights (Step 378) ===\n",
            "Gradient norms: [5.972840785980225, 8.01745319366455, 9.40607738494873]\n",
            "Target weights: [0.42017197608947754, 0.31301966309547424, 0.2668083906173706]\n",
            "Updated weights: [0.37790924310684204, 0.3221823275089264, 0.29990845918655396]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 378\n",
            "  Loss for client 0: 1.0529\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 378\n",
            "  Loss for client 1: 1.4582\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 378\n",
            "  Loss for client 2: 1.8703\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 379\n",
            "  Client 0: total grad norm = 58.188718 (253 params)\n",
            "  Client 1: total grad norm = 74.184792 (253 params)\n",
            "  Client 2: total grad norm = 83.254897 (253 params)\n",
            "\n",
            "=== Optimizer Step 379 ===\n",
            "\n",
            "=== Updating Client Weights (Step 379) ===\n",
            "Gradient norms: [6.890384674072266, 8.916518211364746, 8.720843315124512]\n",
            "Target weights: [0.3901872932910919, 0.3015235662460327, 0.3082890510559082]\n",
            "Updated weights: [0.38159266114234924, 0.31598469614982605, 0.3024226427078247]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 379\n",
            "  Loss for client 0: 1.0979\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 379\n",
            "  Loss for client 1: 1.9765\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 379\n",
            "  Loss for client 2: 1.9504\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 380\n",
            "  Client 0: total grad norm = 75.185786 (253 params)\n",
            "  Client 1: total grad norm = 79.732841 (253 params)\n",
            "  Client 2: total grad norm = 78.951423 (253 params)\n",
            "\n",
            "=== Optimizer Step 380 ===\n",
            "\n",
            "=== Updating Client Weights (Step 380) ===\n",
            "Gradient norms: [8.548211097717285, 10.637801170349121, 8.40014934539795]\n",
            "Target weights: [0.354459673166275, 0.2848329544067383, 0.36070743203163147]\n",
            "Updated weights: [0.373452752828598, 0.3066391944885254, 0.319908082485199]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 380\n",
            "  Loss for client 0: 1.5991\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 380\n",
            "  Loss for client 1: 1.6533\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 380\n",
            "  Loss for client 2: 1.7948\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 381\n",
            "  Client 0: total grad norm = 76.321671 (253 params)\n",
            "  Client 1: total grad norm = 80.443741 (253 params)\n",
            "  Client 2: total grad norm = 103.173117 (253 params)\n",
            "\n",
            "=== Optimizer Step 381 ===\n",
            "\n",
            "=== Updating Client Weights (Step 381) ===\n",
            "Gradient norms: [7.234167575836182, 8.353458404541016, 9.576395034790039]\n",
            "Target weights: [0.3814718723297119, 0.33035796880722046, 0.28817018866539]\n",
            "Updated weights: [0.37585848569869995, 0.3137548267841339, 0.3103867173194885]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 381\n",
            "  Loss for client 0: 1.6824\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 381\n",
            "  Loss for client 1: 1.6006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 381\n",
            "  Loss for client 2: 2.3156\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 382\n",
            "  Client 0: total grad norm = 76.848665 (253 params)\n",
            "  Client 1: total grad norm = 75.263863 (253 params)\n",
            "  Client 2: total grad norm = 92.131281 (253 params)\n",
            "\n",
            "=== Optimizer Step 382 ===\n",
            "\n",
            "=== Updating Client Weights (Step 382) ===\n",
            "Gradient norms: [8.482421875, 7.395866870880127, 8.999642372131348]\n",
            "Target weights: [0.323682963848114, 0.3712364733219147, 0.3050805330276489]\n",
            "Updated weights: [0.36020582914352417, 0.33099931478500366, 0.30879485607147217]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 382\n",
            "  Loss for client 0: 1.3270\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 382\n",
            "  Loss for client 1: 1.6898\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 382\n",
            "  Loss for client 2: 1.5272\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 383\n",
            "  Client 0: total grad norm = 71.755941 (253 params)\n",
            "  Client 1: total grad norm = 75.844160 (253 params)\n",
            "  Client 2: total grad norm = 104.244180 (253 params)\n",
            "\n",
            "=== Optimizer Step 383 ===\n",
            "\n",
            "=== Updating Client Weights (Step 383) ===\n",
            "Gradient norms: [7.429727554321289, 7.7439398765563965, 9.00309944152832]\n",
            "Target weights: [0.359109491109848, 0.34453853964805603, 0.29635190963745117]\n",
            "Updated weights: [0.35987693071365356, 0.33506107330322266, 0.3050619661808014]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 383\n",
            "  Loss for client 0: 1.7343\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 383\n",
            "  Loss for client 1: 2.0631\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 383\n",
            "  Loss for client 2: 2.2306\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 384\n",
            "  Client 0: total grad norm = 72.027681 (253 params)\n",
            "  Client 1: total grad norm = 92.704876 (253 params)\n",
            "  Client 2: total grad norm = 84.766644 (253 params)\n",
            "\n",
            "=== Optimizer Step 384 ===\n",
            "\n",
            "=== Updating Client Weights (Step 384) ===\n",
            "Gradient norms: [7.541493892669678, 8.382983207702637, 8.743409156799316]\n",
            "Target weights: [0.3620362877845764, 0.32569482922554016, 0.31226885318756104]\n",
            "Updated weights: [0.3605247437953949, 0.3322511911392212, 0.3072240352630615]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 384\n",
            "  Loss for client 0: 0.9365\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 384\n",
            "  Loss for client 1: 2.1433\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 384\n",
            "  Loss for client 2: 1.9104\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 385\n",
            "  Client 0: total grad norm = 51.912249 (253 params)\n",
            "  Client 1: total grad norm = 87.431675 (253 params)\n",
            "  Client 2: total grad norm = 69.918575 (253 params)\n",
            "\n",
            "=== Optimizer Step 385 ===\n",
            "\n",
            "=== Updating Client Weights (Step 385) ===\n",
            "Gradient norms: [6.294003963470459, 8.951529502868652, 7.349564075469971]\n",
            "Target weights: [0.390701562166214, 0.2747102677822113, 0.3345881402492523]\n",
            "Updated weights: [0.3695778250694275, 0.3149889409542084, 0.3154332935810089]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 385\n",
            "  Loss for client 0: 1.6906\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 385\n",
            "  Loss for client 1: 1.2951\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 385\n",
            "  Loss for client 2: 1.7843\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 386\n",
            "  Client 0: total grad norm = 87.122159 (253 params)\n",
            "  Client 1: total grad norm = 70.220002 (253 params)\n",
            "  Client 2: total grad norm = 88.376713 (253 params)\n",
            "\n",
            "=== Optimizer Step 386 ===\n",
            "\n",
            "=== Updating Client Weights (Step 386) ===\n",
            "Gradient norms: [8.877158164978027, 7.420306205749512, 8.728961944580078]\n",
            "Target weights: [0.31120550632476807, 0.3723054528236389, 0.31648901104927063]\n",
            "Updated weights: [0.35206612944602966, 0.3321838974952698, 0.31575000286102295]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 386\n",
            "  Loss for client 0: 1.7670\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 386\n",
            "  Loss for client 1: 2.0956\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 386\n",
            "  Loss for client 2: 1.3650\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 387\n",
            "  Client 0: total grad norm = 86.180305 (253 params)\n",
            "  Client 1: total grad norm = 86.618283 (253 params)\n",
            "  Client 2: total grad norm = 86.171982 (253 params)\n",
            "\n",
            "=== Optimizer Step 387 ===\n",
            "\n",
            "=== Updating Client Weights (Step 387) ===\n",
            "Gradient norms: [9.432924270629883, 9.225820541381836, 8.954703330993652]\n",
            "Target weights: [0.32511308789253235, 0.3324113190174103, 0.3424755930900574]\n",
            "Updated weights: [0.34398022294044495, 0.3322521448135376, 0.32376769185066223]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 387\n",
            "  Loss for client 0: 1.6768\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 387\n",
            "  Loss for client 1: 1.3287\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 387\n",
            "  Loss for client 2: 1.8799\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 388\n",
            "  Client 0: total grad norm = 77.791847 (253 params)\n",
            "  Client 1: total grad norm = 83.508017 (253 params)\n",
            "  Client 2: total grad norm = 82.172867 (253 params)\n",
            "\n",
            "=== Optimizer Step 388 ===\n",
            "\n",
            "=== Updating Client Weights (Step 388) ===\n",
            "Gradient norms: [9.187421798706055, 8.196098327636719, 8.540717124938965]\n",
            "Target weights: [0.31282544136047363, 0.35066187381744385, 0.33651262521743774]\n",
            "Updated weights: [0.3346337676048279, 0.3377750515937805, 0.3275911509990692]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 388\n",
            "  Loss for client 0: 1.5195\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 388\n",
            "  Loss for client 1: 1.2271\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 388\n",
            "  Loss for client 2: 1.8112\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 389\n",
            "  Client 0: total grad norm = 87.713002 (253 params)\n",
            "  Client 1: total grad norm = 74.332845 (253 params)\n",
            "  Client 2: total grad norm = 77.932390 (253 params)\n",
            "\n",
            "=== Optimizer Step 389 ===\n",
            "\n",
            "=== Updating Client Weights (Step 389) ===\n",
            "Gradient norms: [8.638075828552246, 8.5269775390625, 9.23209285736084]\n",
            "Target weights: [0.3391340672969818, 0.3435526490211487, 0.3173132836818695]\n",
            "Updated weights: [0.33598387241363525, 0.3395083248615265, 0.32450780272483826]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 389\n",
            "  Loss for client 0: 1.2535\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 389\n",
            "  Loss for client 1: 1.8666\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 389\n",
            "  Loss for client 2: 1.3687\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 390\n",
            "  Client 0: total grad norm = 84.930256 (253 params)\n",
            "  Client 1: total grad norm = 77.618244 (253 params)\n",
            "  Client 2: total grad norm = 61.938356 (253 params)\n",
            "\n",
            "=== Optimizer Step 390 ===\n",
            "\n",
            "=== Updating Client Weights (Step 390) ===\n",
            "Gradient norms: [7.8220133781433105, 8.26496696472168, 6.573672294616699]\n",
            "Target weights: [0.3188464343547821, 0.30175814032554626, 0.37939542531967163]\n",
            "Updated weights: [0.33084264397621155, 0.32818326354026794, 0.3409740924835205]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 390\n",
            "  Loss for client 0: 1.4391\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 390\n",
            "  Loss for client 1: 1.0348\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 390\n",
            "  Loss for client 2: 1.5699\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 391\n",
            "  Client 0: total grad norm = 71.089492 (253 params)\n",
            "  Client 1: total grad norm = 58.850976 (253 params)\n",
            "  Client 2: total grad norm = 69.303504 (253 params)\n",
            "\n",
            "=== Optimizer Step 391 ===\n",
            "\n",
            "=== Updating Client Weights (Step 391) ===\n",
            "Gradient norms: [7.78953218460083, 6.070103168487549, 8.455408096313477]\n",
            "Target weights: [0.31206014752388, 0.40045490860939026, 0.2874849736690521]\n",
            "Updated weights: [0.3252078890800476, 0.34986475110054016, 0.32492735981941223]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 391\n",
            "  Loss for client 0: 1.9698\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 391\n",
            "  Loss for client 1: 2.0227\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 391\n",
            "  Loss for client 2: 1.4251\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 392\n",
            "  Client 0: total grad norm = 99.039784 (253 params)\n",
            "  Client 1: total grad norm = 87.022171 (253 params)\n",
            "  Client 2: total grad norm = 65.838190 (253 params)\n",
            "\n",
            "=== Optimizer Step 392 ===\n",
            "\n",
            "=== Updating Client Weights (Step 392) ===\n",
            "Gradient norms: [11.03333568572998, 8.676326751708984, 6.929619312286377]\n",
            "Target weights: [0.25880861282348633, 0.32911649346351624, 0.4120749235153198]\n",
            "Updated weights: [0.3052881062030792, 0.3436402678489685, 0.35107162594795227]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 392\n",
            "  Loss for client 0: 1.6175\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 392\n",
            "  Loss for client 1: 1.8672\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 392\n",
            "  Loss for client 2: 1.6093\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 393\n",
            "  Client 0: total grad norm = 81.072425 (253 params)\n",
            "  Client 1: total grad norm = 82.562609 (253 params)\n",
            "  Client 2: total grad norm = 65.821121 (253 params)\n",
            "\n",
            "=== Optimizer Step 393 ===\n",
            "\n",
            "=== Updating Client Weights (Step 393) ===\n",
            "Gradient norms: [8.755553245544434, 8.626811981201172, 8.01095199584961]\n",
            "Target weights: [0.32176363468170166, 0.3265654444694519, 0.35167089104652405]\n",
            "Updated weights: [0.3102307617664337, 0.33851781487464905, 0.35125139355659485]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 393\n",
            "  Loss for client 0: 1.4085\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 393\n",
            "  Loss for client 1: 1.9550\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 393\n",
            "  Loss for client 2: 2.2163\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 394\n",
            "  Client 0: total grad norm = 83.162410 (253 params)\n",
            "  Client 1: total grad norm = 86.216641 (253 params)\n",
            "  Client 2: total grad norm = 97.402654 (253 params)\n",
            "\n",
            "=== Optimizer Step 394 ===\n",
            "\n",
            "=== Updating Client Weights (Step 394) ===\n",
            "Gradient norms: [9.175135612487793, 8.836386680603027, 9.743475914001465]\n",
            "Target weights: [0.3355698883533478, 0.348434180021286, 0.3159959614276886]\n",
            "Updated weights: [0.3178325295448303, 0.34149274230003357, 0.3406747877597809]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 394\n",
            "  Loss for client 0: 1.1734\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 394\n",
            "  Loss for client 1: 1.6772\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 394\n",
            "  Loss for client 2: 1.5235\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 395\n",
            "  Client 0: total grad norm = 67.968901 (253 params)\n",
            "  Client 1: total grad norm = 77.515649 (253 params)\n",
            "  Client 2: total grad norm = 89.172191 (253 params)\n",
            "\n",
            "=== Optimizer Step 395 ===\n",
            "\n",
            "=== Updating Client Weights (Step 395) ===\n",
            "Gradient norms: [7.534461975097656, 8.222389221191406, 9.268756866455078]\n",
            "Target weights: [0.36640465259552, 0.3357493579387665, 0.2978459596633911]\n",
            "Updated weights: [0.33240416646003723, 0.33976972103118896, 0.3278261423110962]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 395\n",
            "  Loss for client 0: 2.2205\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 395\n",
            "  Loss for client 1: 2.2186\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 395\n",
            "  Loss for client 2: 2.2490\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 396\n",
            "  Client 0: total grad norm = 84.800010 (253 params)\n",
            "  Client 1: total grad norm = 89.777856 (253 params)\n",
            "  Client 2: total grad norm = 98.020780 (253 params)\n",
            "\n",
            "=== Optimizer Step 396 ===\n",
            "\n",
            "=== Updating Client Weights (Step 396) ===\n",
            "Gradient norms: [9.239302635192871, 9.636563301086426, 10.688158988952637]\n",
            "Target weights: [0.3542056679725647, 0.3396037817001343, 0.3061905801296234]\n",
            "Updated weights: [0.33894461393356323, 0.3397199511528015, 0.32133549451828003]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 396\n",
            "  Loss for client 0: 1.9570\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 396\n",
            "  Loss for client 1: 1.8689\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 396\n",
            "  Loss for client 2: 1.8815\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 397\n",
            "  Client 0: total grad norm = 80.806026 (253 params)\n",
            "  Client 1: total grad norm = 91.489738 (253 params)\n",
            "  Client 2: total grad norm = 80.948906 (253 params)\n",
            "\n",
            "=== Optimizer Step 397 ===\n",
            "\n",
            "=== Updating Client Weights (Step 397) ===\n",
            "Gradient norms: [9.036163330078125, 10.06902027130127, 8.768906593322754]\n",
            "Target weights: [0.34154170751571655, 0.3065071403980255, 0.35195115208625793]\n",
            "Updated weights: [0.33972370624542236, 0.32975608110427856, 0.3305201828479767]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 397\n",
            "  Loss for client 0: 1.1414\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 397\n",
            "  Loss for client 1: 1.8260\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 397\n",
            "  Loss for client 2: 1.8638\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 398\n",
            "  Client 0: total grad norm = 60.217038 (253 params)\n",
            "  Client 1: total grad norm = 71.243376 (253 params)\n",
            "  Client 2: total grad norm = 83.030210 (253 params)\n",
            "\n",
            "=== Optimizer Step 398 ===\n",
            "\n",
            "=== Updating Client Weights (Step 398) ===\n",
            "Gradient norms: [6.860384941101074, 7.812199592590332, 9.028334617614746]\n",
            "Target weights: [0.3790699243545532, 0.332885205745697, 0.28804489970207214]\n",
            "Updated weights: [0.3515275716781616, 0.3306948244571686, 0.3177776038646698]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 398\n",
            "  Loss for client 0: 2.0598\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 398\n",
            "  Loss for client 1: 1.6264\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 398\n",
            "  Loss for client 2: 1.4273\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 399\n",
            "  Client 0: total grad norm = 78.111773 (253 params)\n",
            "  Client 1: total grad norm = 75.513031 (253 params)\n",
            "  Client 2: total grad norm = 72.806336 (253 params)\n",
            "\n",
            "=== Optimizer Step 399 ===\n",
            "\n",
            "=== Updating Client Weights (Step 399) ===\n",
            "Gradient norms: [10.091781616210938, 8.2581205368042, 7.550321578979492]\n",
            "Target weights: [0.28100571036338806, 0.3434011936187744, 0.37559306621551514]\n",
            "Updated weights: [0.33037102222442627, 0.3345067501068115, 0.3351222574710846]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 399\n",
            "  Loss for client 0: 2.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 399\n",
            "  Loss for client 1: 1.7614\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 399\n",
            "  Loss for client 2: 0.9223\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 400\n",
            "  Client 0: total grad norm = 85.030303 (253 params)\n",
            "  Client 1: total grad norm = 82.311880 (253 params)\n",
            "  Client 2: total grad norm = 67.796464 (253 params)\n",
            "\n",
            "=== Optimizer Step 400 ===\n",
            "\n",
            "=== Updating Client Weights (Step 400) ===\n",
            "Gradient norms: [9.176024436950684, 9.458413124084473, 6.466569900512695]\n",
            "Target weights: [0.29506024718284607, 0.2862509787082672, 0.4186887741088867]\n",
            "Updated weights: [0.31977778673171997, 0.32003000378608704, 0.3601922392845154]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 400\n",
            "  Val Loss = 1.9147 (5 batches)\n",
            "–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ /content/drive/MyDrive/meritopt_checkpoints/step_400\n",
            "\n",
            "[Processing] Client 0, batch 400\n",
            "  Loss for client 0: 1.6010\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 400\n",
            "  Loss for client 1: 1.7931\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 400\n",
            "  Loss for client 2: 1.5479\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 401\n",
            "  Client 0: total grad norm = 68.953038 (253 params)\n",
            "  Client 1: total grad norm = 78.489647 (253 params)\n",
            "  Client 2: total grad norm = 68.214797 (253 params)\n",
            "\n",
            "=== Optimizer Step 401 ===\n",
            "\n",
            "=== Updating Client Weights (Step 401) ===\n",
            "Gradient norms: [8.492551803588867, 8.981165885925293, 9.622417449951172]\n",
            "Target weights: [0.35358482599258423, 0.33434829115867615, 0.31206685304641724]\n",
            "Updated weights: [0.3299199044704437, 0.3243255019187927, 0.34575462341308594]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 401\n",
            "  Loss for client 0: 0.9379\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 401\n",
            "  Loss for client 1: 1.8245\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 401\n",
            "  Loss for client 2: 1.9020\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 402\n",
            "  Client 0: total grad norm = 55.302356 (253 params)\n",
            "  Client 1: total grad norm = 86.344843 (253 params)\n",
            "  Client 2: total grad norm = 83.388492 (253 params)\n",
            "\n",
            "=== Optimizer Step 402 ===\n",
            "\n",
            "=== Updating Client Weights (Step 402) ===\n",
            "Gradient norms: [7.007796764373779, 9.375454902648926, 11.638619422912598]\n",
            "Target weights: [0.425608366727829, 0.3181261122226715, 0.2562655210494995]\n",
            "Updated weights: [0.35862645506858826, 0.3224656879901886, 0.31890788674354553]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 402\n",
            "  Loss for client 0: 1.0419\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 402\n",
            "  Loss for client 1: 2.2426\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 402\n",
            "  Loss for client 2: 1.5843\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 403\n",
            "  Client 0: total grad norm = 100.432229 (253 params)\n",
            "  Client 1: total grad norm = 90.820887 (253 params)\n",
            "  Client 2: total grad norm = 73.022346 (253 params)\n",
            "\n",
            "=== Optimizer Step 403 ===\n",
            "\n",
            "=== Updating Client Weights (Step 403) ===\n",
            "Gradient norms: [11.463796615600586, 10.449134826660156, 7.754408359527588]\n",
            "Target weights: [0.27968403697013855, 0.30684271454811096, 0.41347330808639526]\n",
            "Updated weights: [0.3349437117576599, 0.3177787661552429, 0.3472774922847748]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 403\n",
            "  Loss for client 0: 1.6051\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 403\n",
            "  Loss for client 1: 1.9834\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 403\n",
            "  Loss for client 2: 1.4157\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 404\n",
            "  Client 0: total grad norm = 81.294748 (253 params)\n",
            "  Client 1: total grad norm = 75.037126 (253 params)\n",
            "  Client 2: total grad norm = 62.368744 (253 params)\n",
            "\n",
            "=== Optimizer Step 404 ===\n",
            "\n",
            "=== Updating Client Weights (Step 404) ===\n",
            "Gradient norms: [8.946842193603516, 9.0093355178833, 6.892968654632568]\n",
            "Target weights: [0.3038562536239624, 0.3017485439777374, 0.39439520239830017]\n",
            "Updated weights: [0.3256174921989441, 0.31296971440315247, 0.36141282320022583]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 404\n",
            "  Loss for client 0: 1.5782\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 404\n",
            "  Loss for client 1: 1.5300\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 404\n",
            "  Loss for client 2: 1.9987\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 405\n",
            "  Client 0: total grad norm = 76.048869 (253 params)\n",
            "  Client 1: total grad norm = 100.970708 (253 params)\n",
            "  Client 2: total grad norm = 70.273852 (253 params)\n",
            "\n",
            "=== Optimizer Step 405 ===\n",
            "\n",
            "=== Updating Client Weights (Step 405) ===\n",
            "Gradient norms: [8.08171558380127, 10.739880561828613, 8.8335542678833]\n",
            "Target weights: [0.37489914894104004, 0.2821100354194641, 0.34299084544181824]\n",
            "Updated weights: [0.3404020071029663, 0.30371180176734924, 0.35588622093200684]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 405\n",
            "  Loss for client 0: 2.2668\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 405\n",
            "  Loss for client 1: 2.4806\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 405\n",
            "  Loss for client 2: 1.9252\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 406\n",
            "  Client 0: total grad norm = 104.069024 (253 params)\n",
            "  Client 1: total grad norm = 91.285411 (253 params)\n",
            "  Client 2: total grad norm = 78.317149 (253 params)\n",
            "\n",
            "=== Optimizer Step 406 ===\n",
            "\n",
            "=== Updating Client Weights (Step 406) ===\n",
            "Gradient norms: [10.901984214782715, 9.746515274047852, 8.015026092529297]\n",
            "Target weights: [0.28745996952056885, 0.32153892517089844, 0.3910010755062103]\n",
            "Updated weights: [0.32451939582824707, 0.3090599477291107, 0.3664206862449646]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 406\n",
            "  Loss for client 0: 0.7625\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 406\n",
            "  Loss for client 1: 1.5665\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 406\n",
            "  Loss for client 2: 1.7449\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 407\n",
            "  Client 0: total grad norm = 49.039738 (253 params)\n",
            "  Client 1: total grad norm = 72.761142 (253 params)\n",
            "  Client 2: total grad norm = 81.020380 (253 params)\n",
            "\n",
            "=== Optimizer Step 407 ===\n",
            "\n",
            "=== Updating Client Weights (Step 407) ===\n",
            "Gradient norms: [5.792358875274658, 8.212448120117188, 8.501090049743652]\n",
            "Target weights: [0.4189918637275696, 0.2955210506916046, 0.2854870557785034]\n",
            "Updated weights: [0.3528611361980438, 0.3049982786178589, 0.3421405851840973]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 407\n",
            "  Loss for client 0: 1.7450\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 407\n",
            "  Loss for client 1: 1.6467\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 407\n",
            "  Loss for client 2: 1.7843\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 408\n",
            "  Client 0: total grad norm = 90.915834 (253 params)\n",
            "  Client 1: total grad norm = 66.688616 (253 params)\n",
            "  Client 2: total grad norm = 77.944295 (253 params)\n",
            "\n",
            "=== Optimizer Step 408 ===\n",
            "\n",
            "=== Updating Client Weights (Step 408) ===\n",
            "Gradient norms: [10.54986572265625, 7.70365571975708, 8.466547012329102]\n",
            "Target weights: [0.27658483386039734, 0.378772497177124, 0.34464263916015625]\n",
            "Updated weights: [0.32997825741767883, 0.3271305561065674, 0.34289121627807617]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 408\n",
            "  Loss for client 0: 1.1774\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 408\n",
            "  Loss for client 1: 2.4810\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 408\n",
            "  Loss for client 2: 1.8056\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 409\n",
            "  Client 0: total grad norm = 78.127142 (253 params)\n",
            "  Client 1: total grad norm = 94.059017 (253 params)\n",
            "  Client 2: total grad norm = 75.193784 (253 params)\n",
            "\n",
            "=== Optimizer Step 409 ===\n",
            "\n",
            "=== Updating Client Weights (Step 409) ===\n",
            "Gradient norms: [8.647627830505371, 10.05730152130127, 8.74536418914795]\n",
            "Target weights: [0.351042240858078, 0.3018386662006378, 0.3471190631389618]\n",
            "Updated weights: [0.3362974524497986, 0.3195430040359497, 0.3441595733165741]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 409\n",
            "  Loss for client 0: 1.3524\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 409\n",
            "  Loss for client 1: 2.0287\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 409\n",
            "  Loss for client 2: 1.3597\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 410\n",
            "  Client 0: total grad norm = 93.652319 (253 params)\n",
            "  Client 1: total grad norm = 83.937604 (253 params)\n",
            "  Client 2: total grad norm = 68.449419 (253 params)\n",
            "\n",
            "=== Optimizer Step 410 ===\n",
            "\n",
            "=== Updating Client Weights (Step 410) ===\n",
            "Gradient norms: [10.07214641571045, 11.544166564941406, 7.885883331298828]\n",
            "Target weights: [0.3174879550933838, 0.2770044505596161, 0.4055075943470001]\n",
            "Updated weights: [0.3306546211242676, 0.30678144097328186, 0.36256396770477295]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 410\n",
            "  Loss for client 0: 1.2176\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 410\n",
            "  Loss for client 1: 1.4005\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 410\n",
            "  Loss for client 2: 2.0493\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 411\n",
            "  Client 0: total grad norm = 68.001486 (253 params)\n",
            "  Client 1: total grad norm = 68.750189 (253 params)\n",
            "  Client 2: total grad norm = 79.405276 (253 params)\n",
            "\n",
            "=== Optimizer Step 411 ===\n",
            "\n",
            "=== Updating Client Weights (Step 411) ===\n",
            "Gradient norms: [7.50407075881958, 7.188586235046387, 9.297219276428223]\n",
            "Target weights: [0.35075196623802185, 0.36614537239074707, 0.2831026315689087]\n",
            "Updated weights: [0.33668380975723267, 0.32459062337875366, 0.33872556686401367]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 411\n",
            "  Loss for client 0: 1.5509\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 411\n",
            "  Loss for client 1: 1.9774\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 411\n",
            "  Loss for client 2: 1.5481\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 412\n",
            "  Client 0: total grad norm = 97.500339 (253 params)\n",
            "  Client 1: total grad norm = 115.146309 (253 params)\n",
            "  Client 2: total grad norm = 85.729233 (253 params)\n",
            "\n",
            "=== Optimizer Step 412 ===\n",
            "\n",
            "=== Updating Client Weights (Step 412) ===\n",
            "Gradient norms: [10.754213333129883, 9.714021682739258, 10.589151382446289]\n",
            "Target weights: [0.3202388286590576, 0.3545304536819458, 0.3252306580543518]\n",
            "Updated weights: [0.3317503333091736, 0.3335725963115692, 0.334677129983902]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 412\n",
            "  Loss for client 0: 1.5024\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 412\n",
            "  Loss for client 1: 1.6203\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 412\n",
            "  Loss for client 2: 1.5394\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 413\n",
            "  Client 0: total grad norm = 70.277439 (253 params)\n",
            "  Client 1: total grad norm = 72.662205 (253 params)\n",
            "  Client 2: total grad norm = 78.331214 (253 params)\n",
            "\n",
            "=== Optimizer Step 413 ===\n",
            "\n",
            "=== Updating Client Weights (Step 413) ===\n",
            "Gradient norms: [8.505362510681152, 8.557063102722168, 8.331586837768555]\n",
            "Target weights: [0.33169522881507874, 0.32969120144844055, 0.3386135697364807]\n",
            "Updated weights: [0.3317337930202484, 0.33240818977355957, 0.3358580470085144]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 413\n",
            "  Loss for client 0: 1.7033\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 413\n",
            "  Loss for client 1: 1.8646\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 413\n",
            "  Loss for client 2: 2.0646\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 414\n",
            "  Client 0: total grad norm = 83.478159 (253 params)\n",
            "  Client 1: total grad norm = 75.106764 (253 params)\n",
            "  Client 2: total grad norm = 79.345471 (253 params)\n",
            "\n",
            "=== Optimizer Step 414 ===\n",
            "\n",
            "=== Updating Client Weights (Step 414) ===\n",
            "Gradient norms: [8.481660842895508, 8.276712417602539, 8.863426208496094]\n",
            "Target weights: [0.3353802561759949, 0.34368497133255005, 0.3209347724914551]\n",
            "Updated weights: [0.33282771706581116, 0.3357912302017212, 0.33138105273246765]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 414\n",
            "  Loss for client 0: 1.5309\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 414\n",
            "  Loss for client 1: 2.3939\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 414\n",
            "  Loss for client 2: 1.7417\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 415\n",
            "  Client 0: total grad norm = 75.689029 (253 params)\n",
            "  Client 1: total grad norm = 96.140458 (253 params)\n",
            "  Client 2: total grad norm = 95.276143 (253 params)\n",
            "\n",
            "=== Optimizer Step 415 ===\n",
            "\n",
            "=== Updating Client Weights (Step 415) ===\n",
            "Gradient norms: [8.264524459838867, 10.795269012451172, 11.232327461242676]\n",
            "Target weights: [0.39978423714637756, 0.3060624599456787, 0.2941533327102661]\n",
            "Updated weights: [0.3529146909713745, 0.3268725872039795, 0.320212721824646]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 415\n",
            "  Loss for client 0: 1.0577\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 415\n",
            "  Loss for client 1: 2.1915\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 415\n",
            "  Loss for client 2: 1.7951\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 416\n",
            "  Client 0: total grad norm = 72.758842 (253 params)\n",
            "  Client 1: total grad norm = 88.004696 (253 params)\n",
            "  Client 2: total grad norm = 95.445684 (253 params)\n",
            "\n",
            "=== Optimizer Step 416 ===\n",
            "\n",
            "=== Updating Client Weights (Step 416) ===\n",
            "Gradient norms: [7.958132266998291, 8.651806831359863, 8.52713680267334]\n",
            "Target weights: [0.3504966199398041, 0.3223949372768402, 0.3271084725856781]\n",
            "Updated weights: [0.3521892726421356, 0.3255292773246765, 0.32228145003318787]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 416\n",
            "  Loss for client 0: 1.6470\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 416\n",
            "  Loss for client 1: 1.7444\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 416\n",
            "  Loss for client 2: 1.5557\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 417\n",
            "  Client 0: total grad norm = 94.366651 (253 params)\n",
            "  Client 1: total grad norm = 73.809183 (253 params)\n",
            "  Client 2: total grad norm = 64.150059 (253 params)\n",
            "\n",
            "=== Optimizer Step 417 ===\n",
            "\n",
            "=== Updating Client Weights (Step 417) ===\n",
            "Gradient norms: [10.61435604095459, 8.026248931884766, 7.704040050506592]\n",
            "Target weights: [0.2702541947364807, 0.3573991060256958, 0.3723467290401459]\n",
            "Updated weights: [0.32760876417160034, 0.3350902199745178, 0.33730101585388184]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 417\n",
            "  Loss for client 0: 1.2564\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 417\n",
            "  Loss for client 1: 2.3243\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 417\n",
            "  Loss for client 2: 2.4110\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 418\n",
            "  Client 0: total grad norm = 71.250543 (253 params)\n",
            "  Client 1: total grad norm = 95.157385 (253 params)\n",
            "  Client 2: total grad norm = 81.580000 (253 params)\n",
            "\n",
            "=== Optimizer Step 418 ===\n",
            "\n",
            "=== Updating Client Weights (Step 418) ===\n",
            "Gradient norms: [6.952825546264648, 9.801308631896973, 8.499834060668945]\n",
            "Target weights: [0.39566779136657715, 0.2806777358055115, 0.323654443025589]\n",
            "Updated weights: [0.34802645444869995, 0.3187664747238159, 0.33320704102516174]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 418\n",
            "  Loss for client 0: 1.1799\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 418\n",
            "  Loss for client 1: 2.0419\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 418\n",
            "  Loss for client 2: 1.7126\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 419\n",
            "  Client 0: total grad norm = 65.705283 (253 params)\n",
            "  Client 1: total grad norm = 87.561884 (253 params)\n",
            "  Client 2: total grad norm = 74.045494 (253 params)\n",
            "\n",
            "=== Optimizer Step 419 ===\n",
            "\n",
            "=== Updating Client Weights (Step 419) ===\n",
            "Gradient norms: [6.838116645812988, 9.208605766296387, 7.8933939933776855]\n",
            "Target weights: [0.3833051025867462, 0.2846342921257019, 0.3320605754852295]\n",
            "Updated weights: [0.3586100935935974, 0.3085268437862396, 0.33286312222480774]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 419\n",
            "  Loss for client 0: 1.7138\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 419\n",
            "  Loss for client 1: 1.9642\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 419\n",
            "  Loss for client 2: 1.9858\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 420\n",
            "  Client 0: total grad norm = 72.610114 (253 params)\n",
            "  Client 1: total grad norm = 83.892871 (253 params)\n",
            "  Client 2: total grad norm = 91.438935 (253 params)\n",
            "\n",
            "=== Optimizer Step 420 ===\n",
            "\n",
            "=== Updating Client Weights (Step 420) ===\n",
            "Gradient norms: [7.791113376617432, 9.304287910461426, 9.55355453491211]\n",
            "Target weights: [0.37694770097732544, 0.31564396619796753, 0.30740830302238464]\n",
            "Updated weights: [0.36411136388778687, 0.3106619715690613, 0.32522666454315186]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 420\n",
            "  Loss for client 0: 2.0272\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 420\n",
            "  Loss for client 1: 1.8045\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 420\n",
            "  Loss for client 2: 1.6230\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 421\n",
            "  Client 0: total grad norm = 99.335089 (253 params)\n",
            "  Client 1: total grad norm = 76.584861 (253 params)\n",
            "  Client 2: total grad norm = 84.391871 (253 params)\n",
            "\n",
            "=== Optimizer Step 421 ===\n",
            "\n",
            "=== Updating Client Weights (Step 421) ===\n",
            "Gradient norms: [10.893045425415039, 7.7383928298950195, 8.451525688171387]\n",
            "Target weights: [0.27052268385887146, 0.3808046579360962, 0.34867265820503235]\n",
            "Updated weights: [0.33603477478027344, 0.3317047655582428, 0.33226045966148376]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 421\n",
            "  Loss for client 0: 1.9905\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 421\n",
            "  Loss for client 1: 1.6842\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 421\n",
            "  Loss for client 2: 1.4013\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 422\n",
            "  Client 0: total grad norm = 93.450004 (253 params)\n",
            "  Client 1: total grad norm = 71.346174 (253 params)\n",
            "  Client 2: total grad norm = 65.625171 (253 params)\n",
            "\n",
            "=== Optimizer Step 422 ===\n",
            "\n",
            "=== Updating Client Weights (Step 422) ===\n",
            "Gradient norms: [9.197089195251465, 7.711928844451904, 7.1813435554504395]\n",
            "Target weights: [0.2879129946231842, 0.34335917234420776, 0.3687278628349304]\n",
            "Updated weights: [0.32159823179244995, 0.33520108461380005, 0.34320068359375]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 422\n",
            "  Loss for client 0: 1.6939\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 422\n",
            "  Loss for client 1: 2.2026\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 422\n",
            "  Loss for client 2: 1.6577\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 423\n",
            "  Client 0: total grad norm = 82.814370 (253 params)\n",
            "  Client 1: total grad norm = 80.410830 (253 params)\n",
            "  Client 2: total grad norm = 68.597242 (253 params)\n",
            "\n",
            "=== Optimizer Step 423 ===\n",
            "\n",
            "=== Updating Client Weights (Step 423) ===\n",
            "Gradient norms: [8.458510398864746, 8.411911964416504, 7.554238796234131]\n",
            "Target weights: [0.3199750781059265, 0.3217476010322571, 0.3582773208618164]\n",
            "Updated weights: [0.3211112916469574, 0.33116504549980164, 0.34772366285324097]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 423\n",
            "  Loss for client 0: 1.4468\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 423\n",
            "  Loss for client 1: 1.5973\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 423\n",
            "  Loss for client 2: 2.2554\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 424\n",
            "  Client 0: total grad norm = 77.371434 (253 params)\n",
            "  Client 1: total grad norm = 65.352603 (253 params)\n",
            "  Client 2: total grad norm = 75.389746 (253 params)\n",
            "\n",
            "=== Optimizer Step 424 ===\n",
            "\n",
            "=== Updating Client Weights (Step 424) ===\n",
            "Gradient norms: [9.401236534118652, 8.10403823852539, 8.93307876586914]\n",
            "Target weights: [0.31128644943237305, 0.3611134886741638, 0.3276000916957855]\n",
            "Updated weights: [0.31816384196281433, 0.34014958143234253, 0.3416866064071655]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 424\n",
            "  Loss for client 0: 1.8675\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 424\n",
            "  Loss for client 1: 1.9877\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 424\n",
            "  Loss for client 2: 2.1349\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 425\n",
            "  Client 0: total grad norm = 74.487285 (253 params)\n",
            "  Client 1: total grad norm = 67.891247 (253 params)\n",
            "  Client 2: total grad norm = 89.886792 (253 params)\n",
            "\n",
            "=== Optimizer Step 425 ===\n",
            "\n",
            "=== Updating Client Weights (Step 425) ===\n",
            "Gradient norms: [7.8033037185668945, 8.38503646850586, 12.344613075256348]\n",
            "Target weights: [0.39020663499832153, 0.36313506960868835, 0.24665826559066772]\n",
            "Updated weights: [0.3397766947746277, 0.34704524278640747, 0.3131781220436096]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 425\n",
            "  Loss for client 0: 1.1283\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 425\n",
            "  Loss for client 1: 1.8331\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 425\n",
            "  Loss for client 2: 1.8674\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 426\n",
            "  Client 0: total grad norm = 60.887358 (253 params)\n",
            "  Client 1: total grad norm = 74.501556 (253 params)\n",
            "  Client 2: total grad norm = 68.727498 (253 params)\n",
            "\n",
            "=== Optimizer Step 426 ===\n",
            "\n",
            "=== Updating Client Weights (Step 426) ===\n",
            "Gradient norms: [6.924355506896973, 7.2709760665893555, 9.143759727478027]\n",
            "Target weights: [0.3690575063228607, 0.35146385431289673, 0.27947860956192017]\n",
            "Updated weights: [0.3485609292984009, 0.34837082028388977, 0.30306828022003174]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 426\n",
            "  Loss for client 0: 1.8856\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 426\n",
            "  Loss for client 1: 2.1446\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 426\n",
            "  Loss for client 2: 1.6814\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 427\n",
            "  Client 0: total grad norm = 74.081868 (253 params)\n",
            "  Client 1: total grad norm = 91.924640 (253 params)\n",
            "  Client 2: total grad norm = 63.791270 (253 params)\n",
            "\n",
            "=== Optimizer Step 427 ===\n",
            "\n",
            "=== Updating Client Weights (Step 427) ===\n",
            "Gradient norms: [7.821829319000244, 10.051131248474121, 7.225592136383057]\n",
            "Target weights: [0.3495621681213379, 0.27203065156936646, 0.37840715050697327]\n",
            "Updated weights: [0.34886130690574646, 0.3254687786102295, 0.32566994428634644]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 427\n",
            "  Loss for client 0: 1.4285\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 427\n",
            "  Loss for client 1: 1.6274\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 427\n",
            "  Loss for client 2: 2.3340\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 428\n",
            "  Client 0: total grad norm = 68.848425 (253 params)\n",
            "  Client 1: total grad norm = 72.327298 (253 params)\n",
            "  Client 2: total grad norm = 85.378331 (253 params)\n",
            "\n",
            "=== Optimizer Step 428 ===\n",
            "\n",
            "=== Updating Client Weights (Step 428) ===\n",
            "Gradient norms: [7.85021448135376, 8.614339828491211, 11.021883010864258]\n",
            "Target weights: [0.381165087223053, 0.3473542630672455, 0.27148061990737915]\n",
            "Updated weights: [0.3585524559020996, 0.3320344388484955, 0.3094131350517273]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 428\n",
            "  Loss for client 0: 1.2451\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 428\n",
            "  Loss for client 1: 2.3204\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 428\n",
            "  Loss for client 2: 1.9008\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 429\n",
            "  Client 0: total grad norm = 59.627059 (253 params)\n",
            "  Client 1: total grad norm = 92.579768 (253 params)\n",
            "  Client 2: total grad norm = 70.306100 (253 params)\n",
            "\n",
            "=== Optimizer Step 429 ===\n",
            "\n",
            "=== Updating Client Weights (Step 429) ===\n",
            "Gradient norms: [6.758943557739258, 9.852245330810547, 8.140161514282227]\n",
            "Target weights: [0.39740079641342163, 0.27262917160987854, 0.3299700617790222]\n",
            "Updated weights: [0.37020695209503174, 0.3142128586769104, 0.31558021903038025]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 429\n",
            "  Loss for client 0: 1.8867\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 429\n",
            "  Loss for client 1: 1.6942\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 429\n",
            "  Loss for client 2: 1.6860\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 430\n",
            "  Client 0: total grad norm = 96.539539 (253 params)\n",
            "  Client 1: total grad norm = 75.621621 (253 params)\n",
            "  Client 2: total grad norm = 80.015010 (253 params)\n",
            "\n",
            "=== Optimizer Step 430 ===\n",
            "\n",
            "=== Updating Client Weights (Step 430) ===\n",
            "Gradient norms: [12.781071662902832, 9.297401428222656, 10.805036544799805]\n",
            "Target weights: [0.2810904383659363, 0.38641300797462463, 0.3324965238571167]\n",
            "Updated weights: [0.3434720039367676, 0.3358728885650635, 0.32065510749816895]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 430\n",
            "  Loss for client 0: 2.0502\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 430\n",
            "  Loss for client 1: 2.0020\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 430\n",
            "  Loss for client 2: 2.0232\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 431\n",
            "  Client 0: total grad norm = 74.100061 (253 params)\n",
            "  Client 1: total grad norm = 72.856101 (253 params)\n",
            "  Client 2: total grad norm = 85.637903 (253 params)\n",
            "\n",
            "=== Optimizer Step 431 ===\n",
            "\n",
            "=== Updating Client Weights (Step 431) ===\n",
            "Gradient norms: [8.406344413757324, 9.17228889465332, 10.45532512664795]\n",
            "Target weights: [0.3675769567489624, 0.33688193559646606, 0.29554110765457153]\n",
            "Updated weights: [0.35070347785949707, 0.3361755907535553, 0.31312090158462524]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 431\n",
            "  Loss for client 0: 1.7911\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 431\n",
            "  Loss for client 1: 1.7198\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 431\n",
            "  Loss for client 2: 1.6427\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 432\n",
            "  Client 0: total grad norm = 77.164750 (253 params)\n",
            "  Client 1: total grad norm = 66.551150 (253 params)\n",
            "  Client 2: total grad norm = 62.708445 (253 params)\n",
            "\n",
            "=== Optimizer Step 432 ===\n",
            "\n",
            "=== Updating Client Weights (Step 432) ===\n",
            "Gradient norms: [7.740133285522461, 7.179906368255615, 7.772890567779541]\n",
            "Target weights: [0.32532885670661926, 0.3507132828235626, 0.3239578306674957]\n",
            "Updated weights: [0.34309113025665283, 0.3405369222164154, 0.31637200713157654]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 432\n",
            "  Loss for client 0: 1.1172\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 432\n",
            "  Loss for client 1: 1.3744\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 432\n",
            "  Loss for client 2: 1.9945\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 433\n",
            "  Client 0: total grad norm = 66.304612 (253 params)\n",
            "  Client 1: total grad norm = 64.559940 (253 params)\n",
            "  Client 2: total grad norm = 91.233712 (253 params)\n",
            "\n",
            "=== Optimizer Step 433 ===\n",
            "\n",
            "=== Updating Client Weights (Step 433) ===\n",
            "Gradient norms: [6.7132954597473145, 7.622183322906494, 8.086081504821777]\n",
            "Target weights: [0.36886948347091675, 0.3248845934867859, 0.30624595284461975]\n",
            "Updated weights: [0.35082465410232544, 0.33584123849868774, 0.313334196805954]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 433\n",
            "  Loss for client 0: 1.5516\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 433\n",
            "  Loss for client 1: 1.6567\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 433\n",
            "  Loss for client 2: 2.1055\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 434\n",
            "  Client 0: total grad norm = 66.000612 (253 params)\n",
            "  Client 1: total grad norm = 73.446229 (253 params)\n",
            "  Client 2: total grad norm = 75.671585 (253 params)\n",
            "\n",
            "=== Optimizer Step 434 ===\n",
            "\n",
            "=== Updating Client Weights (Step 434) ===\n",
            "Gradient norms: [8.541708946228027, 8.01086139678955, 9.431939125061035]\n",
            "Target weights: [0.33648720383644104, 0.35878485441207886, 0.3047279715538025]\n",
            "Updated weights: [0.3465234041213989, 0.3427242934703827, 0.3107523024082184]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 434\n",
            "  Loss for client 0: 1.2054\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 434\n",
            "  Loss for client 1: 1.6497\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 434\n",
            "  Loss for client 2: 1.5419\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 435\n",
            "  Client 0: total grad norm = 60.801826 (253 params)\n",
            "  Client 1: total grad norm = 80.388121 (253 params)\n",
            "  Client 2: total grad norm = 65.928548 (253 params)\n",
            "\n",
            "=== Optimizer Step 435 ===\n",
            "\n",
            "=== Updating Client Weights (Step 435) ===\n",
            "Gradient norms: [6.479921340942383, 8.196611404418945, 7.8688435554504395]\n",
            "Target weights: [0.38254785537719727, 0.30242741107940674, 0.3150247037410736]\n",
            "Updated weights: [0.3573307394981384, 0.3306352198123932, 0.3120340406894684]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 435\n",
            "  Loss for client 0: 1.3122\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 435\n",
            "  Loss for client 1: 1.6501\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 435\n",
            "  Loss for client 2: 1.7002\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 436\n",
            "  Client 0: total grad norm = 64.190595 (253 params)\n",
            "  Client 1: total grad norm = 73.259463 (253 params)\n",
            "  Client 2: total grad norm = 76.058634 (253 params)\n",
            "\n",
            "=== Optimizer Step 436 ===\n",
            "\n",
            "=== Updating Client Weights (Step 436) ===\n",
            "Gradient norms: [7.243467807769775, 8.221884727478027, 7.832890510559082]\n",
            "Target weights: [0.35641106963157654, 0.31399762630462646, 0.3295912444591522]\n",
            "Updated weights: [0.35705482959747314, 0.32564395666122437, 0.3173012137413025]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 436\n",
            "  Loss for client 0: 1.6211\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 436\n",
            "  Loss for client 1: 1.6754\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 436\n",
            "  Loss for client 2: 1.9248\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 437\n",
            "  Client 0: total grad norm = 73.390681 (253 params)\n",
            "  Client 1: total grad norm = 70.567484 (253 params)\n",
            "  Client 2: total grad norm = 73.631577 (253 params)\n",
            "\n",
            "=== Optimizer Step 437 ===\n",
            "\n",
            "=== Updating Client Weights (Step 437) ===\n",
            "Gradient norms: [9.360983848571777, 9.101078033447266, 7.95875358581543]\n",
            "Target weights: [0.31203731894493103, 0.32094839215278625, 0.3670142889022827]\n",
            "Updated weights: [0.34354957938194275, 0.32423529028892517, 0.3322151303291321]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 437\n",
            "  Loss for client 0: 0.9598\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 437\n",
            "  Loss for client 1: 1.5812\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 437\n",
            "  Loss for client 2: 1.3975\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 438\n",
            "  Client 0: total grad norm = 55.131898 (253 params)\n",
            "  Client 1: total grad norm = 71.733572 (253 params)\n",
            "  Client 2: total grad norm = 57.354068 (253 params)\n",
            "\n",
            "=== Optimizer Step 438 ===\n",
            "\n",
            "=== Updating Client Weights (Step 438) ===\n",
            "Gradient norms: [6.4313201904296875, 8.810091972351074, 6.373185157775879]\n",
            "Target weights: [0.365081250667572, 0.2665073573589325, 0.36841142177581787]\n",
            "Updated weights: [0.35000908374786377, 0.30691689252853394, 0.3430740237236023]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 438\n",
            "  Loss for client 0: 1.1843\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 438\n",
            "  Loss for client 1: 0.9052\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 438\n",
            "  Loss for client 2: 1.7328\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 439\n",
            "  Client 0: total grad norm = 73.974130 (253 params)\n",
            "  Client 1: total grad norm = 59.895298 (253 params)\n",
            "  Client 2: total grad norm = 82.663085 (253 params)\n",
            "\n",
            "=== Optimizer Step 439 ===\n",
            "\n",
            "=== Updating Client Weights (Step 439) ===\n",
            "Gradient norms: [7.892735004425049, 6.732828140258789, 8.799449920654297]\n",
            "Target weights: [0.3258141577243805, 0.3819442391395569, 0.29224154353141785]\n",
            "Updated weights: [0.342750608921051, 0.3294250965118408, 0.32782426476478577]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 439\n",
            "  Loss for client 0: 1.5888\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 439\n",
            "  Loss for client 1: 1.6585\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 439\n",
            "  Loss for client 2: 1.7467\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 440\n",
            "  Client 0: total grad norm = 65.915594 (253 params)\n",
            "  Client 1: total grad norm = 94.076344 (253 params)\n",
            "  Client 2: total grad norm = 76.157390 (253 params)\n",
            "\n",
            "=== Optimizer Step 440 ===\n",
            "\n",
            "=== Updating Client Weights (Step 440) ===\n",
            "Gradient norms: [7.537168025970459, 11.160024642944336, 8.588679313659668]\n",
            "Target weights: [0.3917049169540405, 0.26454654335975647, 0.343748539686203]\n",
            "Updated weights: [0.3574368953704834, 0.3099615275859833, 0.33260154724121094]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 440\n",
            "  Loss for client 0: 1.7399\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 440\n",
            "  Loss for client 1: 1.2240\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 440\n",
            "  Loss for client 2: 2.2434\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 441\n",
            "  Client 0: total grad norm = 86.034376 (253 params)\n",
            "  Client 1: total grad norm = 55.234591 (253 params)\n",
            "  Client 2: total grad norm = 86.469394 (253 params)\n",
            "\n",
            "=== Optimizer Step 441 ===\n",
            "\n",
            "=== Updating Client Weights (Step 441) ===\n",
            "Gradient norms: [8.819415092468262, 6.421752452850342, 10.25383472442627]\n",
            "Target weights: [0.30926480889320374, 0.4247337281703949, 0.266001433134079]\n",
            "Updated weights: [0.3429853022098541, 0.34439322352409363, 0.312621533870697]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 441\n",
            "  Loss for client 0: 1.3791\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 441\n",
            "  Loss for client 1: 2.1274\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 441\n",
            "  Loss for client 2: 1.7938\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 442\n",
            "  Client 0: total grad norm = 67.333884 (253 params)\n",
            "  Client 1: total grad norm = 77.813607 (253 params)\n",
            "  Client 2: total grad norm = 95.489435 (253 params)\n",
            "\n",
            "=== Optimizer Step 442 ===\n",
            "\n",
            "=== Updating Client Weights (Step 442) ===\n",
            "Gradient norms: [7.423514366149902, 7.989657402038574, 10.924646377563477]\n",
            "Target weights: [0.3833385407924652, 0.35617536306381226, 0.2604861557483673]\n",
            "Updated weights: [0.35509127378463745, 0.34792786836624146, 0.29698091745376587]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 442\n",
            "  Loss for client 0: 1.7028\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 442\n",
            "  Loss for client 1: 2.4638\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 442\n",
            "  Loss for client 2: 1.3965\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 443\n",
            "  Client 0: total grad norm = 72.120042 (253 params)\n",
            "  Client 1: total grad norm = 104.902460 (253 params)\n",
            "  Client 2: total grad norm = 70.422972 (253 params)\n",
            "\n",
            "=== Optimizer Step 443 ===\n",
            "\n",
            "=== Updating Client Weights (Step 443) ===\n",
            "Gradient norms: [8.41586685180664, 10.232102394104004, 7.015661716461182]\n",
            "Target weights: [0.3308980166912079, 0.27216240763664246, 0.39693957567214966]\n",
            "Updated weights: [0.3478333055973053, 0.325198233127594, 0.3269685208797455]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 443\n",
            "  Loss for client 0: 1.6278\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 443\n",
            "  Loss for client 1: 1.9291\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 443\n",
            "  Loss for client 2: 1.5810\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 444\n",
            "  Client 0: total grad norm = 77.777498 (253 params)\n",
            "  Client 1: total grad norm = 89.707168 (253 params)\n",
            "  Client 2: total grad norm = 84.206982 (253 params)\n",
            "\n",
            "=== Optimizer Step 444 ===\n",
            "\n",
            "=== Updating Client Weights (Step 444) ===\n",
            "Gradient norms: [7.344601154327393, 8.373700141906738, 8.261801719665527]\n",
            "Target weights: [0.3615216314792633, 0.31709185242652893, 0.32138657569885254]\n",
            "Updated weights: [0.3519397974014282, 0.32276633381843567, 0.3252939283847809]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 444\n",
            "  Loss for client 0: 0.9425\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 444\n",
            "  Loss for client 1: 1.9072\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 444\n",
            "  Loss for client 2: 1.5437\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 445\n",
            "  Client 0: total grad norm = 65.196660 (253 params)\n",
            "  Client 1: total grad norm = 78.259657 (253 params)\n",
            "  Client 2: total grad norm = 71.024908 (253 params)\n",
            "\n",
            "=== Optimizer Step 445 ===\n",
            "\n",
            "=== Updating Client Weights (Step 445) ===\n",
            "Gradient norms: [8.059808731079102, 7.9776482582092285, 7.8234405517578125]\n",
            "Target weights: [0.3288920223712921, 0.332279235124588, 0.33882877230644226]\n",
            "Updated weights: [0.3450254797935486, 0.32562020421028137, 0.3293543756008148]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 445\n",
            "  Loss for client 0: 1.7784\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 445\n",
            "  Loss for client 1: 1.2491\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 445\n",
            "  Loss for client 2: 1.7247\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 446\n",
            "  Client 0: total grad norm = 84.079523 (253 params)\n",
            "  Client 1: total grad norm = 65.661627 (253 params)\n",
            "  Client 2: total grad norm = 82.747577 (253 params)\n",
            "\n",
            "=== Optimizer Step 446 ===\n",
            "\n",
            "=== Updating Client Weights (Step 446) ===\n",
            "Gradient norms: [8.668237686157227, 7.132969856262207, 9.281122207641602]\n",
            "Target weights: [0.3175409734249115, 0.3858870267868042, 0.2965719401836395]\n",
            "Updated weights: [0.3367801308631897, 0.34370025992393494, 0.31951963901519775]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 446\n",
            "  Loss for client 0: 1.3078\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 446\n",
            "  Loss for client 1: 1.7473\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 446\n",
            "  Loss for client 2: 1.4515\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 447\n",
            "  Client 0: total grad norm = 89.956861 (253 params)\n",
            "  Client 1: total grad norm = 90.606210 (253 params)\n",
            "  Client 2: total grad norm = 74.341439 (253 params)\n",
            "\n",
            "=== Optimizer Step 447 ===\n",
            "\n",
            "=== Updating Client Weights (Step 447) ===\n",
            "Gradient norms: [9.185938835144043, 9.06227970123291, 7.612417697906494]\n",
            "Target weights: [0.31052523851394653, 0.31476250290870667, 0.3747122287750244]\n",
            "Updated weights: [0.3289036750793457, 0.33501893281936646, 0.33607742190361023]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 447\n",
            "  Loss for client 0: 1.9538\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 447\n",
            "  Loss for client 1: 1.8834\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 447\n",
            "  Loss for client 2: 1.6913\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 448\n",
            "  Client 0: total grad norm = 88.756125 (253 params)\n",
            "  Client 1: total grad norm = 86.436790 (253 params)\n",
            "  Client 2: total grad norm = 76.199896 (253 params)\n",
            "\n",
            "=== Optimizer Step 448 ===\n",
            "\n",
            "=== Updating Client Weights (Step 448) ===\n",
            "Gradient norms: [8.4076509475708, 8.719328880310059, 6.834221363067627]\n",
            "Target weights: [0.31303977966308594, 0.30184996128082275, 0.3851102888584137]\n",
            "Updated weights: [0.32414451241493225, 0.32506823539733887, 0.35078728199005127]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 448\n",
            "  Loss for client 0: 1.8711\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 448\n",
            "  Loss for client 1: 2.1029\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 448\n",
            "  Loss for client 2: 1.7852\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 449\n",
            "  Client 0: total grad norm = 81.467365 (253 params)\n",
            "  Client 1: total grad norm = 90.263870 (253 params)\n",
            "  Client 2: total grad norm = 73.924328 (253 params)\n",
            "\n",
            "=== Optimizer Step 449 ===\n",
            "\n",
            "=== Updating Client Weights (Step 449) ===\n",
            "Gradient norms: [8.014683723449707, 9.253854751586914, 7.901461601257324]\n",
            "Target weights: [0.34717151522636414, 0.3006822466850281, 0.3521462678909302]\n",
            "Updated weights: [0.33105260133743286, 0.3177524507045746, 0.35119497776031494]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 449\n",
            "  Loss for client 0: 1.6431\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 449\n",
            "  Loss for client 1: 1.3275\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 449\n",
            "  Loss for client 2: 1.9375\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 450\n",
            "  Client 0: total grad norm = 76.568090 (253 params)\n",
            "  Client 1: total grad norm = 64.306318 (253 params)\n",
            "  Client 2: total grad norm = 76.102488 (253 params)\n",
            "\n",
            "=== Optimizer Step 450 ===\n",
            "\n",
            "=== Updating Client Weights (Step 450) ===\n",
            "Gradient norms: [8.157934188842773, 6.618685722351074, 7.763638019561768]\n",
            "Target weights: [0.3045671284198761, 0.37539759278297424, 0.32003533840179443]\n",
            "Updated weights: [0.323106974363327, 0.3350459933280945, 0.34184709191322327]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 450\n",
            "  Val Loss = 1.8004 (5 batches)\n",
            "\n",
            "[Processing] Client 0, batch 450\n",
            "  Loss for client 0: 1.5627\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 450\n",
            "  Loss for client 1: 1.2309\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 450\n",
            "  Loss for client 2: 1.1795\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 451\n",
            "  Client 0: total grad norm = 86.034865 (253 params)\n",
            "  Client 1: total grad norm = 64.035804 (253 params)\n",
            "  Client 2: total grad norm = 60.748125 (253 params)\n",
            "\n",
            "=== Optimizer Step 451 ===\n",
            "\n",
            "=== Updating Client Weights (Step 451) ===\n",
            "Gradient norms: [9.246970176696777, 6.960562229156494, 7.2157511711120605]\n",
            "Target weights: [0.2770099639892578, 0.3680022656917572, 0.3549877107143402]\n",
            "Updated weights: [0.30927786231040955, 0.34493288397789, 0.3457892835140228]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 451\n",
            "  Loss for client 0: 1.7001\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 451\n",
            "  Loss for client 1: 1.5534\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 451\n",
            "  Loss for client 2: 1.4511\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 452\n",
            "  Client 0: total grad norm = 76.147755 (253 params)\n",
            "  Client 1: total grad norm = 95.546345 (253 params)\n",
            "  Client 2: total grad norm = 72.590805 (253 params)\n",
            "\n",
            "=== Optimizer Step 452 ===\n",
            "\n",
            "=== Updating Client Weights (Step 452) ===\n",
            "Gradient norms: [8.389548301696777, 10.898395538330078, 8.263526916503906]\n",
            "Target weights: [0.3590603470802307, 0.2764034569263458, 0.3645361363887787]\n",
            "Updated weights: [0.32421261072158813, 0.32437407970428467, 0.3514133393764496]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 452\n",
            "  Loss for client 0: 0.8777\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 452\n",
            "  Loss for client 1: 1.5002\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 452\n",
            "  Loss for client 2: 1.1667\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 453\n",
            "  Client 0: total grad norm = 61.784999 (253 params)\n",
            "  Client 1: total grad norm = 76.765261 (253 params)\n",
            "  Client 2: total grad norm = 70.012682 (253 params)\n",
            "\n",
            "=== Optimizer Step 453 ===\n",
            "\n",
            "=== Updating Client Weights (Step 453) ===\n",
            "Gradient norms: [5.990843772888184, 8.233200073242188, 8.39208698272705]\n",
            "Target weights: [0.40958213806152344, 0.29803022742271423, 0.29238763451576233]\n",
            "Updated weights: [0.3498234748840332, 0.3164709210395813, 0.3337056338787079]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 453\n",
            "  Loss for client 0: 2.0673\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 453\n",
            "  Loss for client 1: 2.2006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 453\n",
            "  Loss for client 2: 1.6584\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 454\n",
            "  Client 0: total grad norm = 85.226364 (253 params)\n",
            "  Client 1: total grad norm = 110.394224 (253 params)\n",
            "  Client 2: total grad norm = 78.360175 (253 params)\n",
            "\n",
            "=== Optimizer Step 454 ===\n",
            "\n",
            "=== Updating Client Weights (Step 454) ===\n",
            "Gradient norms: [8.96574592590332, 9.10987377166748, 9.912845611572266]\n",
            "Target weights: [0.346184104681015, 0.34070709347724915, 0.31310874223709106]\n",
            "Updated weights: [0.34873166680336, 0.3237417936325073, 0.3275265693664551]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 454\n",
            "  Loss for client 0: 1.4787\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 454\n",
            "  Loss for client 1: 2.1895\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 454\n",
            "  Loss for client 2: 1.4917\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 455\n",
            "  Client 0: total grad norm = 67.605205 (253 params)\n",
            "  Client 1: total grad norm = 91.130817 (253 params)\n",
            "  Client 2: total grad norm = 70.386557 (253 params)\n",
            "\n",
            "=== Optimizer Step 455 ===\n",
            "\n",
            "=== Updating Client Weights (Step 455) ===\n",
            "Gradient norms: [6.505609035491943, 8.834803581237793, 7.213271141052246]\n",
            "Target weights: [0.3790382742881775, 0.2791091799736023, 0.3418525159358978]\n",
            "Updated weights: [0.3578236401081085, 0.31035202741622925, 0.3318243622779846]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 455\n",
            "  Loss for client 0: 2.2957\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 455\n",
            "  Loss for client 1: 1.8935\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 455\n",
            "  Loss for client 2: 1.6766\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 456\n",
            "  Client 0: total grad norm = 101.008696 (253 params)\n",
            "  Client 1: total grad norm = 85.131169 (253 params)\n",
            "  Client 2: total grad norm = 90.638867 (253 params)\n",
            "\n",
            "=== Optimizer Step 456 ===\n",
            "\n",
            "=== Updating Client Weights (Step 456) ===\n",
            "Gradient norms: [10.04773998260498, 8.92922306060791, 8.486498832702637]\n",
            "Target weights: [0.3021848797798157, 0.340038001537323, 0.35777711868286133]\n",
            "Updated weights: [0.3411320149898529, 0.31925782561302185, 0.33961018919944763]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 456\n",
            "  Loss for client 0: 1.7467\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 456\n",
            "  Loss for client 1: 1.6126\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 456\n",
            "  Loss for client 2: 2.3431\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 457\n",
            "  Client 0: total grad norm = 77.802399 (253 params)\n",
            "  Client 1: total grad norm = 63.951565 (253 params)\n",
            "  Client 2: total grad norm = 91.256155 (253 params)\n",
            "\n",
            "=== Optimizer Step 457 ===\n",
            "\n",
            "=== Updating Client Weights (Step 457) ===\n",
            "Gradient norms: [9.930252075195312, 7.249238014221191, 10.069897651672363]\n",
            "Target weights: [0.2979767620563507, 0.4081786870956421, 0.2938445508480072]\n",
            "Updated weights: [0.32818543910980225, 0.34593409299850464, 0.3258804976940155]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 457\n",
            "  Loss for client 0: 1.6467\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 457\n",
            "  Loss for client 1: 1.5778\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 457\n",
            "  Loss for client 2: 1.9594\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 458\n",
            "  Client 0: total grad norm = 81.614809 (253 params)\n",
            "  Client 1: total grad norm = 76.139773 (253 params)\n",
            "  Client 2: total grad norm = 76.689851 (253 params)\n",
            "\n",
            "=== Optimizer Step 458 ===\n",
            "\n",
            "=== Updating Client Weights (Step 458) ===\n",
            "Gradient norms: [8.767824172973633, 7.849388122558594, 8.043776512145996]\n",
            "Target weights: [0.31181588768959045, 0.34830060601234436, 0.3398835062980652]\n",
            "Updated weights: [0.32327455282211304, 0.3466440439224243, 0.33008140325546265]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 458\n",
            "  Loss for client 0: 1.8081\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 458\n",
            "  Loss for client 1: 1.7819\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 458\n",
            "  Loss for client 2: 1.5107\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 459\n",
            "  Client 0: total grad norm = 66.278958 (253 params)\n",
            "  Client 1: total grad norm = 77.181403 (253 params)\n",
            "  Client 2: total grad norm = 63.215610 (253 params)\n",
            "\n",
            "=== Optimizer Step 459 ===\n",
            "\n",
            "=== Updating Client Weights (Step 459) ===\n",
            "Gradient norms: [7.017340183258057, 8.037182807922363, 6.76066780090332]\n",
            "Target weights: [0.3435157239437103, 0.2999268174171448, 0.3565574586391449]\n",
            "Updated weights: [0.3293468952178955, 0.33262887597084045, 0.33802419900894165]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 459\n",
            "  Loss for client 0: 1.2066\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 459\n",
            "  Loss for client 1: 1.6846\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 459\n",
            "  Loss for client 2: 1.6506\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 460\n",
            "  Client 0: total grad norm = 78.830382 (253 params)\n",
            "  Client 1: total grad norm = 87.366433 (253 params)\n",
            "  Client 2: total grad norm = 71.275217 (253 params)\n",
            "\n",
            "=== Optimizer Step 460 ===\n",
            "\n",
            "=== Updating Client Weights (Step 460) ===\n",
            "Gradient norms: [8.171878814697266, 7.594559192657471, 8.064784049987793]\n",
            "Target weights: [0.3236982524394989, 0.34830501675605774, 0.32799676060676575]\n",
            "Updated weights: [0.32765230536460876, 0.33733171224594116, 0.3350159823894501]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 460\n",
            "  Loss for client 0: 1.3260\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 460\n",
            "  Loss for client 1: 1.6879\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 460\n",
            "  Loss for client 2: 1.7301\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 461\n",
            "  Client 0: total grad norm = 65.150830 (253 params)\n",
            "  Client 1: total grad norm = 71.756869 (253 params)\n",
            "  Client 2: total grad norm = 83.762742 (253 params)\n",
            "\n",
            "=== Optimizer Step 461 ===\n",
            "\n",
            "=== Updating Client Weights (Step 461) ===\n",
            "Gradient norms: [7.171807765960693, 8.515029907226562, 10.093339920043945]\n",
            "Target weights: [0.3917265236377716, 0.3299327492713928, 0.27834072709083557]\n",
            "Updated weights: [0.3468745946884155, 0.3351120352745056, 0.31801339983940125]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 461\n",
            "  Loss for client 0: 1.1874\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 461\n",
            "  Loss for client 1: 1.5293\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 461\n",
            "  Loss for client 2: 1.8767\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 462\n",
            "  Client 0: total grad norm = 80.369973 (253 params)\n",
            "  Client 1: total grad norm = 84.065234 (253 params)\n",
            "  Client 2: total grad norm = 96.738160 (253 params)\n",
            "\n",
            "=== Optimizer Step 462 ===\n",
            "\n",
            "=== Updating Client Weights (Step 462) ===\n",
            "Gradient norms: [8.663195610046387, 10.835387229919434, 8.985363006591797]\n",
            "Target weights: [0.3618372678756714, 0.28929901123046875, 0.3488636910915375]\n",
            "Updated weights: [0.3513633906841278, 0.32136812806129456, 0.32726848125457764]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 462\n",
            "  Loss for client 0: 1.8528\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 462\n",
            "  Loss for client 1: 1.7546\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 462\n",
            "  Loss for client 2: 1.7144\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 463\n",
            "  Client 0: total grad norm = 103.807757 (253 params)\n",
            "  Client 1: total grad norm = 82.619485 (253 params)\n",
            "  Client 2: total grad norm = 80.847403 (253 params)\n",
            "\n",
            "=== Optimizer Step 463 ===\n",
            "\n",
            "=== Updating Client Weights (Step 463) ===\n",
            "Gradient norms: [11.138769149780273, 9.119153022766113, 9.056696891784668]\n",
            "Target weights: [0.28974059224128723, 0.35390937328338623, 0.35634997487068176]\n",
            "Updated weights: [0.3328765630722046, 0.3311305046081543, 0.3359929323196411]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 463\n",
            "  Loss for client 0: 1.1608\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 463\n",
            "  Loss for client 1: 2.3383\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 463\n",
            "  Loss for client 2: 1.4706\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 464\n",
            "  Client 0: total grad norm = 57.057421 (253 params)\n",
            "  Client 1: total grad norm = 90.556094 (253 params)\n",
            "  Client 2: total grad norm = 67.423425 (253 params)\n",
            "\n",
            "=== Optimizer Step 464 ===\n",
            "\n",
            "=== Updating Client Weights (Step 464) ===\n",
            "Gradient norms: [6.761725902557373, 8.924789428710938, 6.889983654022217]\n",
            "Target weights: [0.36509421467781067, 0.276607871055603, 0.3582979142665863]\n",
            "Updated weights: [0.3425418436527252, 0.31477370858192444, 0.34268444776535034]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 464\n",
            "  Loss for client 0: 2.2275\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 464\n",
            "  Loss for client 1: 1.6362\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 464\n",
            "  Loss for client 2: 1.8732\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 465\n",
            "  Client 0: total grad norm = 72.165395 (253 params)\n",
            "  Client 1: total grad norm = 67.025571 (253 params)\n",
            "  Client 2: total grad norm = 93.805733 (253 params)\n",
            "\n",
            "=== Optimizer Step 465 ===\n",
            "\n",
            "=== Updating Client Weights (Step 465) ===\n",
            "Gradient norms: [8.655220031738281, 7.611435413360596, 8.200568199157715]\n",
            "Target weights: [0.31322675943374634, 0.35618069767951965, 0.3305925130844116]\n",
            "Updated weights: [0.33374732732772827, 0.32719582319259644, 0.3390568792819977]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 465\n",
            "  Loss for client 0: 1.5843\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 465\n",
            "  Loss for client 1: 1.9290\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 465\n",
            "  Loss for client 2: 1.9517\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 466\n",
            "  Client 0: total grad norm = 68.802795 (253 params)\n",
            "  Client 1: total grad norm = 83.322845 (253 params)\n",
            "  Client 2: total grad norm = 83.512293 (253 params)\n",
            "\n",
            "=== Optimizer Step 466 ===\n",
            "\n",
            "=== Updating Client Weights (Step 466) ===\n",
            "Gradient norms: [7.873809814453125, 9.90247631072998, 9.40150260925293]\n",
            "Target weights: [0.3798466920852661, 0.302029550075531, 0.3181236982345581]\n",
            "Updated weights: [0.34757715463638306, 0.3196459412574768, 0.3327769339084625]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 466\n",
            "  Loss for client 0: 1.0494\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 466\n",
            "  Loss for client 1: 2.2683\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 466\n",
            "  Loss for client 2: 1.4794\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 467\n",
            "  Client 0: total grad norm = 63.447044 (253 params)\n",
            "  Client 1: total grad norm = 80.930632 (253 params)\n",
            "  Client 2: total grad norm = 74.734963 (253 params)\n",
            "\n",
            "=== Optimizer Step 467 ===\n",
            "\n",
            "=== Updating Client Weights (Step 467) ===\n",
            "Gradient norms: [6.7660322189331055, 9.261950492858887, 7.475425720214844]\n",
            "Target weights: [0.3794170022010803, 0.27717140316963196, 0.34341156482696533]\n",
            "Updated weights: [0.3571290969848633, 0.30690357089042664, 0.3359673321247101]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 467\n",
            "  Loss for client 0: 1.1322\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 467\n",
            "  Loss for client 1: 2.0161\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 467\n",
            "  Loss for client 2: 1.9431\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 468\n",
            "  Client 0: total grad norm = 60.291184 (253 params)\n",
            "  Client 1: total grad norm = 91.356643 (253 params)\n",
            "  Client 2: total grad norm = 76.041455 (253 params)\n",
            "\n",
            "=== Optimizer Step 468 ===\n",
            "\n",
            "=== Updating Client Weights (Step 468) ===\n",
            "Gradient norms: [6.051817893981934, 9.345396041870117, 8.086857795715332]\n",
            "Target weights: [0.41737544536590576, 0.2702806890010834, 0.3123438358306885]\n",
            "Updated weights: [0.375203013420105, 0.29591670632362366, 0.32888028025627136]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 468\n",
            "  Loss for client 0: 1.8054\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 468\n",
            "  Loss for client 1: 1.8202\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 468\n",
            "  Loss for client 2: 1.7224\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 469\n",
            "  Client 0: total grad norm = 71.670756 (253 params)\n",
            "  Client 1: total grad norm = 86.634219 (253 params)\n",
            "  Client 2: total grad norm = 72.974026 (253 params)\n",
            "\n",
            "=== Optimizer Step 469 ===\n",
            "\n",
            "=== Updating Client Weights (Step 469) ===\n",
            "Gradient norms: [8.953871726989746, 9.462119102478027, 7.745048999786377]\n",
            "Target weights: [0.3223349452018738, 0.3050210773944855, 0.37264397740364075]\n",
            "Updated weights: [0.3593426048755646, 0.29864802956581116, 0.34200939536094666]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 469\n",
            "  Loss for client 0: 1.2556\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 469\n",
            "  Loss for client 1: 1.8407\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 469\n",
            "  Loss for client 2: 1.9162\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 470\n",
            "  Client 0: total grad norm = 91.238359 (253 params)\n",
            "  Client 1: total grad norm = 85.487768 (253 params)\n",
            "  Client 2: total grad norm = 80.740163 (253 params)\n",
            "\n",
            "=== Optimizer Step 470 ===\n",
            "\n",
            "=== Updating Client Weights (Step 470) ===\n",
            "Gradient norms: [9.768482208251953, 9.927088737487793, 9.402987480163574]\n",
            "Target weights: [0.33080893754959106, 0.3255235552787781, 0.34366750717163086]\n",
            "Updated weights: [0.35078251361846924, 0.30671069025993347, 0.3425068259239197]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 470\n",
            "  Loss for client 0: 1.4423\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 470\n",
            "  Loss for client 1: 2.2432\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 470\n",
            "  Loss for client 2: 1.7245\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 471\n",
            "  Client 0: total grad norm = 74.067221 (253 params)\n",
            "  Client 1: total grad norm = 90.064172 (253 params)\n",
            "  Client 2: total grad norm = 76.267569 (253 params)\n",
            "\n",
            "=== Optimizer Step 471 ===\n",
            "\n",
            "=== Updating Client Weights (Step 471) ===\n",
            "Gradient norms: [9.148717880249023, 10.87106990814209, 8.046183586120605]\n",
            "Target weights: [0.33572918176651, 0.28253811597824097, 0.381732702255249]\n",
            "Updated weights: [0.346266508102417, 0.29945892095565796, 0.35427460074424744]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 471\n",
            "  Loss for client 0: 1.1539\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 471\n",
            "  Loss for client 1: 1.7602\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 471\n",
            "  Loss for client 2: 1.6185\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 472\n",
            "  Client 0: total grad norm = 87.151547 (253 params)\n",
            "  Client 1: total grad norm = 159.768544 (253 params)\n",
            "  Client 2: total grad norm = 71.948152 (253 params)\n",
            "\n",
            "=== Optimizer Step 472 ===\n",
            "\n",
            "=== Updating Client Weights (Step 472) ===\n",
            "Gradient norms: [8.410177230834961, 12.723304748535156, 7.409748077392578]\n",
            "Target weights: [0.35765108466148376, 0.23640941083431244, 0.4059394598007202]\n",
            "Updated weights: [0.34968188405036926, 0.28054407238960266, 0.3697740435600281]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 472\n",
            "  Loss for client 0: 1.6170\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 472\n",
            "  Loss for client 1: 2.2143\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 472\n",
            "  Loss for client 2: 0.9941\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 473\n",
            "  Client 0: total grad norm = 73.288277 (253 params)\n",
            "  Client 1: total grad norm = 77.798350 (253 params)\n",
            "  Client 2: total grad norm = 81.281906 (253 params)\n",
            "\n",
            "=== Optimizer Step 473 ===\n",
            "\n",
            "=== Updating Client Weights (Step 473) ===\n",
            "Gradient norms: [7.569652557373047, 7.842589378356934, 8.423784255981445]\n",
            "Target weights: [0.3491860330104828, 0.3370337188243866, 0.3137802481651306]\n",
            "Updated weights: [0.3495331406593323, 0.2974909842014313, 0.35297590494155884]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 473\n",
            "  Loss for client 0: 1.4871\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 473\n",
            "  Loss for client 1: 0.9998\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 473\n",
            "  Loss for client 2: 1.9103\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 474\n",
            "  Client 0: total grad norm = 85.234544 (253 params)\n",
            "  Client 1: total grad norm = 71.334465 (253 params)\n",
            "  Client 2: total grad norm = 79.195375 (253 params)\n",
            "\n",
            "=== Optimizer Step 474 ===\n",
            "\n",
            "=== Updating Client Weights (Step 474) ===\n",
            "Gradient norms: [8.835908889770508, 7.448902606964111, 10.124556541442871]\n",
            "Target weights: [0.32691240310668945, 0.3877843916416168, 0.2853032052516937]\n",
            "Updated weights: [0.34274691343307495, 0.32457900047302246, 0.3326740860939026]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 474\n",
            "  Loss for client 0: 0.8974\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 474\n",
            "  Loss for client 1: 1.7774\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 474\n",
            "  Loss for client 2: 1.4218\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 475\n",
            "  Client 0: total grad norm = 63.559245 (253 params)\n",
            "  Client 1: total grad norm = 77.954111 (253 params)\n",
            "  Client 2: total grad norm = 65.261505 (253 params)\n",
            "\n",
            "=== Optimizer Step 475 ===\n",
            "\n",
            "=== Updating Client Weights (Step 475) ===\n",
            "Gradient norms: [6.302084445953369, 7.847131729125977, 7.327715873718262]\n",
            "Target weights: [0.375496506690979, 0.3015637993812561, 0.32293975353240967]\n",
            "Updated weights: [0.3525717854499817, 0.3176744282245636, 0.3297537863254547]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 475\n",
            "  Loss for client 0: 0.8474\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 475\n",
            "  Loss for client 1: 1.2414\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 475\n",
            "  Loss for client 2: 1.8540\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 476\n",
            "  Client 0: total grad norm = 79.943573 (253 params)\n",
            "  Client 1: total grad norm = 69.209978 (253 params)\n",
            "  Client 2: total grad norm = 74.396277 (253 params)\n",
            "\n",
            "=== Optimizer Step 476 ===\n",
            "\n",
            "=== Updating Client Weights (Step 476) ===\n",
            "Gradient norms: [10.236137390136719, 6.849165439605713, 7.383552074432373]\n",
            "Target weights: [0.2576753795146942, 0.38509809970855713, 0.35722652077674866]\n",
            "Updated weights: [0.32410287857055664, 0.3379015326499939, 0.33799558877944946]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 476\n",
            "  Loss for client 0: 1.8586\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 476\n",
            "  Loss for client 1: 1.5716\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 476\n",
            "  Loss for client 2: 1.1835\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 477\n",
            "  Client 0: total grad norm = 99.870952 (253 params)\n",
            "  Client 1: total grad norm = 68.849470 (253 params)\n",
            "  Client 2: total grad norm = 62.912395 (253 params)\n",
            "\n",
            "=== Optimizer Step 477 ===\n",
            "\n",
            "=== Updating Client Weights (Step 477) ===\n",
            "Gradient norms: [11.877551078796387, 7.802213191986084, 6.609567642211914]\n",
            "Target weights: [0.2315160483121872, 0.352444052696228, 0.4160398840904236]\n",
            "Updated weights: [0.29632681608200073, 0.3422642946243286, 0.36140888929367065]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 477\n",
            "  Loss for client 0: 1.8246\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 477\n",
            "  Loss for client 1: 1.4623\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 477\n",
            "  Loss for client 2: 1.4043\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 478\n",
            "  Client 0: total grad norm = 82.200006 (253 params)\n",
            "  Client 1: total grad norm = 61.448009 (253 params)\n",
            "  Client 2: total grad norm = 70.527220 (253 params)\n",
            "\n",
            "=== Optimizer Step 478 ===\n",
            "\n",
            "=== Updating Client Weights (Step 478) ===\n",
            "Gradient norms: [9.20604133605957, 7.07503080368042, 8.056164741516113]\n",
            "Target weights: [0.290365606546402, 0.37782418727874756, 0.33181020617485046]\n",
            "Updated weights: [0.2945384383201599, 0.35293227434158325, 0.35252928733825684]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 478\n",
            "  Loss for client 0: 2.0457\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 478\n",
            "  Loss for client 1: 1.3277\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 478\n",
            "  Loss for client 2: 2.2882\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 479\n",
            "  Client 0: total grad norm = 93.911265 (253 params)\n",
            "  Client 1: total grad norm = 62.315852 (253 params)\n",
            "  Client 2: total grad norm = 107.744318 (253 params)\n",
            "\n",
            "=== Optimizer Step 479 ===\n",
            "\n",
            "=== Updating Client Weights (Step 479) ===\n",
            "Gradient norms: [8.672511100769043, 6.4973320960998535, 9.749224662780762]\n",
            "Target weights: [0.31014102697372437, 0.4139701724052429, 0.2758888006210327]\n",
            "Updated weights: [0.2992192208766937, 0.3712436556816101, 0.32953715324401855]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 479\n",
            "  Loss for client 0: 2.0485\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 479\n",
            "  Loss for client 1: 1.3645\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 479\n",
            "  Loss for client 2: 1.6186\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 480\n",
            "  Client 0: total grad norm = 91.207193 (253 params)\n",
            "  Client 1: total grad norm = 79.975007 (253 params)\n",
            "  Client 2: total grad norm = 72.089468 (253 params)\n",
            "\n",
            "=== Optimizer Step 480 ===\n",
            "\n",
            "=== Updating Client Weights (Step 480) ===\n",
            "Gradient norms: [9.3170804977417, 7.067688941955566, 8.24191951751709]\n",
            "Target weights: [0.28996312618255615, 0.3822479844093323, 0.32778891921043396]\n",
            "Updated weights: [0.2964423894882202, 0.3745449483394623, 0.3290126919746399]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 480\n",
            "  Loss for client 0: 1.8506\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 480\n",
            "  Loss for client 1: 2.1697\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 480\n",
            "  Loss for client 2: 1.5992\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 481\n",
            "  Client 0: total grad norm = 86.459042 (253 params)\n",
            "  Client 1: total grad norm = 84.989967 (253 params)\n",
            "  Client 2: total grad norm = 73.585733 (253 params)\n",
            "\n",
            "=== Optimizer Step 481 ===\n",
            "\n",
            "=== Updating Client Weights (Step 481) ===\n",
            "Gradient norms: [10.212629318237305, 8.436424255371094, 8.269552230834961]\n",
            "Target weights: [0.29023298621177673, 0.3513386845588684, 0.35842835903167725]\n",
            "Updated weights: [0.29457956552505493, 0.3675830662250519, 0.3378373980522156]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 481\n",
            "  Loss for client 0: 0.9654\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 481\n",
            "  Loss for client 1: 1.5770\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 481\n",
            "  Loss for client 2: 1.5827\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 482\n",
            "  Client 0: total grad norm = 57.567856 (253 params)\n",
            "  Client 1: total grad norm = 78.547081 (253 params)\n",
            "  Client 2: total grad norm = 66.994096 (253 params)\n",
            "\n",
            "=== Optimizer Step 482 ===\n",
            "\n",
            "=== Updating Client Weights (Step 482) ===\n",
            "Gradient norms: [6.077473163604736, 8.584402084350586, 8.461811065673828]\n",
            "Target weights: [0.4121687412261963, 0.2918018400669098, 0.29602935910224915]\n",
            "Updated weights: [0.32985633611679077, 0.3448486924171448, 0.32529500126838684]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 482\n",
            "  Loss for client 0: 1.8979\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 482\n",
            "  Loss for client 1: 1.5390\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 482\n",
            "  Loss for client 2: 1.7265\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 483\n",
            "  Client 0: total grad norm = 86.789502 (253 params)\n",
            "  Client 1: total grad norm = 110.289791 (253 params)\n",
            "  Client 2: total grad norm = 84.370690 (253 params)\n",
            "\n",
            "=== Optimizer Step 483 ===\n",
            "\n",
            "=== Updating Client Weights (Step 483) ===\n",
            "Gradient norms: [9.428770065307617, 9.1787691116333, 8.046586036682129]\n",
            "Target weights: [0.31259676814079285, 0.32111090421676636, 0.3662923574447632]\n",
            "Updated weights: [0.3246784806251526, 0.3377273678779602, 0.337594211101532]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 483\n",
            "  Loss for client 0: 1.1548\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 483\n",
            "  Loss for client 1: 2.3186\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 483\n",
            "  Loss for client 2: 1.9859\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 484\n",
            "  Client 0: total grad norm = 98.672877 (253 params)\n",
            "  Client 1: total grad norm = 107.501130 (253 params)\n",
            "  Client 2: total grad norm = 91.012878 (253 params)\n",
            "\n",
            "=== Optimizer Step 484 ===\n",
            "\n",
            "=== Updating Client Weights (Step 484) ===\n",
            "Gradient norms: [11.85045051574707, 10.276010513305664, 9.590922355651855]\n",
            "Target weights: [0.2950892448425293, 0.34030139446258545, 0.36460942029953003]\n",
            "Updated weights: [0.3158017098903656, 0.3384995758533478, 0.3456987738609314]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 484\n",
            "  Loss for client 0: 1.7990\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 484\n",
            "  Loss for client 1: 1.6010\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 484\n",
            "  Loss for client 2: 1.5597\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 485\n",
            "  Client 0: total grad norm = 80.074698 (253 params)\n",
            "  Client 1: total grad norm = 67.497922 (253 params)\n",
            "  Client 2: total grad norm = 85.191420 (253 params)\n",
            "\n",
            "=== Optimizer Step 485 ===\n",
            "\n",
            "=== Updating Client Weights (Step 485) ===\n",
            "Gradient norms: [9.44823169708252, 8.014744758605957, 8.599685668945312]\n",
            "Target weights: [0.305107980966568, 0.3596784472465515, 0.3352135121822357]\n",
            "Updated weights: [0.31259357929229736, 0.3448532223701477, 0.34255319833755493]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 485\n",
            "  Loss for client 0: 0.8610\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 485\n",
            "  Loss for client 1: 1.6050\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 485\n",
            "  Loss for client 2: 1.4585\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 486\n",
            "  Client 0: total grad norm = 68.280794 (253 params)\n",
            "  Client 1: total grad norm = 71.930810 (253 params)\n",
            "  Client 2: total grad norm = 69.450807 (253 params)\n",
            "\n",
            "=== Optimizer Step 486 ===\n",
            "\n",
            "=== Updating Client Weights (Step 486) ===\n",
            "Gradient norms: [7.252668380737305, 8.49122142791748, 7.37689733505249]\n",
            "Target weights: [0.35244810581207275, 0.30103906989097595, 0.3465127944946289]\n",
            "Updated weights: [0.32454994320869446, 0.33170896768569946, 0.3437410891056061]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 486\n",
            "  Loss for client 0: 1.1870\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 486\n",
            "  Loss for client 1: 1.9662\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 486\n",
            "  Loss for client 2: 2.2553\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 487\n",
            "  Client 0: total grad norm = 63.792081 (253 params)\n",
            "  Client 1: total grad norm = 76.433848 (253 params)\n",
            "  Client 2: total grad norm = 91.320892 (253 params)\n",
            "\n",
            "=== Optimizer Step 487 ===\n",
            "\n",
            "=== Updating Client Weights (Step 487) ===\n",
            "Gradient norms: [7.167291641235352, 8.682918548583984, 9.923308372497559]\n",
            "Target weights: [0.39250847697257996, 0.32399505376815796, 0.2834964394569397]\n",
            "Updated weights: [0.3449375033378601, 0.32939478754997253, 0.32566770911216736]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 487\n",
            "  Loss for client 0: 1.4340\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 487\n",
            "  Loss for client 1: 1.0806\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 487\n",
            "  Loss for client 2: 1.3256\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 488\n",
            "  Client 0: total grad norm = 94.494454 (253 params)\n",
            "  Client 1: total grad norm = 67.576438 (253 params)\n",
            "  Client 2: total grad norm = 64.191156 (253 params)\n",
            "\n",
            "=== Optimizer Step 488 ===\n",
            "\n",
            "=== Updating Client Weights (Step 488) ===\n",
            "Gradient norms: [9.532691955566406, 6.858308792114258, 6.6384782791137695]\n",
            "Target weights: [0.2613748013973236, 0.36329737305641174, 0.37532779574394226]\n",
            "Updated weights: [0.31986868381500244, 0.33956557512283325, 0.3405657410621643]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 488\n",
            "  Loss for client 0: 1.4035\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 488\n",
            "  Loss for client 1: 1.9141\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 488\n",
            "  Loss for client 2: 1.7205\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 489\n",
            "  Client 0: total grad norm = 81.719260 (253 params)\n",
            "  Client 1: total grad norm = 77.198207 (253 params)\n",
            "  Client 2: total grad norm = 77.853311 (253 params)\n",
            "\n",
            "=== Optimizer Step 489 ===\n",
            "\n",
            "=== Updating Client Weights (Step 489) ===\n",
            "Gradient norms: [7.658051490783691, 7.7658185958862305, 9.229130744934082]\n",
            "Target weights: [0.3551271855831146, 0.3501990735530853, 0.29467374086380005]\n",
            "Updated weights: [0.3304462432861328, 0.34275561571121216, 0.32679814100265503]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 489\n",
            "  Loss for client 0: 1.6775\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 489\n",
            "  Loss for client 1: 2.2283\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 489\n",
            "  Loss for client 2: 1.8006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 490\n",
            "  Client 0: total grad norm = 84.054655 (253 params)\n",
            "  Client 1: total grad norm = 88.729643 (253 params)\n",
            "  Client 2: total grad norm = 85.731057 (253 params)\n",
            "\n",
            "=== Optimizer Step 490 ===\n",
            "\n",
            "=== Updating Client Weights (Step 490) ===\n",
            "Gradient norms: [8.853941917419434, 9.171978950500488, 8.193035125732422]\n",
            "Target weights: [0.32830023765563965, 0.31691649556159973, 0.35478320717811584]\n",
            "Updated weights: [0.3298024535179138, 0.33500388264656067, 0.3351936638355255]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 490\n",
            "  Loss for client 0: 1.5660\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 490\n",
            "  Loss for client 1: 1.4992\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 490\n",
            "  Loss for client 2: 2.0658\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 491\n",
            "  Client 0: total grad norm = 96.539392 (253 params)\n",
            "  Client 1: total grad norm = 104.279162 (253 params)\n",
            "  Client 2: total grad norm = 84.275981 (253 params)\n",
            "\n",
            "=== Optimizer Step 491 ===\n",
            "\n",
            "=== Updating Client Weights (Step 491) ===\n",
            "Gradient norms: [10.424281120300293, 8.87025260925293, 8.525259017944336]\n",
            "Target weights: [0.2942952513694763, 0.3458544611930847, 0.3598502278327942]\n",
            "Updated weights: [0.31915026903152466, 0.3382590711116791, 0.3425906300544739]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 491\n",
            "  Loss for client 0: 1.3245\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 491\n",
            "  Loss for client 1: 1.2784\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 491\n",
            "  Loss for client 2: 1.3713\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 492\n",
            "  Client 0: total grad norm = 61.168253 (253 params)\n",
            "  Client 1: total grad norm = 62.167139 (253 params)\n",
            "  Client 2: total grad norm = 99.054005 (253 params)\n",
            "\n",
            "=== Optimizer Step 492 ===\n",
            "\n",
            "=== Updating Client Weights (Step 492) ===\n",
            "Gradient norms: [6.7072978019714355, 6.830350875854492, 8.182634353637695]\n",
            "Target weights: [0.3569282591342926, 0.3504979908466339, 0.2925737798213959]\n",
            "Updated weights: [0.33048367500305176, 0.3419307470321655, 0.3275855779647827]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 492\n",
            "  Loss for client 0: 1.6272\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 492\n",
            "  Loss for client 1: 1.9383\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 492\n",
            "  Loss for client 2: 1.9763\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 493\n",
            "  Client 0: total grad norm = 77.414872 (253 params)\n",
            "  Client 1: total grad norm = 87.955014 (253 params)\n",
            "  Client 2: total grad norm = 79.057630 (253 params)\n",
            "\n",
            "=== Optimizer Step 493 ===\n",
            "\n",
            "=== Updating Client Weights (Step 493) ===\n",
            "Gradient norms: [8.613631248474121, 11.36435604095459, 8.648598670959473]\n",
            "Target weights: [0.36312028765678406, 0.2752275764942169, 0.361652135848999]\n",
            "Updated weights: [0.3402746617794037, 0.3219197988510132, 0.33780553936958313]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 493\n",
            "  Loss for client 0: 1.7843\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 493\n",
            "  Loss for client 1: 1.3483\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 493\n",
            "  Loss for client 2: 1.7276\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 494\n",
            "  Client 0: total grad norm = 84.888521 (253 params)\n",
            "  Client 1: total grad norm = 69.226561 (253 params)\n",
            "  Client 2: total grad norm = 75.630711 (253 params)\n",
            "\n",
            "=== Optimizer Step 494 ===\n",
            "\n",
            "=== Updating Client Weights (Step 494) ===\n",
            "Gradient norms: [9.425708770751953, 7.3556227684021, 8.157102584838867]\n",
            "Target weights: [0.2909556031227112, 0.37283894419670105, 0.33620548248291016]\n",
            "Updated weights: [0.3254789412021637, 0.3371955454349518, 0.3373255133628845]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 494\n",
            "  Loss for client 0: 1.5262\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 494\n",
            "  Loss for client 1: 1.2813\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 494\n",
            "  Loss for client 2: 1.9611\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 495\n",
            "  Client 0: total grad norm = 64.143576 (253 params)\n",
            "  Client 1: total grad norm = 60.964468 (253 params)\n",
            "  Client 2: total grad norm = 93.048898 (253 params)\n",
            "\n",
            "=== Optimizer Step 495 ===\n",
            "\n",
            "=== Updating Client Weights (Step 495) ===\n",
            "Gradient norms: [7.321184158325195, 6.539732933044434, 11.013869285583496]\n",
            "Target weights: [0.3591674268245697, 0.4020853340625763, 0.238747239112854]\n",
            "Updated weights: [0.33558547496795654, 0.35666248202323914, 0.30775201320648193]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 495\n",
            "  Loss for client 0: 1.6866\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 495\n",
            "  Loss for client 1: 1.0356\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 495\n",
            "  Loss for client 2: 1.2446\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 496\n",
            "  Client 0: total grad norm = 100.541087 (253 params)\n",
            "  Client 1: total grad norm = 70.777535 (253 params)\n",
            "  Client 2: total grad norm = 74.388705 (253 params)\n",
            "\n",
            "=== Optimizer Step 496 ===\n",
            "\n",
            "=== Updating Client Weights (Step 496) ===\n",
            "Gradient norms: [9.817174911499023, 7.278822898864746, 7.6359124183654785]\n",
            "Target weights: [0.2751493752002716, 0.37110254168510437, 0.3537481427192688]\n",
            "Updated weights: [0.31745463609695435, 0.36099451780319214, 0.3215508460998535]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 496\n",
            "  Loss for client 0: 1.4903\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 496\n",
            "  Loss for client 1: 1.8095\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 496\n",
            "  Loss for client 2: 1.7546\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 497\n",
            "  Client 0: total grad norm = 82.817836 (253 params)\n",
            "  Client 1: total grad norm = 77.794481 (253 params)\n",
            "  Client 2: total grad norm = 77.689311 (253 params)\n",
            "\n",
            "=== Optimizer Step 497 ===\n",
            "\n",
            "=== Updating Client Weights (Step 497) ===\n",
            "Gradient norms: [8.524785041809082, 8.668517112731934, 8.434784889221191]\n",
            "Target weights: [0.3339913785457611, 0.3284534811973572, 0.3375551104545593]\n",
            "Updated weights: [0.32241564989089966, 0.3512322008609772, 0.3263521194458008]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 497\n",
            "  Loss for client 0: 2.0380\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 497\n",
            "  Loss for client 1: 1.4084\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 497\n",
            "  Loss for client 2: 1.6525\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 498\n",
            "  Client 0: total grad norm = 89.557278 (253 params)\n",
            "  Client 1: total grad norm = 71.311774 (253 params)\n",
            "  Client 2: total grad norm = 80.629766 (253 params)\n",
            "\n",
            "=== Optimizer Step 498 ===\n",
            "\n",
            "=== Updating Client Weights (Step 498) ===\n",
            "Gradient norms: [9.532651901245117, 7.457854747772217, 9.688897132873535]\n",
            "Target weights: [0.3065531849861145, 0.3918371796607971, 0.3016096353530884]\n",
            "Updated weights: [0.31765690445899963, 0.3634136915206909, 0.31892937421798706]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 498\n",
            "  Loss for client 0: 1.5319\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 498\n",
            "  Loss for client 1: 1.8463\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 498\n",
            "  Loss for client 2: 1.7768\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 499\n",
            "  Client 0: total grad norm = 73.397575 (253 params)\n",
            "  Client 1: total grad norm = 97.054690 (253 params)\n",
            "  Client 2: total grad norm = 85.042158 (253 params)\n",
            "\n",
            "=== Optimizer Step 499 ===\n",
            "\n",
            "=== Updating Client Weights (Step 499) ===\n",
            "Gradient norms: [8.792258262634277, 10.701194763183594, 8.790019035339355]\n",
            "Target weights: [0.35437503457069397, 0.2911596894264221, 0.3544653058052063]\n",
            "Updated weights: [0.3286723494529724, 0.3417375087738037, 0.3295901417732239]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 499\n",
            "  Loss for client 0: 1.3243\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 499\n",
            "  Loss for client 1: 2.1020\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 499\n",
            "  Loss for client 2: 1.7903\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 500\n",
            "  Client 0: total grad norm = 73.807580 (253 params)\n",
            "  Client 1: total grad norm = 89.274899 (253 params)\n",
            "  Client 2: total grad norm = 78.508166 (253 params)\n",
            "\n",
            "=== Optimizer Step 500 ===\n",
            "\n",
            "=== Updating Client Weights (Step 500) ===\n",
            "Gradient norms: [8.05285358428955, 9.940433502197266, 9.94119930267334]\n",
            "Target weights: [0.38165614008903503, 0.30918383598327637, 0.3091599941253662]\n",
            "Updated weights: [0.3445674777030945, 0.3319714069366455, 0.32346111536026]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 500\n",
            "  Val Loss = 1.8106 (5 batches)\n",
            "–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ /content/drive/MyDrive/meritopt_checkpoints/step_500\n",
            "\n",
            "Reached max steps (500), stopping...\n",
            "\n",
            "Epoch 1 completed:\n",
            "   Train Loss = 0.2701\n",
            "   Val Loss = 1.6939 (8 batches)\n",
            "–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ /content/drive/MyDrive/meritopt_checkpoints/epoch_1\n",
            "\n",
            "============================================================\n",
            "Quick test:\n",
            "============================================================\n",
            "Client 0: kil…îmetr biu î... -> –ö–∏–ª–º–µ—Ç—Ä –ø–æ–π–¥—ë—Ç....\n",
            "Client 1: Ou k…ônt…ô√∞u n ºenat ºa…Åa ≈ãanu…ôm…ônu huÕ°aa k…ônd…ô s ºiti…Å... -> –û—É, –∫–æ—Ç–æ—Ä–æ–π –ø–æ—Å—Ç–∞–ª –ø–æ —Ç–æ–ª—å–∫–æ–º—É, –∫–æ–≥–¥–∞ –ø–æ—à–ª...\n",
            "Client 2: A p ºixi e≈°ƒçe p ºim ºi... -> –ê —ç—Ç–æ –≤—Ä–µ–º—è –ø–æ—à–µ–ª–æ....\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABSgAAAKqCAYAAAA5TrDVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3XdYFMf/B/D3HuVoIr2KgAh2FLtiwYq9xRJjj5oYe+8RNJaoscXe9ZvEFmsSjV0Se9fYOxorKNKkc/v743634bgDDgTu1Pfree7Rm52dnb2542Y/NzsjiKIogoiIiIiIiIiIiEgPZPquABEREREREREREX26GKAkIiIiIiIiIiIivWGAkoiIiIiIiIiIiPSGAUoiIiIiIiIiIiLSGwYoiYiIiIiIiIiISG8YoCQiIiIiIiIiIiK9YYCSiIiIiIiIiIiI9IYBSiIiIiIiIiIiItIbBiiJiIiIiIiIiIhIbxigJNKD3r17w8vLK0/7hoaGQhCE/K0QFahz587B1NQUjx8/LtTjCoKA0NDQQj2mIdH2WfHy8kLv3r1z3HfDhg0QBAHh4eH5Vp/w8HAIgoANGzbkW5mZpaamwsPDA8uWLSuwYxARERWmzN/dYWFhEAQBYWFhequTvrxP/0S174ULF/K/YgZM175fbnzK70GigsQAJVEGgiDo9PhUv4x69+6t9jpYW1ujYsWKmDdvHpKTk/VdPYM1adIkdO3aFZ6enlLnMKdHXgPYH6KIiAgYGxuje/fuWeaJi4uDubk5OnToUIg1y5tNmzZh4cKFejm2iYkJRo4ciRkzZiApKUkvdSAiItLFgwcP8PXXX6NEiRIwMzODtbU1AgMDsWjRIiQmJuq7epg5cyZ2796dY76IiAgIgoBhw4ZpbBs2bBgEQUBISIjGtp49e8LExAQJCQn5Ud18tWzZsnz9QVUV0MvqsWXLlnw7Vn7K79eBiLJnrO8KEBmSn376Se35//73Pxw6dEgjvUyZMu91nNWrV0OhUORp38mTJ2P8+PHvdfz3IZfLsWbNGgBAdHQ0duzYgdGjR+P8+fMG27nQpytXruDw4cM4deoUAKBevXoa76d+/fqhevXq+Oqrr6Q0Kyur9z52YmIijI0N/8+8k5MTmjRpgj179iAhIQEWFhYaeXbu3ImkpKRsg5i6uHPnDmSygv1tbtOmTbh+/TqGDx+ulu7p6YnExESYmJgU6PH79OmD8ePHY9OmTfjyyy8L9FhERER5sXfvXnTq1AlyuRw9e/ZE+fLlkZKSghMnTmDMmDG4ceMGVq1apXXfevXqITExEaampgVax5kzZ6Jjx45o165dtvmcnJzg6+uLEydOaGw7efIkjI2NcfLkSa3bAgICtPZ7stKjRw98/vnnkMvlOu+TF8uWLYODg0O+jzwcOnQoqlWrppFeq1atfD1OfsnqdSis9yDRp8bwr1yJClHm4MeZM2dw6NChHIMiWQVVsvI+AQpjY2O9Bp0yj3QbOHAgatSoga1bt2L+/Plwc3PT2EcURSQlJcHc3LxQ6pjb9ihI69evR/HixVGzZk0AQIkSJVCiRAm1PAMGDECJEiWyfZ+lpaVBoVDkqiNkZmaWt0rrQbdu3bB//3789ttv+PzzzzW2b9q0CUWLFkXLli3f6zgF3aHPjiAIhdImNjY2aNq0KTZs2MAAJRERGZxHjx7h888/h6enJ44ePQpXV1dp26BBg3D//n3s3bs3y/1lMpnB9XHq1KmD//3vf4iPj5d+ZH737h2uXr2Kzp0747fffkN6ejqMjIwAAC9evMDDhw/Rtm3bXB3HyMhIKuNDVLduXXTs2FHf1XhvhvgeJPoY8BZvolwKCgpC+fLlcfHiRdSrVw8WFhaYOHEiAGDPnj1o2bIl3NzcIJfL4ePjg++++w7p6elqZWSeg1I1N90PP/yAVatWwcfHB3K5HNWqVcP58+fV9tU2r54gCBg8eDB2796N8uXLQy6Xo1y5cti/f79G/cPCwlC1alWYmZnBx8cHK1eufK95LWUyGYKCgqTzAJRzvbRq1QoHDhxA1apVYW5ujpUrVwIAHj58iE6dOsHOzg4WFhaoWbOm1k7o48eP0aZNG1haWsLJyQkjRozAgQMHNG6xz649kpOTERISgpIlS0Iul8PDwwNjx47VuB390KFDqFOnDmxsbGBlZYVSpUpJZagsXrwY5cqVg4WFBWxtbVG1alVs2rQpx9dn9+7daNiwYa5e34zvh4ULF0rvh5s3byIlJQVTpkxBlSpVULRoUVhaWqJu3bo4duyYRjmZ56BUtfP9+/fRu3dv2NjYoGjRoujTp0+OtxcNHjwYVlZWWvN17doVLi4u0vv8woULCA4OhoODA8zNzeHt7Z1joKx9+/awtLTU+ppGRETgyJEj6NixI+RyOY4fP45OnTqhePHiUruOGDFCp9vBtM1DdOPGDTRs2BDm5uYoVqwYpk+frnWEsy6f76CgIOzduxePHz/WuF0/qzkojx49irp168LS0hI2NjZo27Ytbt26pZYnt23XpEkTnDhxAlFRUTm+JkRERIVpzpw5iI+Px9q1a9WCkyolS5bUeru0Slbz/509exbNmjVD0aJFYWFhgfr162uMXNT1+1QQBLx79w4bN26Uvs+zG01Yp04dpKen48yZM2r1SUtLw+jRoxEfH48rV65I21T1qlOnTq7qr20OSoVCgdDQULi5ucHCwgINGjTAzZs3s5x7MTk5GSNHjoSjoyMsLS3Rvn17REZGStu9vLxw48YN/PXXX9K5q/r6qampmDp1Knx9fWFmZgZ7e3vUqVMHhw4dyvK1yY3y5cujQYMGGukKhQLu7u5qwc13795h1KhR8PDwgFwuR6lSpfDDDz9AFMVsj5HVdU/m1za71yGr9+Cvv/6KKlWqwNzcHA4ODujevTuePXumlqd3796wsrLCs2fP0K5dO1hZWcHR0RGjR4/WuGYk+tRwBCVRHrx58wbNmzfH559/ju7du8PZ2RmA8ovNysoKI0eOhJWVFY4ePYopU6YgNjYWc+fOzbHcTZs2IS4uDl9//TUEQcCcOXPQoUMHPHz4MMdRlydOnMDOnTsxcOBAFClSBD/++CM+++wzPHnyBPb29gCAy5cvo1mzZnB1dcXUqVORnp6OadOmwdHR8b1ejwcPHgCAdBxAeStt165d8fXXX6N///4oVaoUXr16hdq1ayMhIQFDhw6Fvb09Nm7ciDZt2mD79u1o3749AGWHo2HDhnjx4gWGDRsGFxcXbNq0SWsQDtDeHgqFAm3atMGJEyfw1VdfoUyZMrh27RoWLFiAu3fvSnMK3bhxA61atYK/vz+mTZsGuVyO+/fvq3UIV69ejaFDh6Jjx44YNmwYkpKS8M8//+Ds2bP44osvsnxdnj17hidPnqBy5cp5el3Xr1+PpKQkfPXVV5DL5bCzs0NsbCzWrFmDrl27on///oiLi8PatWsRHByMc+fOoVKlSjmW27lzZ3h7e2PWrFm4dOkS1qxZAycnJ8yePTvLfbp06YKlS5dKt2SpJCQk4Pfff0fv3r1hZGSEiIgING3aFI6Ojhg/fjxsbGwQHh6OnTt3ZlsnS0tLtG3bFtu3b0dUVBTs7OykbVu3bkV6ejq6desGQNn5S0hIwDfffAN7e3ucO3cOixcvxtOnT/Hrr7/meP4ZvXz5Eg0aNEBaWhrGjx8PS0tLrFq1SutoX10+35MmTUJMTAyePn2KBQsWAMj+dv3Dhw+jefPmKFGiBEJDQ5GYmIjFixcjMDAQly5d0piLVNe2q1KlCkRRxKlTp9CqVatcvSZEREQF6ffff0eJEiVQu3btfCvz6NGjaN68OapUqYKQkBDIZDKsX78eDRs2xPHjx1G9enW1/Dl9n/70008aU/D4+PhkeXxVoPHEiRNo3LgxAGUQ0s/PDwEBAShWrBhOnjyJKlWqSNsy7pfb+mc0YcIEzJkzB61bt0ZwcDCuXr2K4ODgLOeiHjJkCGxtbRESEoLw8HAsXLgQgwcPxtatWwEACxcuxJAhQ2BlZYVJkyYBgHStExoailmzZkmvTWxsLC5cuIBLly6hSZMmWdZRJS4uDq9fv9ZIt7e3hyAI6NKlC0JDQ/Hy5Uu4uLhI20+cOIHnz59Ld9mIoog2bdrg2LFj6Nu3LypVqoQDBw5gzJgxePbsmdQHex/ZvQ7abNiwAX369EG1atUwa9YsvHr1CosWLcLJkydx+fJl2NjYSHnT09MRHByMGjVq4IcffsDhw4cxb948+Pj44JtvvnnvuhN9sEQiytKgQYPEzB+T+vXriwDEFStWaORPSEjQSPv6669FCwsLMSkpSUrr1auX6OnpKT1/9OiRCEC0t7cXo6KipPQ9e/aIAMTff/9dSgsJCdGoEwDR1NRUvH//vpR29epVEYC4ePFiKa1169aihYWF+OzZMynt3r17orGxsUaZ2vTq1Uu0tLQUIyMjxcjISPH+/fvizJkzRUEQRH9/fymfp6enCEDcv3+/2v7Dhw8XAYjHjx+X0uLi4kRvb2/Ry8tLTE9PF0VRFOfNmycCEHfv3i3lS0xMFEuXLi0CEI8dOyalZ9UeP/30kyiTydSOJYqiuGLFChGAePLkSVEURXHBggUiADEyMjLL827btq1Yrly5HF+fzA4fPqzRftpYWlqKvXr1kp6r3g/W1tZiRESEWt60tDQxOTlZLe3t27eis7Oz+OWXX6qlAxBDQkKk56r3TuZ87du3F+3t7bOto0KhEN3d3cXPPvtMLX3btm0iAPHvv/8WRVEUd+3aJQIQz58/n2152uzdu1cEIK5cuVItvWbNmqK7u7v0/tD2OZs1a5YoCIL4+PFjKU3bZ8XT01PttVa9J8+ePSulRUREiEWLFhUBiI8ePZLSdf18t2zZUu3zraJq1/Xr10tplSpVEp2cnMQ3b95IaVevXhVlMpnYs2dPjXPRte2eP38uAhBnz56tsY2IiEhfYmJiRABi27Ztdd4n83f3sWPH1PqDCoVC9PX1FYODg0WFQiHlS0hIEL29vcUmTZpIabn5Ps3cP8uJk5OT2KhRI+l5cHCw2KdPH1EURbFz585ip06dpG1Vq1YVfX19c13/9evXq/VPXr58KRobG4vt2rVTq0toaKgIQK3+qn0bN26sdpwRI0aIRkZGYnR0tJRWrlw5sX79+hrnWLFiRbFly5Y6vyYqqjbL6vHixQtRFEXxzp07GtcvoiiKAwcOFK2srKS+2O7du0UA4vTp09XydezYURQEQe2aKPP7R1v/UBQ1X1tRzPp1yPweTElJEZ2cnMTy5cuLiYmJUr4//vhDBCBOmTJFSuvVq5cIQJw2bZpamQEBAWKVKlW0vHpEnw7e4k2UB3K5HH369NFIzzjqSvULYd26dZGQkIDbt2/nWG6XLl1ga2srPa9bty4A5W3ROWncuLHaL7v+/v6wtraW9k1PT8fhw4fRrl07tXkiS5YsiebNm+dYvsq7d+/g6OgIR0dHlCxZEhMnTkStWrWwa9cutXze3t4IDg5WS9u3bx+qV6+udjuLlZUVvvrqK4SHh+PmzZsAgP3798Pd3R1t2rSR8pmZmaF///5a66StPX799VeUKVMGpUuXxuvXr6VHw4YNAUAajan6NXPPnj1ZLlxkY2ODp0+fatxun5M3b94AgFqb5sZnn32mMbrVyMhImodSoVAgKioKaWlpqFq1Ki5duqRTuQMGDFB7XrduXbx58waxsbFZ7iMIAjp16oR9+/YhPj5eSt+6dSvc3d2lNlW9nn/88QdSU1N1qo+KauRlxtu8Hz16hDNnzqBr167S4jYZP2fv3r3D69evUbt2bYiiiMuXL+fqmPv27UPNmjXVRiY4OjpKozUzet/Pd2YvXrzAlStX0Lt3b7URo/7+/mjSpAn27dunsY+ubad6z2kbpUBERKQvqu+rIkWK5FuZV65cwb179/DFF1/gzZs3Up/v3bt3aNSoEf7++2+NPl5e+kI5CQwMxNmzZ5Geng6FQoEzZ85Io0QDAwOlUZMJCQm4cuWK1HfKS/1Vjhw5grS0NAwcOFAtfciQIVnW86uvvlK7xblu3bpIT0/H48ePczxHGxsb3LhxA/fu3csxrzZTpkzBoUOHNB6qfpCfnx8qVaokjeYElNcw27dvR+vWraW+2L59+2BkZIShQ4eqlT9q1CiIoog///wzT/XLqwsXLiAiIgIDBw5Um5uyZcuWKF26tNbprLS9B3W55iP6mDFASZQH7u7uWhcruXHjBtq3b4+iRYvC2toajo6O0sInMTExOZZbvHhxteeqIMPbt29zva9qf9W+ERERSExMRMmSJTXyaUvLipmZmdSZ+Pvvv/Hvv//i5MmTGgu/eHt7a+z7+PFjlCpVSiNdtSq6qmP0+PFj+Pj4aMwPk1U9tbXHvXv3cOPGDSmYqnr4+fkBUL4egDIoHBgYiH79+sHZ2Rmff/45tm3bptYRHDduHKysrFC9enX4+vpi0KBBWldjzIqYw1w4WdH2GgLAxo0b4e/vL8394+joiL179+r0HgPy/j7r0qULEhMT8dtvvwEA4uPjsW/fPnTq1Elqq/r16+Ozzz7D1KlT4eDggLZt22L9+vUa835qY2xsjC5duuD48ePSfD2qYGXGgOGTJ0+koJ5q3p769esD0O1zltHjx4/h6+urka7tffq+n29tx87qWGXKlJEuTjLSte1U77m8zi1LRERUEKytrQEof+jLL6pgWa9evTT6fWvWrEFycrLG9/T79LmzUqdOHWmuyevXryMmJgaBgYEAgNq1a+P58+cIDw+X5qZUBSjzUn8VVV8icx/Zzs4uyx/I3+fcp02bhujoaPj5+aFChQoYM2YM/vnnnxz3U6lQoQIaN26s8cjYj+/SpQtOnjwp9QXDwsIQERGBLl26SHkeP34MNzc3jUB35muKwpJdn6506dIa9TEzM9MYhJDxuo3oU8U5KInyQNv8dNHR0ahfvz6sra0xbdo0+Pj4wMzMDJcuXcK4ceOy/OUzo6xW5dMlwPU+++aGkZGRNLdOdgprxe6sjqVQKFChQgXMnz9f6z4eHh7Svn///TeOHTuGvXv3Yv/+/di6dSsaNmyIgwcPwsjICGXKlMGdO3fwxx9/YP/+/dixYweWLVuGKVOmYOrUqVnWSzUnZ147G9rO6+eff0bv3r3Rrl07jBkzBk5OTjAyMsKsWbOkuUBzktf3Ss2aNeHl5YVt27bhiy++wO+//47ExES1DqMgCNi+fTvOnDmD33//HQcOHMCXX36JefPm4cyZM9nOxwgA3bt3x5IlS7B582aMHj0amzdvRtmyZaW5NdPT09GkSRNERUVh3LhxKF26NCwtLfHs2TP07t1bp89ZXuTH5zs/6Np2qvecg4NDgdeJiIhIV9bW1nBzc8P169fzrUzVd/DcuXOznIs7c/+jIPrNGeehNDU1hZ2dHUqXLg0AqFSpEiwsLHDixAk8evRILX9e6v8+3ufc69WrhwcPHmDPnj04ePAg1qxZgwULFmDFihXo169fvtSvS5cumDBhAn799VcMHz4c27ZtQ9GiRdGsWbN8KT+rH28Lc4GaD3kldqKCxAAlUT4JCwvDmzdvsHPnTtSrV09KV3VC9M3JyQlmZma4f/++xjZtaQXB09MTd+7c0UhX3R7r6ekp/Xvz5k2IoqjWichNPX18fHD16lU0atQox1FkMpkMjRo1QqNGjTB//nzMnDkTkyZNwrFjx6RgrKWlJbp06YIuXbogJSUFHTp0wIwZMzBhwgS1WzkyUnVK8/M9sH37dpQoUQI7d+5UO6+QkJB8O0Z2OnfujEWLFiE2NhZbt26Fl5cXatasqZGvZs2aqFmzJmbMmIFNmzahW7du2LJlS46d1xo1asDHxwebNm1CkyZNcOPGDcyYMUPafu3aNdy9excbN25Ez549pfS8rh7p6emp9TalzO/T3Hy+dR21qHq/Z/WZcHBwgKWlpU5lZaaql2okARERkaFo1aoVVq1ahdOnT6NWrVrvXZ5qiiNra2udfkTXVW7vQqhcubIUhJTL5ahVq5ZUhrGxMapVq4aTJ0/i0aNHcHJyku7qeZ/6q/oS9+/fV7vz5s2bN+81Gi+7c7ezs0OfPn3Qp08fxMfHo169eggNDc23AKW3tzeqV6+OrVu3YvDgwdi5cyfatWsHuVwu5fH09MThw4cRFxenNooy8zWFNqoRo9HR0WoL12gbdZmXPp1qOimVO3fuZFsfIvoPb/EmyieqX8Iy/vqYkpKCZcuW6atKalQjH3fv3o3nz59L6ffv3y+0eVpatGiBc+fO4fTp01Lau3fvsGrVKnh5eaFs2bIAgODgYDx79ky6lRgAkpKSsHr1ap2P1blzZzx79kzrPomJidKts1FRURrbVb9eq25LVs0lqWJqaoqyZctCFMVs51l0d3eHh4cHLly4oHO9c6LtfXb27Fm117QgdenSBcnJydi4cSP279+Pzp07q21/+/atxi/wmV/PnHTr1g2XL19GSEgIBEFQWyld2/mLoohFixbl5XTQokULnDlzBufOnZPSIiMj8csvv6jly83n29LSUqdbvl1dXVGpUiVs3LgR0dHRUvr169dx8OBBtGjRIrenI7l48SIEQciXCz8iIqL8NHbsWFhaWqJfv3549eqVxvYHDx7k6nu9SpUq8PHxwQ8//KA2T7ZKZGRknuppaWmp9v2cE2NjY9SoUQMnT57EyZMnNVYpr127Nv7++2+cOXNGuvX7fevfqFEjGBsbY/ny5WrpS5Ys0bne2mR17pn7xFZWVihZsqTOfTxddenSBWfOnMG6devw+vVrtbt1AGX/LT09XeM8FyxYAEEQsp1fXxUQ/vvvv6W0d+/eYePGjRp5dX0PVK1aFU5OTlixYoXaa/Hnn3/i1q1baNmyZY5lEBFHUBLlm9q1a8PW1ha9evXC0KFDIQgCfvrpp3y/xfp9hIaG4uDBgwgMDMQ333wjfbGXL18eV65cKfDjjx8/Hps3b0bz5s0xdOhQ2NnZYePGjXj06BF27NghLYLy9ddfY8mSJejatSuGDRsGV1dX/PLLL9JIRV1+zezRowe2bduGAQMG4NixYwgMDER6ejpu376Nbdu24cCBA6hatSqmTZuGv//+Gy1btoSnpyciIiKwbNkyFCtWTLr1pmnTpnBxcUFgYCCcnZ1x69YtLFmyBC1btsxxkve2bdti165dGqNB86pVq1bYuXMn2rdvj5YtW+LRo0dYsWIFypYtq7VTm98qV66MkiVLYtKkSUhOTtboMG7cuBHLli1D+/bt4ePjg7i4OKxevRrW1tY6B9y6d++OadOmYc+ePQgMDISXl5e0rXTp0vDx8cHo0aPx7NkzWFtbY8eOHXkeJTB27Fj89NNPaNasGYYNGwZLS0usWrUKnp6eanMq5ebzXaVKFWzduhUjR45EtWrVYGVlhdatW2s9/ty5c9G8eXPUqlULffv2RWJiIhYvXoyiRYsiNDQ0T+cEKEeUBgYGStMMEBERGQrVnRJdunRBmTJl0LNnT5QvXx4pKSk4deoUfv31V/Tu3Vvn8mQyGdasWYPmzZujXLly6NOnD9zd3fHs2TMcO3YM1tbW+P3333NdzypVquDw4cOYP38+3Nzc4O3tjRo1amS7T506daSFGDMGIQFlX2LWrFlSvvyov7OzM4YNG4Z58+ahTZs2aNasGa5evYo///wTDg4Oee57VqlSBcuXL8f06dNRsmRJODk5oWHDhihbtiyCgoJQpUoV2NnZ4cKFC9i+fTsGDx6sU7nHjx9HUlKSRrq/vz/8/f2l5507d8bo0aMxevRo2NnZaYwsbd26NRo0aIBJkyYhPDwcFStWxMGDB7Fnzx4MHz5cbeHQzJo2bYrixYujb9++GDNmDIyMjLBu3To4OjriyZMnOr0OmZmYmGD27Nno06cP6tevj65du+LVq1dYtGgRvLy8MGLECJ1eH6JPXqGuGU70gRk0aJCY+WNSv359sVy5clrznzx5UqxZs6Zobm4uurm5iWPHjhUPHDggAhCPHTsm5evVq5fo6ekpPX/06JEIQJw7d65GmQDEkJAQ6XlISIhGnQCIgwYN0tjX09NT7NWrl1rakSNHxICAANHU1FT08fER16xZI44aNUo0MzPL4lX4T69evURLS8sc83l6eootW7bUuu3Bgwdix44dRRsbG9HMzEysXr26+Mcff2jke/jwodiyZUvR3NxcdHR0FEeNGiXu2LFDBCCeOXNGypdde6SkpIizZ88Wy5UrJ8rlctHW1lasUqWKOHXqVDEmJkZ6Pdq2bSu6ubmJpqamopubm9i1a1fx7t27UjkrV64U69WrJ9rb24tyuVz08fERx4wZI5WRnUuXLokAxOPHj2eZx9LSUq2dsns/KBQKcebMmaKnp6col8vFgIAA8Y8//tB4T4li1u+dyMhItXzr168XAYiPHj3K8XxEURQnTZokAhBLliyp9Xy7du0qFi9eXJTL5aKTk5PYqlUr8cKFCzqVrVKtWjURgLhs2TKNbTdv3hQbN24sWllZiQ4ODmL//v3Fq1evigDE9evXa5xvRto+E//8849Yv3590czMTHR3dxe/++47ce3atRqvia6f7/j4ePGLL74QbWxsRABSu6jaNWMdRVEUDx8+LAYGBorm5uaitbW12Lp1a/HmzZtqeXLTdtHR0aKpqam4Zs0a7S8uERGRAbh7967Yv39/0cvLSzQ1NRWLFCkiBgYGiosXLxaTkpKkfJm/u48dO6bx3SuKonj58mWxQ4cOUn/N09NT7Ny5s3jkyBEpT26+T2/fvi3Wq1dPNDc3FwFo9B+0UfULjI2NxXfv3qlte/PmjSgIgghAPHv2rMa+utRfWz3T0tLEb7/9VnRxcRHNzc3Fhg0birdu3RLt7e3FAQMGaOx7/vx5teNqez1fvnwptmzZUixSpIgIQKxfv74oiqI4ffp0sXr16qKNjY1obm4uli5dWpwxY4aYkpKS7euiOkZWj4z9VZXAwEARgNivXz+tZcbFxYkjRowQ3dzcRBMTE9HX11ecO3euqFAo1PJp6/tdvHhRrFGjhmhqaioWL15cnD9/vtbXNqvXIav34NatW8WAgABRLpeLdnZ2Yrdu3cSnT5+q5cnqekpbv5XoUyOIogEN7yIivWjXrh1u3LihdS4+Q7Jw4UKMGDECT58+hbu7u76ro7NGjRrBzc0NP/30k76rQp+AhQsXYs6cOXjw4EGhLlZFREREhiE6Ohq2traYPn06Jk2apO/qEBHphHNQEn1iEhMT1Z7fu3cP+/btQ1BQkH4qlIXM9UxKSsLKlSvh6+v7QQUnAWDmzJnYunWr1sm3ifJTamoq5s+fj8mTJzM4SURE9AnI3GcGlD9WAjC4/j0RUXY4gpLoE+Pq6orevXujRIkSePz4MZYvX47k5GRcvnwZvr6++q6epHnz5ihevDgqVaqEmJgY/Pzzz7hx4wZ++eUXtUVTiIiIiIg+VRs2bMCGDRvQokULWFlZ4cSJE9i8eTOaNm2KAwcO6Lt6REQ64yI5RJ+YZs2aYfPmzXj58iXkcjlq1aqFmTNnGlRwElCu5L1mzRr88ssvSE9PR9myZbFlyxaNRVmIiIiIiD5V/v7+MDY2xpw5cxAbGystnDN9+nR9V42IKFc4gpKIiIiIiIiIiIj0hnNQEhERERERERERkd4wQElERERERERERER6wzkotVAoFHj+/DmKFCkCQRD0XR0iIiKiXBNFEXFxcXBzc4NMxt+kPzTsjxIREdGHLjf9UQYotXj+/Dk8PDz0XQ0iIiKi9/bvv/+iWLFi+q4G5RL7o0RERPSx0KU/ygClFkWKFAGgfAGtra0L5BgKhQKRkZFwdHTkqAYDwnYxXGwbw8W2MUxsF8NVWG0TGxsLDw8PqV9DHxb2Rz9tbBvDxHYxXGwbw8W2MUyG2B9lgFIL1W001tbWBdohTEpKgrW1NT+kBoTtYrjYNoaLbWOY2C6Gq7DbhrcHf5jYH/20sW0ME9vFcLFtDBfbxjAZYn+U7w4iIiIiIiIiIiLSGwYoiYiIiIiIiIiISG8YoCQiIiIiIiIiIiK94RyUREQfKYVCgZSUlEI5TmpqKpKSkjivjAFhuxiu/GobExMTGBkZ5WPNiIiI6FPBa4VPmyH2RxmgJCL6CKWkpODRo0dQKBQFfixRFKFQKBAXF8fFOAwI28Vw5Wfb2NjYwMXFhW1MREREOuO1Ahlif5QBSiKij4woinjx4gWMjIzg4eFR4L9UiqKItLQ0GBsbs9NhQNguhis/2kYURSQkJCAiIgIA4Orqmp9VJCIioo8UrxUIMMz+KAOUREQfmbS0NCQkJMDNzQ0WFhYFfjx2OgwT28Vw5VfbmJubAwAiIiLg5OTE270L0N9//425c+fi4sWLePHiBXbt2oV27dplu09YWBhGjhyJGzduwMPDA5MnT0bv3r0Lpb5ERERZ4bUCAYbZH+UEAEREH5n09HQAgKmpqZ5rQkQFTXVhkZqaqueafNzevXuHihUrYunSpTrlf/ToEVq2bIkGDRrgypUrGD58OPr164cDBw4UcE2JiIiyx2sFym/51R/lCEoioo8Uf6Ek+vjxc144mjdvjubNm+ucf8WKFfD29sa8efMAAGXKlMGJEyewYMECBAcHF1Q1iYiIdMY+BOWX/HovMUBJRERERJSPTp8+jcaNG6ulBQcHY/jw4Vnuk5ycjOTkZOl5bGwsAOUqmwW1iIFCoZAmySfDwrYxTGwXw8W20Z3qtVI9CoPqOIV1PNJNfrWL6r2krc+Sm88kA5RERPTR8vLywvDhw7MNCnxovv32W7x69QqrVq3Sd1VyrXfv3oiOjsbu3bvztdwNGzZg+PDhiI6Oztdy9en169coW7YsLl26hGLFium7OpRLL1++hLOzs1qas7MzYmNjkZiYKM3XlNGsWbMwdepUjfTIyEgkJSUVSD0VCgViYmIgimKBL5JAucO2MUxsF8PFttFdamoqFAoF0tLSkJaWVuDHE0VRuq28MEZtmpqa4tdff0Xbtm0RHh4OPz8/nDt3DpUqVSrwY+eXo0ePYtiwYbhy5UqBzTGeVbscOHAAkyZNwrlz53T+LKWlpUGhUODNmzcwMTFR2xYXF6dznRigJCIivcupsxISEoLQ0NBcl3v+/HlYWlrmsVZKQUFBqFSpEhYuXPhe5eSHly9fYtGiRbh27Zq+q5Kt8PBweHt74/Llyx9UZzC/6LKgiiiKmDJlCtasWYPo6GgEBgZi+fLl8PX1BQA4ODigZ8+eCAkJwdq1a/VwFlTYJkyYgJEjR0rPY2Nj4eHhAUdHR1hbWxfIMRUKBQRBgKOjIy/oDQzbxjCxXQwX20Z3SUlJiIuLg7GxMYyNCy8klDlwlRcvX77EjBkzsG/fPjx79gxOTk6oVKkShg0bhkaNGkn5jIyMYGxsDG9vbzx//hwODg75eq4ymQw7d+7MccG8qKgoDB06FL///jtkMhk6dOiARYsWwcrKKtv9Jk6ciMmTJ0MulwNQ/hj/5ZdfauSTy+VITEwEAPTp0wcbN27EzJkzMX78eCnP7t270aFDB2kkY1hYGBo2bKj1mNOnT0fLli0xdepUbN26FT169Mi2nirGxsaQyWSwt7eHmZmZ2rbMz7MtR+ecREREBeTFixfS/7du3YopU6bgzp07UlrGL3HVr326dDIcHR3zt6J6tmbNGtSuXRuenp76rgplQ7WgypdffokOHTpozfPDDz9g8eLF2LhxI7y9vfHtt98iODgYN2/elDpyffr0QZUqVTB37lzY2dkV5inQe3JxccGrV6/U0l69egVra2utoycB5UWG6kIkI5lMVqAX24IgFPgxKG/YNoaJ7WK42Da6kclkEARBehQ0URSl47zP8cLDwxEYGAgbGxvMnTsXFSpUQGpqKg4cOIDBgwfj9u3bUl7VuRkbG8PV1fW9z0EbXV6/7t2748WLFzh06BBSU1PRp08ffP3119i0aVOW+5w4cQIPHjxAx44d1V43a2trtesjbXUwMzPDnDlzMGDAANja2kp5tP17+/ZtWFhYSKt4W1lZSdt69+6NxYsXo2fPnrl6LbR9/nLzeeQnl4joI6ZQAJGR+n3oMu2Ii4uL9ChatCgEQZCe3759G0WKFMGff/6JKlWqQC6XS1/cbdu2hbOzM6ysrFCtWjUcPnxYrVwvLy+1kY+CIGDNmjVo3749LCws4Ovri99+++29XuMdO3agXLlykMvl8PLykhbFUFm2bBl8fX1hZmYGZ2dndOzYUdq2fft2VKhQAebm5rC3t0fjxo3x7t27LI+1ZcsWtG7dWi0tuzL69u2L9u3bY+bMmXB2doaNjQ2mTZuGtLQ0jBkzBnZ2dihWrBjWr1+vVua1a9fQsGFDqcyvvvoK8fHx0naFQoFp06ahWLFikMvlqFSpEvbv3y9t9/b2BgAEBARAEAQEBQWplf/DDz/A1dUV9vb2GDRokNqKf8nJyRg9ejTc3d1haWmJGjVqICwsTG3/DRs2oHjx4rCwsED79u3x5s2bLF8zbf744w/Y2NhIt7VcuXIFgiCo/drcr18/dO/ePVflqjRv3hzTp09H+/bttW4XRRGLFy/GpEmT0LZtW/j7++N///sfnj9/rnb7e7ly5eDm5oZdu3blqR6kP7Vq1cKRI0fU0g4dOoRatWrpqUZERESaPpRrBQAYOHAgBEHAuXPn8Nlnn8HPzw/lypXDyJEjcebMGa37hIeHQxAEXLlyRUq7fv06mjdvDisrKzg7O6NHjx54/fq1tD0oKAhDhw7F2LFjYWdnBxcXF7U7uby8vAAA7du3hyAI0vPMbt26hf3792PNmjWoUaMG6tSpg8WLF2PLli14/vx5lue5ZcsWNGnSRGPkYcbrI9Uj83QyjRs3houLC2bNmpVl+SpOTk5qZWUcENK6dWtcuHABDx48yLGc/MQAJRHRR+zNG8DJqWAfzs4C3N1N4OwsaN2ey9hRlsaPH4/vv/8et27dgr+/P+Lj49GiRQscOXIEly9fRrNmzdC6dWs8efIk23KmTp2Kzp07459//kGLFi3QrVs3REVF5alOFy9eROfOnfH555/j2rVrCA0NxbfffosNGzYAAC5cuIChQ4di2rRpuHPnDvbv34969eoBUI4a7dq1K7788kvcunULYWFh6NChQ5aTVEdFReHmzZuoWrWqlKZLGUePHsXz58/x999/Y/78+QgJCUGrVq1ga2uLs2fPYsCAAfj666/x9OlTAMrRf8HBwbC1tcX58+fx66+/4vDhwxg8eLBU5qJFizBv3jz88MMP+OeffxAcHIw2bdrg3r17AIBz584BAA4fPowXL15g586d0r7Hjh3DgwcPcOzYMWzcuBEbNmyQXi8AGDx4ME6fPo0tW7bgn3/+QadOndCsWTOp7LNnz6Jv374YPHgwrly5ggYNGmD69Om5are6desiLi4Oly9fBgD89ddfcHBwUAuE/vXXX1Jg9cmTJ7Cyssr2MXPmTJ2P/+jRI7x8+VJtEZWiRYuiRo0aOH36tFre6tWr4/jx47k6P8p/8fHxuHLlinSB8+jRI1y5ckX6ezNhwgS1UQYDBgzAw4cPMXbsWNy+fRvLli3Dtm3bMGLECH1Un4iISKsP5VohKioK+/fvx6BBg7RO32RjY6PT+UZHR6Nhw4YICAjAhQsXsH//frx69QqdO3dWy7dx40ZYWlri7NmzmDNnDqZNm4ZDhw4BUE4hBQDr16/HixcvpOeZnT59GjY2Nmp998aNG0Mmk+Hs2bNZ1vH48eNq++SGkZERZs6cicWLF0t9+7woXrw4nJ2dC78PKpKGmJgYEYAYExNTYMdIT08XX7x4IaanpxfYMSj32C6Gi22ju8TERPHmzZtiYmKiGBEhioB+HxERuav/+vXrxaJFi0rPjx07JgIQd+/eneO+5cqVExcvXiw99/T0FBcsWCA9ByBOnjxZeh4fHy8CEP/8888sy6xfv744bNgwrdu++OILsUmTJmppY8aMEcuWLSuKoiju2LFDtLa2FmNjYzX2vXjxoghADA8Pz/G8RFEUL1++LAIQnzx5olMZCoVC7NGjh+jp6an2uSlVqpRYt25d6XlaWppoaWkpbt68WRRFUVy1apVoa2srxsfHS3n27t0rymQy8eXLl6IoiqKbm5s4Y8YMteNVq1ZNHDhwoCiKovjo0SMRgHj58mW1PL169RI9PT3FtLQ0Ka1Tp05ily5dRFEUxcePH4tGRkbis2fP1PZr1KiROGHCBFEURbFr165iixYt1LZ36dJF7T2ji8qVK4tz584VRVEU27VrJ86YMUM0NTUV4+LixKdPn4oAxLt374qiKIqpqanivXv3sn28efNG63EAiLt27VJLO3HihAhA4zw7deokdu7cWS1txIgRYlBQUJbnkfHznllh9Gc+Faq/Q5kfvXr1EkVR+d6uX7++xj6VKlUSTU1NxRIlSojr16/P1THZH/20sW0ME9vFcLFtdPchXiucPXtWBCDu3Lkzx7wZ+16Z+6Tfffed2LRpU7X8//77rwhAvHPnjiiKyr5/nTp11PJUq1ZNHDdunNZjZGXGjBmin5+fRrqjo6O4bNmyLPcrWrSo+L///U8tbf369SIA0dLSUu3RrFkzKU+vXr3Etm3biqIoijVr1hS//PJLURRFcdeuXWLG0J+qT5O5rNevX6sdMyAgQAwNDc32HFXyqz/KOSiJiOiDkPmXxPj4eISGhmLv3r148eIF0tLSkJiYmOMISn9/f+n/lpaWsLa2RkRERJ7qdOvWLbRt21YtLTAwEAsXLkR6ejqaNGkCT09PlChRAs2aNUOzZs2k28srVqyIRo0aoUKFCggODkbTpk3RsWNHab6YzFQTYGe83UOXMsqVK6c294uzszPKly8vPTcyMoK9vb30Gty6dQsVK1ZU+3U6MDAQCoUCd+7cgbm5OZ4/f47AwECN87569WqOr1m5cuXUViN0dXWVFv25du0a0tPT4efnp7ZPcnIy7O3tpfplvnW6Vq1aareY66J+/foICwvDqFGjcPz4ccyaNQvbtm3DiRMnEBUVBTc3N2nBGmNjY5QsWTJX5ecXc3NzJCQk6OXY9J+goKAsRzcDUBsFnHEf1ShdIiIiyrvsvoNz4+rVqzh27JjWRWoePHgg9UEzXi8Ayv5qXq8XcisxMVHrwjJFihTBpUuX1NKymtd69uzZaNiwIUaPHp3lcf7++2+Ym5tLc1BmvgbRRx+UAUoiIvogZL6dY/To0Th06BB++OEHlCxZEubm5ujYsSNSUlKyLSfzCoKCIEir2uU3VUciLCwMBw8exJQpUxAaGorz58/DxsYGhw4dwqlTp3Dw4EFpTsKzZ89Kczhm5ODgAAB4+/attPiPkZFRlmWo5sPRdr6F+Rpklt2x4+PjYWRkhIsXL6oFMQHkuNphbgUFBWHdunW4evUqTExMULp0aQQFBSEsLAxv375F/fr1pbxPnjxB2bJlsy1v4sSJmDhxok7HdnFxAaBcNMXNzU1Kf/Xqlcaq51FRUR/dYk9EREREueHr6wtBENQWwsmL+Ph4tG7dGrNnz9bYlnExnfzoK7u4uGgENdPS0hAVFSX1BbVxcHDA27dvNdJlMpnOP5jXq1cPwcHBmDBhAnr37q01j7e3N6ysrKQAZWb66IMyQElE9BGztwcK+sc+URSRlpaW5Zfb/w98y3cnT55E7969pdF08fHxCA8PL5iDZaFMmTI4efKkRr38/PykAJuxsTEaN26Mxo0bIyQkBDY2Njh69Cg6dOgAQRAQGBiIwMBATJkyBZ6enti1axdGjhypcSwfHx9YW1vj5s2baiMMsyojr3PdlSlTBhs2bMC7d++koPDJkychk8lQqlQpWFtbw83NDSdPnlQL4p08eRLVq1cHAJiamgKAtAiNrgICApCeno6IiAjUrVs3y/plnrcnq4nRs6Oah3LBggXSeQQFBeH777/H27dvMWrUKCmvm5ub2uTq2uRmlW1vb2+4uLjgyJEjCAgIAADExsbi7Nmz+Oabb9TyXr9+XWORISIiIqL88KFcK9jZ2SE4OBhLly7F0KFDNQYuREdH6zQPZeXKlbFjxw54eXnB2Djv4TATE5Mc+7m1atVCdHQ0Ll68iCpVqgBQzg2vUChQo0aNLPcLCAjAzZs381w3le+//x6VKlVCqVKlcr1vUlISHjx4IPVTCwsDlEREHzGZDCjoH75EEUhLA4yNAS19jgLj6+uLnTt3onXr1hAEAd9++22BjQKMjIzUCFC5urpi1KhRqFatGr777jt06dIFp0+fxpIlS7Bs2TIAytWiHz58iHr16sHW1hb79u2DQqFAqVKlcPbsWRw5cgRNmzaFk5MTzp49i8jISJQpU0ZrHWQyGRo3bowTJ06gXbt2AJDrMnTRrVs3hISEoFevXggNDUVkZCSGDBmCHj16SCsFjhkzBiEhIfDx8UGlSpWwfv16XLlyBb/88gsA5aqA5ubm2L9/P4oVKwYzMzMULVo0x2P7+fmhW7du6NmzJ+bNm4eAgABERkbiyJEj8Pf3R8uWLTF06FAEBgbihx9+QNu2bXHgwIFc394NALa2tvD398cvv/yCJUuWAFD+2ty5c2ekpqaqBV9ze4t3fHw87t+/Lz1XLahiZ2eH4sWLQxAEDBkyBDNmzICfnx+8vb3x7bffws3NTWpbAEhISMDFixdztQAPERERka4+pGuFpUuXIjAwENWrV8e0adPg7++PtLQ0HDp0CMuXL8etW7dyLGPQoEFYvXo1unbtKq3Sff/+fWzZsgVr1qzRuIMnK15eXjhy5AgCAwMhl8u1TtFUpkwZNGvWDP3798eKFSuQmpqKwYMH4/PPP1e7gyaz4OBgbNy4USNdFEW8fPlSI93JyUltOieVChUqoFu3bvjxxx91OqeMzpw5A7lcjlq1auV63/fBVbyJiOiDNH/+fNja2qJ27dpo3bo1goODUbly5QI51qZNmxAQEKD2WL16NSpXroxt27Zhy5YtKF++PKZMmYJp06ZJt1LY2Nhg586daNiwIcqUKYMVK1Zg8+bNKFeuHKytrfH333+jRYsW8PPzw+TJkzFv3jw0b948y3r069cPW7ZskQKxeSkjJxYWFjhw4ACioqJQrVo1dOzYEY0aNZKCeAAwdOhQjBw5EqNGjUKFChWwf/9+/Pbbb2pzNv74449YuXIl3NzcNObpzM769evRs2dPjBo1CqVKlUK7du1w/vx5FC9eHABQs2ZNrF69GosWLULFihVx8OBBTJ48Wa2M8PBwCIKgtiq3NvXr10d6ero0QtHOzg5ly5aFi4tLnn5tVrlw4YL0PgGAkSNHIiAgAFOmTJHyjB49GoMHD8ZXX32FatWqIT4+Hvv371ebc2jPnj0oXrx4lqNJiYiIiD4VJUqUwKVLl9CgQQOMGjUK5cuXR5MmTXDkyBEsX75cpzJUdwGlp6ejadOmqFChAoYPHw4bGxutQb6szJs3D4cOHYKHh0e2owx/+eUXlC5dGo0aNUKLFi1Qp04drFq1Ktuyu3Xrhhs3buDOnTtq6bGxsXB1ddV4ZDc35rRp0/I0gGPz5s3o1q0bLCwscr3v+xDE/Jpt9CMSGxuLokWLIiYmBtbW1gVyDIVCgYiIiCyj3aQfbBfDxbbRXVJSEh49egRvb2+tEyznt5xu26D8I4oiatSogREjRqBr16455v1U2+XYsWPo0KEDHj58mOWiQ/qka9vUrFkTQ4cOxRdffJFlnuw+74XRn6GCw/7op41tY5jYLoaLbaM7XisYvjFjxiA2NhYrV64ssGNk1S6vX79GqVKlcOHCBa3z4muTX/1RfnKJiIg+EIIgYNWqVUhLS9N3VQzavn37MHHiRIMMTurq9evX6NChQ46BaCIiIiL6uEyaNAmenp6FtohlRuHh4Vi2bJnOwcn8xDkoiYiIPiCVKlXSWOmZ1M2dO1ffVXhvDg4OGDt2rL6rQURERESFzMbGBhMnTtTLsatWrYqqVavq5dgcQUlERERERERERER6wwAlERERERERERER6Q0DlERERERERERERKQ3DFASERERERERERGR3jBASURERERERERERHrDACURERERERERERHpDQOUREREREREREREpDcMUBIR0UcjKCgIw4cP13c1dHbnzh24uLggLi5O31XJtbCwMAiCgOjo6HwvWxAE7N69GwDw+vVrODk54enTp/l+HCIiIiL6uGTsR4aHh0MQBFy5ckWvdcotQ7pG+PzzzzFv3rxCORYDlEREpHetW7dGs2bNtG47fvw4BEHAP//8897H2bBhA2xsbN67nPwyYcIEDBkyBEWKFNF3VbKlz8Cvg4MDevbsiZCQEL0cn4iIiIgMw8uXLzFkyBCUKFECcrkcHh4eaN26NY4cOaI1v4eHB168eIHy5cvnaz0yBkGzM2PGDNSuXRsWFha5ugbJfI2gGhhQrlw5pKenq+W1sbHBhg0b1NIuX76MLl26wNXVFXK5HJ6enmjVqhV+//13iKKocbzg4GAYGRnh/PnzGtsmT56MGTNmICYmRuf65xUDlEREpHd9+/bFoUOHtI6SW79+PapWrQp/f3891KzgPHnyBH/88Qd69+6t76oYvD59+uCXX35BVFSUvqtCRERERHoQHh6OKlWq4OjRo5g7dy6uXbuG/fv3o0GDBhg0aJDWfYyMjODi4gJjY+NCrq1SSkoKOnXqhG+++UbnfbK7Rnj48CH+97//Zbv/nj17ULNmTcTHx2Pjxo24desW9u/fj/bt22Py5MkagcYnT57g1KlTGDx4MNatW6dRXvny5eHj44Off/5Z53PIq48yQPns2TN0794d9vb2MDc3R4UKFXDhwgV9V4uISL8iI/P+SEzMutzXr7XvkwutWrWCo6Ojxq9/8fHx+PXXX9G3b1+8efMGXbt2hbu7OywsLFChQgVs3rw5Dy9E1p48eYK2bdvCysoK1tbW6Ny5M169eiVtv3r1Kho0aIAiRYrA2toaVapUkb5fHj9+jNatW8PW1haWlpYoV64c9u3bl+Wxtm3bhooVK8Ld3V1Ky64M1S+nBw4cQEBAAMzNzdGwYUNERETgzz//RJkyZWBtbY0vvvgCCQkJUpnJyckYOnQonJycYGZmhjp16mj8OvrXX3+hevXqkMvlcHV1xfjx45GWlgYA6N27N/766y8sWrQIgiBAEASEh4dL+168eBFVq1aFhYUFateujTt37qiVvWfPHlSuXBlmZmYoUaIEpk6dKpUNAPfu3UO9evVgZmaGsmXL4tChQxqvVbly5eDm5oZdu3Zl+XoSERERUe4oRAUi30Xq9aEQFTrVdeDAgRAEAefOncNnn30GPz8/lCtXDiNHjsSZM2e07qPtFu/r16+jefPmsLKygrOzM3r06IHXr19L24OCgjB06FCMHTsWdnZ2cHFxQWhoqLTdy8sLANC+fXsIgiA912bq1KkYMWIEKlSooNM5AtqvEVSGDBmCkJAQJCcna9333bt36Nu3L1q2bIm9e/eiadOmKFGiBMqUKYO+ffvi6tWrKFq0qNo+69evR6tWrfDNN99g8+bNSNRy3de6dWts2bJF53PIK/2EkQvQ27dvERgYiAYNGuDPP/+Eo6Mj7t27B1tbW31XjYhIv5yc8r7vkiVAFr9MomxZmGT4UpdouX0gK8bGxujZsyc2bNiASZMmQRAEAMCvv/6K9PR0dO3aFfHx8ahSpQrGjRsHa2tr7N27Fz169ICPjw+qV6+el7NSo1AopODkX3/9hbS0NAwaNAhdunRBWFgYAKBbt24ICAjA8uXLYWRkhCtXrsDExAQAMGjQIKSkpODvv/+GpaUlbt68CSsrqyyPd/z4cVStWlUtTZcyQkNDsWTJElhYWKBz587o3Lkz5HI5Nm3ahPj4eLRv3x6LFy/G2LFjAQBjx47Fjh07sHHjRnh6emLOnDkIDg7G/fv3YWdnh2fPnqFFixbo3bs3/ve//+H27dvo378/zMzMEBoaikWLFuHu3bsoX748pk2bBgBwdHSUgpSTJk3CvHnz4OjoiAEDBuDLL7/EyZMnpXPs2bMnfvzxR9StWxcPHjzAV199BQAICQmBQqFAhw4d4OzsjLNnzyImJibLW8mrV6+O48ePo2/fvro3KhERERFl6U3CGzj98B7XCPkgYnQEHC0ds80TFRWF/fv3Y8aMGbC0tNTYruvt09HR0WjYsCH69euHBQsWIDExEePGjUPnzp1x9OhRKd/GjRsxcuRInD17FqdPn0bv3r0RGBiIJk2a4Pz583BycsL69evRrFkzGBkZ5ep8c6LtGkFl+PDh+Pnnn7F48WKMHj1aY/vBgwfx5s0b6TpAG9V1FgCIoogNGzZg6dKlKF26NEqWLInt27ejR48eavtUr14dM2bMQHJyMuRyeR7PLGcfXYBy9uzZ8PDwwPr166U0b29vPdaIiIh08eWXX2Lu3Ln466+/EBQUBED5i95nn32GokWLomjRompfxEOGDMGBAwewbdu2fAlQHjlyBNeuXcOjR4/g4eEBAPjf//6HcuXK4fz586hWrRqePHmCMWPGoHTp0gAAX19faf8nT57gs88+k34hLVGiRLbHe/z4sUbnQ5cypk+fjsDAQADKW+MnTJiABw8eSHk7duyIY8eOYezYsXj37h1WrFiBDRs2oHnz5gCA1atX49ChQ1i7di3GjBmDZcuWwcPDA0uWLIEgCChdujSeP3+OcePGYcqUKShatChMTU1hYWEBFxcXjfrMmDED9evXBwCMHz8eLVu2RFJSEszMzDB16lSMHz8evXr1ks7nu+++w9ixYxESEoLDhw/j9u3bOHDgANzc3AAAM2fOlOqakZubGy5fvpzta0pEREREH5/79+9DFEWpD55XS5YsQUBAAGbOnCmlrVu3Dh4eHrh79y78/PwAAP7+/tL8576+vliyZAmOHDmCJk2awNFRGUy1sbHR2jd+X9quEVQsLCwQEhKCiRMnon///hqjIe/evQsAKFWqlJR2/vx5NGjQQHq+ZcsWtGrVCoDy+ichIQHBwcEAgO7du2Pt2rUaAUo3NzekpKTg5cuX8PT0fP+TzMJHF6D87bffEBwcjE6dOuGvv/6Cu7s7Bg4ciP79+2e5T3JystoQ2djYWADK0TQKhW7DjXNLoVBAFMUCK5/yhu1iuNg2ulO9VqqHipDNPjkRRTFXIyKlfXKhVKlSqF27NtatW4f69evj/v37OH78OKZOnQpRFJGeno6ZM2fi119/xbNnz5CSkoLk5GRYWFioHSvzeWurk7btN2/ehIeHB4oVKyZtL1OmDGxsbHDz5k1UrVoVI0aMQL9+/fDTTz+hUaNG6NSpE3x8fAAoA6YDBw7EwYMH0ahRI3z22WfZzpuZmJgIuVyuVpfsylDlq1ChgvR/JycnWFhYwNvbWy3t3LlzAIAHDx4gNTUVtWvXlrYbGxujevXquHnzJkRRxK1bt1CrVi21Y9SuXRvx8fH4999/Ubx4ca2vq7b6qDppr169QvHixXH16lWcPHkSM2bMkPZLT09HUlIS3r17J73mrq6uUhk1a9bUejwzMzMkJCTk+n1lqLJ7L+a2HNXfxsx/H/n3koiIiD4G+dX/u3r1Ko4dO6b1LqcHDx6oBSgzcnV1RURERL7UISeJiYkwMzPLcnvfvn0xb948zJ49Wy3QmhV/f3/pFndfX1+1qZY2bNiAzp07S3N0du3aFWPGjMGDBw+kaxwAMDc3BwC1aaQKwkcXoHz48CGWL1+OkSNHYuLEiTh//jyGDh0KU1NTaQRHZrNmzcLUqVM10iMjI5GUlFQg9VQoFIiJiYEoipDJPsqpQD9IbBfDxbbRXWpqKhQKBdLS0tS+gEzeo0yFQgFFhrIyyuqLJC2L/Nnp3bs3hg8fjoULF2Lt2rXw8fFBYGAg0tLSMGfOHPz444/44YcfUL58eVhaWmL06NFISkqSjqUK1mR1bFXARtv27Lalp6cjLS0NkydPRufOnfHnn3/iwIEDCA0Nxc8//4x27dqhd+/eaNSoEf78808cOnQI33//PebMmZPlpN329vZ48+aN2vGyK0O1Yp8gCGrna2JiolaGKpibmpoq7ZP5vaAKYqelpUmBrYzbVf9X7aftddVWH1VaSkoK0tLSEB8fjylTpqBdu3Ya529sbKz1Nc9YVsb0N2/ewMHBIU/vK0OjaiNA/TabvFC14Zs3b6TpBlTi4uLeq2wiIiIiQ+Dr6wtBEHD79u33Kic+Ph6tW7fG7NmzNba5urpK/8/cpxIEodB++HVwcMDbt2+z3G5sbIwZM2agd+/eGDx4sNo21d1dd+7ckX70l8vlKFmypEY5UVFR2LNnD1JTU7FixQopPT09HevWrVMbYKBaqFI1erSgfHQBSoVCgapVq0qR5ICAAFy/fh0rVqzIMkA5YcIEjBw5UnoeGxsLDw8PODo6wtrausDqKQgCHB0dGWwxIGwXw8W20V1SUhLi4uJgbGystmKdmGGxl9ySWVlBltXqdzdvIiUlReOLPC+r5X3++ecYOXIktm3bhl9++QUDBgyQyj1z5gzatGkj/S1XKBS4d+8eypYtKx1LtYhLVsdWvXe0bS9Xrhz+/fdfvHjxQrrF++bNm4iOjkaFChWkfcqWLYuyZcti1KhR+OKLL/DTTz+hY8eOAJRTigwcOBADBw7EhAkTsG7dOgwbNkxrXQICAnDnzh2NumRVhmp+m4ztqu18ZDIZBEGAiYkJfH19YWpqirNnz0q/gqampuLixYsYNmwYjI2NUbZsWezcuRNGRkZSsOzs2bMoUqQIvLy8IJPJIJfLoVAo1I6jrT4Z/zU2NkblypVx7969LG/HUb3mkZGRUqdQteiQkZGR2vFu3ryJ+vXr620VxoKQ+TOTF8bGxpDJZLC3t9f4tT27X9+JiIiI7C3sETG6YEcGqn7kNjY21vrDrL2FfY5l2NnZITg4GEuXLsXQoUM15qGMjo7WaR7KypUrY8eOHfDy8nqvPqWJiYn0Y3N+CwgIwM2bN7PN06lTJ8ydO1djoF3Tpk1hZ2eH2bNn57i45C+//IJixYph165dau1y8OBBzJs3D9OmTZP6+9evX0exYsXg4OCQx7PSzcfTy/9/rq6uKFu2rFpamTJlsGPHjiz3kcvlWif6lMlkBRoIEQShwI9Bucd2MVxsG92oAlSqh+R9FsnJhujgACEtDcii05EbRYoUQZcuXTBx4kTExsaiT58+Upm+vr7Yvn07Tp8+DVtbW8yfPx+vXr1C2bJl1Y6rcd4ZCIKA9PR0XL16VS1dLpejSZMmqFChArp3746FCxciLS0NAwcORP369VGtWjUkJiZizJgx6NixI7y9vfH06VOcP38en332GQRBwPDhw9G8eXP4+fnh7du3CAsLQ5kyZbKsS7NmzdCvXz8oFArpyz+7MlTlZP5/xn8z/9/KygoDBgzA2LFjYW9vj+LFi2POnDlISEhAv379IAgCBg0ahEWLFmHo0KEYPHgw7ty5g9DQUIwcOVKql5eXF86dO4fHjx/DysoKdnZ2OdZHEARMmTIFrVq1gqenJzp27AiZTIarV6/i+vXrmD59Opo0aQI/Pz/07t0bc+fORWxsLCZPnqxRbkJCAi5evIiZM2e+93vMEIiiqLXt8kL1Omn728i/lURERJQdmSDLcYGa95VTgFJXS5cuRWBgIKpXr45p06bB398faWlpOHToEJYvX45bt27lWMagQYOwevVqdO3aVVql+/79+9iyZQvWrFmj84I3Xl5eOHLkCAIDAyGXy7NclPnJkyeIiorCkydPkJ6eLt1qXbJkySwX0wwODka/fv2Qnp6ebX2+//57ae5IFSsrK6xZswZdunRBy5YtMXToUPj6+iI+Ph779+8H8N8gg3Xr1qF9+/YoX768Wrt4eHhgwoQJ2L9/P1q2bAlAuXBP06ZNdXpt3sdH13MNDAzEnTt31NLu3r1boBN5EhFR/unbty/evn2L4OBgaeEUAJg8eTIqV66M4OBgBAUFwcXFReutwzmJj49HQECA2qN169YQBAF79uyBra0t6tWrh8aNG6NEiRLYunUrAOWX+Zs3b9CzZ0/4+fmhc+fOaN68ufTLZXp6OgYNGoQyZcqgWbNm8PPzw7Jly7KsR/PmzWFsbIzDhw9LabktQxfff/89PvvsM/To0QOVK1fG/fv3ceDAAakj5e7ujn379uHcuXOoWLEiBgwYgL59+0qBQgAYPXo0jIyMULZsWTg6OuLJkyc6HTs4OBh//PEHDh48iGrVqqFmzZpYsGCB9J0sk8mwa9cuJCYmonr16ujXr5/a7SQqe/bsQfHixVG3bt33ei2IiIiI6MNUokQJXLp0CQ0aNMCoUaNQvnx5NGnSBEeOHMHy5ct1KsPNzQ0nT55Eeno6mjZtigoVKmD48OGwsbHJ1Q+78+bNw6FDh+Dh4YGAgIAs802ZMgUBAQEICQlRuwZR3TGkjbZrBG0aNmyIhg0bakx/1L59e5w6dQoWFhbo2bMnSpUqhYYNG+Lo0aPSAjkXL17E1atX0aFDB41yixYtikaNGmHt2rUAlHfn7d69O9t1XfKLIH4ss83/v/Pnz6N27dqYOnUqOnfujHPnzqF///5YtWoVunXrplMZsbGxKFq0KGJiYgr0Fu+IiAg4OTlxhIMBYbsYLraN7pKSkvDo0SN4e3sXyi2e+fWr6Kdo6dKl+O2333DgwIF8L/tjapeaNWti6NCh+OKLL/RdlXyRn22T3ee9MPozVHDYH/20sW0ME9vFcLFtdMdrBcNXkNcIKrq2y/Lly7Fr1y4cPHgwyzz51R/96G7xrlatGnbt2oUJEyZg2rRp8Pb2xsKFC3UOThIRERWWr7/+GtHR0YiLi0ORIkX0XR2D9Pr1a3To0AFdu3bVd1WIiIiIiAqcIV0jmJiYYPHixYVyrI8uQAkArVq1QqtWrfRdDSIiomwZGxtj0qRJ+q6GQXNwcMDYsWP1XQ0iIiIiokJhSNcI/fr1K7RjcewzERERERERERER6Q0DlERERERERERERKQ3DFASEX2kPrI10IhIC37OiYiIiOhjwAAlEdFHxsjICACQkpKi55oQUUFLSEgAoJzAnIiIiIjoQ/VRLpJDRPQpMzY2hoWFBSIjI2FiYgKZrGB/ixJFEWlpaTA2NoYgCAV6LNId28Vw5UfbiKKIhIQEREREwMbGRvphgoiIiIjoQ8QAJRHRR0YQBLi6uuLRo0d4/PhxgR9PFEUoFArIZDIGwgwI28Vw5Wfb2NjYwMXFJZ9qRkRERESkHwxQEhF9hExNTeHr61sot3krFAq8efMG9vb2BT5ak3THdjFc+dU2JiYmHDlJRERERB8FBiiJiD5SMpkMZmZmBX4chUIBExMTmJmZMRBmQNguhottQ0RERFQwBEHArl270K5dO4SHh8Pb2xuXL19GpUqV9F01nR05cgSDBw/G9evX9fZjdEpKCvz8/LB9+3ZUrVq1UI7JXjERERERERERERm0ly9fYsiQIShRogTkcjk8PDzQunVrHDlyRGt+Dw8PvHjxAuXLl8/XegiCgN27d2ebJzw8HH379oW3tzfMzc3h4+ODkJAQne5wGzt2LCZPniwFJzds2ABBENCsWTO1fNHR0RAEAWFhYWp10/bYsmULACAsLAyCIKB8+fJIT09XK8/GxgYbNmwAoLwjb/To0Rg3blyO9c0vDFASEREREREREZHBCg8PR5UqVXD06FHMnTsX165dw/79+9GgQQMMGjRI6z5GRkZwcXGBsXHh3zx8+/ZtKBQKrFy5Ejdu3MCCBQuwYsUKTJw4Mdv9Tpw4gQcPHuCzzz5TSzc2Nsbhw4dx7NixHI+9fv16vHjxQu3Rrl07tTwPHz7Ezz//nG053bp1w4kTJ3Djxo0cj5kfeIs3EREREREREdGnRqEA3rwp2GOIIpCWBhgbA9oWB7S3B3SY8mbgwIEQBAHnzp2DpaWllF6uXDl8+eWXWvfRdov39evXMWbMGBw/fhyWlpZo2rQpFixYAAcHBwBAUFAQ/P39YWZmhjVr1sDU1BQDBgxAaGgoAMDLywsA0L59ewCAp6cnwsPDNY7drFkztRGPJUqUwJ07d7B8+XL88MMPWZ7nli1b0KRJE42puiwtLdG5c2eMHz8eZ8+ezfa10mURxcGDB2PatGno3r17ltOC2draIjAwEFu2bMF3332XbXn5gSMoiYiIiIh0sHTpUnh5ecHMzAw1atTAuXPnss2/cOFClCpVCubm5vDw8MCIESOQlJRUSLUlIiLKwZs3gJNTgT4EZ2eYuLtDcHbWnkeHAGlUVBT279+PQYMGqQUnVWxsbHQ63ejoaDRs2BABAQG4cOEC9u/fj1evXqFz585q+TZu3AhLS0ucPXsWc+bMwbRp03Do0CEAwPnz5wH8N0pR9VwXMTExsLOzyzbP8ePHs5zzMTQ0FNeuXcP27dt1PmZWhg8fjrS0NCxevDjbfNWrV8fx48ff+3i6YICSiIiIiCgHW7duxciRIxESEoJLly6hYsWKCA4ORkREhNb8mzZtwvjx4xESEoJbt25h7dq12Lp1a463dhEREZG6+/fvQxRFlC5d+r3KWbJkCQICAjBz5kyULl0aAQEBWLduHY4dO4a7d+9K+fz9/RESEgJfX1/07NkTVatWlea5dHR0BPDfKEXVc13OYfHixfj666+zzff48WO4ublp3ebm5oZhw4Zh0qRJSEtLy7KMrl27wsrKSu3x5MkTtTwWFhaYPHkyvv/+e8TExGRZlpubGx4/fpxtnfMLA5RERERERDmYP38++vfvjz59+qBs2bJYsWIFLCwssG7dOq35T506hcDAQHzxxRfw8vJC06ZN0bVr1xxHXRIREZE6URTzpZyrV6/i2LFjaoE7VdDzwYMHUj5/f3+1/VxdXbP8QVIXz549Q7NmzdCpUyf0798/27yJiYlZ3nINAOPGjUNkZGSW/Q8AWLBgAa5cuaL20Bb07NOnD+zt7TF79uwsyzI3N0dCQkK2dc4vDFASEREREWUjJSUFFy9eROPGjaU0mUyGxo0b4/Tp01r3qV27Ni5evCgFJB8+fIh9+/ahRYsWhVJnIiKij4Wvry8EQcDt27ffq5z4+Hi0bt1aI3h379491KtXT8pnYmKitp8gCFAoFHk65vPnz9GgQQPUrl0bq1atyjG/g4MD3r59m+V2GxsbTJgwAVOnTs0ycOji4oKSJUuqPbQtFGRsbIzp06dj0aJFeP78udayoqKidB4l+r64SA4RERERUTZev36N9PR0ODs7q6U7OztnebH0xRdf4PXr16hTpw5EUURaWhoGDBiQ5S3eycnJSE5Olp7HxsYCABQKRZ4vinKiUCggimKBlU95x7YxTGwXw8W20Z3qtRJFEaKdHfDqVYEfMzU1VSPoJ7GzUy6kkw1bW1sEBwdj6dKlGDJkiMY8lNHR0WrzUErn9//lqv4fEBCAnTt3wtPTU2vALnP+rLabmJggLS0tx5Gdz549Q8OGDVGlShWsW7cOgiDkuE9AQABu3Lihli9jvQDlAjc//vgjFi5cqLW+WdU/Yxmq/3fq1Ak//PCDtAhQ5n2vX7+OgICAbOut2kdbnyU3n0kGKImIiIiI8llYWBhmzpyJZcuWoUaNGrh//z6GDRuG7777Dt9++61G/lmzZmHq1Kka6ZGRkQW2sI5CoUBMTAxEUYRMhxVUqfCwbQwT28VwsW10l5qaCoVCgbS0NKQZGwO2tgV6PFEUkZ6eDhgZQdC2irdCoXzkYOHChQgKCkL16tUREhKCChUqIC0tDUeOHMHKlStx7do1KW96erry/P5/nkbV/7/++musWbMGn3/+OUaPHg1bW1s8ePAA27Ztw8qVK2FkZCQF2zLO8agKvKnSPD09cfjwYdSoUQNyuRy2Wl7DZ8+eoUmTJihevDhmzZqFFy9eSNuyW2G7cePG+OmnnzSOrzoPQDnyccqUKRg6dKja+apERUXh6dOnauUWKVIElpaWyraA8n2g+v/06dPRsmVL6VgZyzp+/DhCQkKynfMyLS0NCoUCb9680QhEx8XFZblfZgxQEhERERFlw8HBAUZGRniVaZTJq1evsrzI+Pbbb9GjRw/069cPAFChQgW8e/cOX331FSZNmqRxAT1hwgSMHDlSeh4bGwsPDw84OjrC2to6n89ISaFQQBAEODo68oLewLBtDBPbxXCxbXSXlJSEuLg4GBsbax1FWFCyHEGpIz8/P1y8eBEzZszAuHHj8OLFCzg6OqJKlSpYvny52rkYGRmpnZ/q/8WLF8eJEycwfvx4tGjRAsnJyfD09ERwcDBMTU0hCIL0yFieTCaDTCaT0ubNm4dRo0Zh7dq1cHd3x6NHjzTqe+zYMdy/fx/379+Ht7e32rbsRhX26NEDEyZMwIMHD1CqVCnp+KrzUOnTpw8WLlyImzdvSueroup7ZDRz5kyMHz8eRkZGAJTtYWRkBBMTEzRp0gQNGzbEwYMH1c7z9OnTiImJQZcuXbJ9rxgbG0Mmk8He3l5j/szs5tPUKEfnnEREREREnyBTU1NUqVIFR44cQbt27QAoLy6OHDmCwYMHa90nISFB4yJZdVGg7TYpuVwOuVyuka66KCoogiAU+DEob9g2hontYrjYNrqRyWRqgbiCJoqidJz3PZ6bmxuWLl2KpUuXZns8FW9vb43vXD8/P+zcuTPL/cPCwjTSdu/erfa8TZs2aNOmTbZ17dOnD/r06ZNtHm3s7e0xePBgLFiwACtXrsyyLGNjY9y4cUNj/5xuIW/QoIHGKFFBEHDgwAGNvIsWLcKYMWNgYWGRbZmq95K2z19uPo/85BIRERER5WDkyJFYvXo1Nm7ciFu3buGbb77Bu3fvpAuGnj17YsKECVL+1q1bY/ny5diyZQsePXqEQ4cO4dtvv0Xr1q2lQCURERFRZpMmTYKnp6de51RNSUlBhQoVMGLEiEI7JkdQEhERERHloEuXLoiMjMSUKVPw8uVLVKpUCfv375cWznny5InaKIHJkydDEARMnjwZz549g6OjI1q3bo0ZM2bo6xSIiIjoA2BjY5PlonqFxdTUFJMnTy7UYzJASURERESkg8GDB2d5S3fmW8KMjY0REhKCkJCQQqgZERER0YeNt3gTERERERERERGR3jBASURERERERERERHrDACURERERERERERHpDQOUREREREREREREpDcMUBIREREREREREZHeMEBJREREREREREREesMAJRERERERERERkQ6CgoIwfPjwHPPVq1cPmzZtKvD6fP7555g3b16BH6egMUBJREREREREREQGq3fv3hAEQePRrFkzfVdNq99++w2vXr3C559/LqWtWrUKQUFBsLa2hiAIiI6O1rpvYmIiLC0tcf/+fZw4cQKBgYGwt7eHubk5SpcujQULFqjlnzx5MmbMmIGYmJiCPKUCZ6zvChAREREREREREWWnWbNmWL9+vVqaXC7XU22y9+OPP6JPnz6Qyf4bF5iQkIBmzZqhWbNmmDBhQpb7Hjp0CJ6enihZsiTi4uIwePBg+Pv7w9LSEidOnMDXX38NS0tLfPXVVwCA8uXLw8fHBz///DMGDRpU4OdWUDiCkoiIiIiIiIiIDJpcLoeLi4vaw9bWVtp+79491KtXD2ZmZihbtiwOHToEQRCwe/duAEBYWJjGyMUrV65AEASEh4cDAN68eYOuXbvC3d0dFhYWqFChAjZv3pyrekZGRuLo0aNo3bq1Wvrw4cMxfvx41KxZM9v99+zZgzZt2gAAAgIC0LVrV5QrVw5eXl7o3r07goODcfz4cbV9WrdujS1btuSqnoaGAUoiIiIiIiIiIvpgKRQKdOjQAaampjh79ixWrFiBcePG5bqcpKQkVKlSBXv37sX169fx1VdfoUePHjh37pzOZZw4cQIWFhYoU6ZMro+vUCjwxx9/oG3btlq3X758GadOnUL9+vXV0qtXr45z584hOTk518c0FLzFm4iIiIiIiIjoE3b79m3cvn0bAFCrVi04OztL2+Lj43H48GEAQLFixVC1alW1ff/66y+8ffsWANCuXTu1bQ8fPsTVq1chk8lQpUoVeHh45LmOf/zxB6ysrNTSJk6ciIkTJ+Lw4cO4ffs2Dhw4ADc3NwDAzJkz0bx581wdw93dHaNHj5aeDxkyBAcOHMC2bdtQvXp1ncp4/PgxnJ2d1W7v1tWZM2cAADVq1FBLL1asGCIjI5GWlobQ0FD069dPbbubmxtSUlLw8uVLeHp65vq4hoABSiIiIiIiIiKiT1hqaioSExMBKEfxZSSKorQtJSVFY9/k5GRpe2ZpaWlITEyEIAhIS0t7rzo2aNAAy5cvV0uzs7MDANy6dQseHh5ScBJQBlpzKz09HTNnzsS2bdvw7NkzpKSkIDk5GRYWFjqXkZiYCDMzs1wfG1De3t2qVSuN4Obx48cRHx+PM2fOYPz48ShZsiS6du0qbTc3NwegnOfyQ8UAJRERERERERHRJ8zExEQKcmUOjgmCIG0zNTXV2Fcul0vbMzM2Noa5uTlkMhmMjd8vBGVpaYmSJUvmeX/VeYmiKKWlpqaq5Zk7dy4WLVqEhQsXokKFCrC0tMTw4cO1Bmaz4uDgII0oza3ffvsN33//vUa6t7c3AKBChQp49eoVQkND1QKUUVFRAABHR8c8HdcQMEBJRERERERERPQJK126NEqXLq11m5WVlcat2xllng8xoxIlSqB48eIwNjaGIAjvW80slSlTBv/++y9evHgBV1dXAP/dLq2iCt69ePFCWlznypUranlOnjyJtm3bonv37gCUo0nv3r2LsmXL6lyXgIAAvHz5Em/fvlVbxCcn9+7dw+PHj9GkSZNs8ykUCo25Jq9fv45ixYrBwcFB5+MZmo9ykZzQ0FAIgqD2yOqDRkREREREREREhi05ORkvX75Ue7x+/RoA0LhxY/j5+aFXr164evUqjh8/jkmTJqntX7JkSXh4eCA0NBT37t3D3r17MW/ePLU8vr6+OHToEE6dOoVbt27h66+/xqtXr3JVz4CAADg4OODkyZNq6S9fvsSVK1dw//59AMC1a9dw5coVafTjnj170LhxY7XbyZcuXYrff/8d9+7dw71797B27Vr88MMPUgBV5fjx42jatGmu6mloPsoAJQCUK1cOL168kB4nTpzQd5WIiIiIiIiIiCgP9u/fD1dXV7VHnTp1AChv3961axcSExNRvXp19OvXDzNmzFDb38TEBJs3b8bt27fh7++P2bNnY/r06Wp5Jk+ejMqVKyM4OBhBQUFwcXHJdvSoNkZGRujTpw9++eUXtfQVK1YgICAA/fv3BwDUq1cPAQEB+O233wAoA5Rt2rRR20ehUGDChAmoVKkSqlatiqVLl2L27NmYNm2alCcpKQm7d++Wyv1QfbS3eBsbG8PFxUXf1SAiIiIiIiIiovewYcMGbNiwIds8fn5+OH78eLZ5AgMD8c8//6ilZZyT0s7ODrt37862jLCwsGy3A8CIESNQrlw5PH78WFpVOzQ0FKGhoVrzv379GmfOnMH27dvV0ocMGYIhQ4Zke6z169ejevXqqFmzZo71MmQf7QjKe/fuwc3NDSVKlEC3bt3w5MkTfVeJiIiIiIiIiIg+ci4uLli7dq3OsaioqCjMnz8fzs7OuT6WiYkJFi9enOv9DM1HOYKyRo0a2LBhA0qVKoUXL15g6tSpqFu3Lq5fv44iRYpo5E9OTlabYDQ2NhaAciitQqEokDoqFAqIolhg5VPesF0MF9vGcLFtDBPbxXAVVtuw7YmIiIj0Jze3hvv5+cHPzy9Px+nXr1+e9jM0H2WAsnnz5tL//f39UaNGDXh6emLbtm3o27evRv5Zs2Zh6tSpGumRkZFISkoqkDoqFArExMRAFEVpqXvSP7aL4WLbGC62jWFiuxiuwmqbuLi4AiubiIiI6EOQ8fZtMmwfZYAyMxsbG/j5+UkrJWU2YcIEjBw5UnoeGxsLDw8PODo6wtraukDqpFAoIAgCHB0deeFoQNguhottY7jYNoaJ7WK4CqttzMzMCqxsIiIiIqL89EkEKOPj4/HgwQP06NFD63a5XA65XK6RLpPJCvTCQRCEAj8G5R7bxXCxbQwX28YwsV0MV2G0DdudiIiIssKRhZRf8uu99FH2XEePHo2//voL4eHhOHXqFNq3bw8jIyN07dpV31UjIiIiIiIiItILIyMjAEBKSoqea0Ifi4SEBADKxXrex0c5gvLp06fo2rUr3rx5A0dHR9SpUwdnzpyBo6OjvqtGRERERERERKQXxsbGsLCwQGRkJExMTAr8jgtRFJGWlgZjY2MIglCgxyLd5Ue7iKKIhIQEREREwMbGRgp+59VHGaDcsmWLvqtARERERERERGRQBEGAq6srHj16hMePHxf48URRhEKhgEwmY4DSgORnu9jY2MDFxeW96/RRBiiJiIiIiIiIiEiTqakpfH19C+U2b4VCgTdv3sDe3p7zYxuQ/GoXExOT9x45qcIAJRERERERERHRJ0Qmk8HMzKzAj6NQKGBiYgIzMzMGKA2IIbaLYdSCiIiIiIiIiIiIPkkMUBIREREREREREZHeMEBJREREREREREREesMAJREREREREREREekNA5RERERERERERESkN1zFWw9u3wauXgXu3LFAbKyAGjWATp30XSsiIiIiIiIiIqLCxwClHixdCixZIgNgDQDo3ZsBSiIiIiIiIiIi+jTxFm89cHdXf/7smX7qQUREREREREREpG8MUOoBA5RERERERERERERKDFDqQbFi6s8ZoCQiIiIiIiIiok8VA5R6kHkEZUwM8O6dfupCRERERERERESkTwxQ6kHmACXAUZRERERERERERPRpYoBSDywtgaJFRbU0BiiJiIiIDNvSpUvh5eUFMzMz1KhRA+fOncs2f3R0NAYNGgRXV1fI5XL4+flh3759hVRbIiIiog+Hsb4r8Klyd1fe2q3CACURERGR4dq6dStGjhyJFStWoEaNGli4cCGCg4Nx584dODk5aeRPSUlBkyZN4OTkhO3bt8Pd3R2PHz+GjY1N4VeeiIiIyMAxQKknbm7AzZv/PWeAkoiIiMhwzZ8/H/3790efPn0AACtWrMDevXuxbt06jB8/XiP/unXrEBUVhVOnTsHExAQA4OXlVZhVJiIiIvpg8BZvPck8DyUDlERERESGKSUlBRcvXkTjxo2lNJlMhsaNG+P06dNa9/ntt99Qq1YtDBo0CM7OzihfvjxmzpyJ9PT0wqo2ERER0QeDIyj1hAFKIiIiog/D69evkZ6eDmdnZ7V0Z2dn3L59W+s+Dx8+xNGjR9GtWzfs27cP9+/fx8CBA5GamoqQkBCN/MnJyUhOTpaex8bGAgAUCgUUCkU+ns1/FAoFRFEssPIp79g2hontYrjYNoaLbWOYCqtdclM+A5R64u4uAhCk5+HheqsKEREREeUzhUIBJycnrFq1CkZGRqhSpQqePXuGuXPnag1Qzpo1C1OnTtVIj4yMRFJSUoHVMSYmBqIoQibjjVWGhG1jmNguhottY7jYNoapsNolLi5O57wMUOpJyZLqz+/eBUQREATt+YmIiIhIPxwcHGBkZIRXr16ppb969QouLi5a93F1dYWJiQmMjIyktDJlyuDly5dISUmBqampWv4JEyZg5MiR0vPY2Fh4eHjA0dER1tbW+Xg2/1EoFBAEAY6OjrxoNDBsG8PEdjFcbBvDxbYxTIXVLmZmZjrnZYBST0qVUn8eHw+8fAm4uuqnPkRERESknampKapUqYIjR46gXbt2AJQd+yNHjmDw4MFa9wkMDMSmTZugUCikjv/du3fh6uqqEZwEALlcDrlcrpEuk8kK9MJBEIQCPwblDdvGMLFdDBfbxnCxbQxTYbRLbsrmu0NP3N0BMzNRLe3OHT1VhoiIiOgj9OTJE4iiqJEuiiKePHmSq7JGjhyJ1atXY+PGjbh16xa++eYbvHv3TlrVu2fPnpgwYYKU/5tvvkFUVBSGDRuGu3fvYu/evZg5cyYGDRr0fidFRERE9BHiCEo9kckAH5803LhhIqXdvQsEBemvTkREREQfE29vb7x48QJOTk5q6VFRUfD29s7VitpdunRBZGQkpkyZgpcvX6JSpUrYv3+/tHDOkydP1EYJeHh44MCBAxgxYgT8/f3h7u6OYcOGYdy4cflzckREREQfEQYo9ahECc0AJRERERHlD1EUIWiZ4Ds+Pj5XcyKpDB48OMtbusPCwjTSatWqhTNnzuT6OERERESfGgYo9ahECfVf7XmLNxEREdH7Uy02IwgCvv32W1hYWEjb0tPTcfbsWVSqVElPtSMiIiKizBig1KOSJdPUnl+7pqeKEBEREX1ELl++DEA5gvLatWtqi9KYmpqiYsWKGD16tL6qR0RERESZMECpR+XKqQcoHz8GoqIAOzs9VYiIiIjoI3Ds2DEAQJ8+fbBo0SJYW1vruUZERERElB2u4q1HJUumQS5XX1nyyhX91IWIiIjoY7N+/XoGJ4mIiIg+ABxBqUcmJkCFCsCFC/+lXboENGyovzoRERERfSzevXuH77//HkeOHEFERAQUCoXa9ocPH+qpZkRERESUEQOUelapknqA8v+nTCIiIiKi99SvXz/89ddf6NGjB1xdXbWu6E1ERERE+scApZ5VqiQC+K+znDFYSURERER59+eff2Lv3r0IDAzUd1WIiIiIKBsGNQflv//+i6dPn0rPz507h+HDh2PVqlV6rFXBqlZN/fndu8qFcoiIiIjo/dja2sKOqw8SERERGTyDClB+8cUX0qqLL1++RJMmTXDu3DlMmjQJ06ZN03PtCoa/P2Bmpp525ox+6kJERET0Mfnuu+8wZcoUJCQk6LsqRERERJQNg7rF+/r166hevToAYNu2bShfvjxOnjyJgwcPYsCAAZgyZYqea5j/TE2BqlWBEyf+Szt9GmjRQn91IiIiIvpQBQQEqM01ef/+fTg7O8PLywsmJiZqeS9dulTY1SMiIiIiLQwqQJmamgq5XA4AOHz4MNq0aQMAKF26NF68eKHPqhWoWrXUA5SnTumvLkREREQfsnbt2um7CkRERESUSwYVoCxXrhxWrFiBli1b4tChQ/juu+8AAM+fP4e9vb2ea1dwatdWf376NJCcDPx/rJaIiIiIdBQSEqLvKhARERFRLhnUHJSzZ8/GypUrERQUhK5du6JixYoAgN9++0269ftjVK8ekOFOJCQmKoOUREREREREREREHzuDClAGBQXh9evXeP36NdatWyelf/XVV1ixYkWeyvz+++8hCAKGDx+eT7XMf3Z2QOXK6mmHD+unLkREREQfC9Uq3pkf9vb2cHd3R/369bF+/Xp9V5OIiIjok2dQAcrExEQkJyfD1tYWAPD48WMsXLgQd+7cgZOTU67LO3/+PFauXAl/f//8rmq+a9xY/fn/L2ZORERERHk0ZcoUyGQytGzZElOnTsXUqVPRsmVLyGQyDBo0CH5+fvjmm2+wevVqfVeViIiI6JNmUHNQtm3bFh06dMCAAQMQHR2NGjVqwMTEBK9fv8b8+fPxzTff6FxWfHw8unXrhtWrV2P69OkFWOv8UacOMHv2f88fPdJfXYiIiIg+BidOnMD06dMxYMAAtfSVK1fi4MGD2LFjB/z9/fHjjz+if//+eqolERERERlUgPLSpUtYsGABAGD79u1wdnbG5cuXsWPHDkyZMiVXAcpBgwahZcuWaNy4cY4ByuTkZCQnJ0vPY2NjAQAKhQIKhSIPZ5IzhUIBURSl8h0dgYwDWqOiRKSni2pzU1LBy9wuZDjYNoaLbWOY2C6Gq7Dahm0PHDhwALMz/gL8/xo1aoRRo0YBAFq0aIHx48cXdtWIiIiIKAODClAmJCSgSJEiAICDBw+iQ4cOkMlkqFmzJh4/fqxzOVu2bMGlS5dw/vx5nfLPmjULU6dO1UiPjIxEUlKSzsfNDYVCgZiYGIiiCJlMBlE0AuAobU9OFvD48StYWBTI4SkLmduFDAfbxnCxbQwT28VwFVbbxMXFFVjZHwo7Ozv8/vvvGDFihFr677//Djs7OwDAu3fvpP4nEREREemHQQUoS5Ysid27d6N9+/Y4cOCA1JmMiIiAtbW1TmX8+++/GDZsGA4dOgQzMzOd9pkwYQJGjhwpPY+NjYWHhwccHR11Pm5uKRQKCIIAR0dHyGQymJho5jEyckIept6k95C5XchwsG0MF9vGMLFdDFdhtY2u/aCP2bfffotvvvkGx44dQ/Xq1QEo5yjft2+ftADjoUOHUL9+fX1Wk4iIiOiTZ1AByilTpuCLL77AiBEj0LBhQ9SqVQuAcjRlQECATmVcvHgRERERqJxhWez09HT8/fffWLJkCZKTk2FkZKS2j1wuh1wu1yhLJpMV6IWDIAjSMWxtAZkMyHg3VnS0DJ6eBXZ4ykLGdiHDwrYxXGwbw8R2MVyF0TZsd6B///4oW7YslixZgp07dwIASpUqhb/++gu1a9cGAOlWbyIiIiLSH4MKUHbs2BF16tTBixcvULFiRSm9UaNGaN++vU5lNGrUCNeuXVNL69OnD0qXLo1x48ZpBCcNhUwG2NoCb978lxYVpb/6EBEREX0MAgMDERgYqO9qEBEREVE2DCpACQAuLi5wcXHB06dPAQDFihWTbsnRRZEiRVC+fHm1NEtLS9jb22ukGxo7O/UAZcb/ExEREVHOYmNjpSl6VAsfZqWgpvIhIiIiotwxqHt/FAoFpk2bhqJFi8LT0xOenp6wsbHBd99990msRGlvr/6cIyiJiIiIcsfW1hYREREAABsbG9ja2mo8VOlEREREZBgMagTlpEmTsHbtWnz//ffSrTgnTpxAaGgokpKSMGPGjDyVGxYWlo+1LDj/v5ikhAFKIiIiotw5evSotEL3sWPH9FwbIiIiItKFQQUoN27ciDVr1qBNmzZSmr+/P9zd3TFw4MA8Byg/FJkDlLzFm4iIiCh3Mq7IzdW5iYiIiD4MBnWLd1RUFEqXLq2RXrp0aUR9AsMJeYs3ERERUf46fvw4unfvjtq1a+PZs2cAgJ9++gknTpzQc82IiIiISMWgApQVK1bEkiVLNNKXLFkCf39/PdSocPEWbyIiIqL8s2PHDgQHB8Pc3ByXLl1CcnIyACAmJgYzZ87Uc+2IiIiISMWgbvGeM2cOWrZsicOHD6NWrVoAgNOnT+Pff//Fvn379Fy7gsdbvImIiIjyz/Tp07FixQr07NkTW7ZskdIDAwMxffp0PdaMiIiIiDIyqBGU9evXx927d9G+fXtER0cjOjoaHTp0wI0bN/DTTz/pu3oFjrd4ExEREeWfO3fuoF69ehrpRYsWRXR0dOFXiIiIiIi0MqgRlADg5uamsRjO1atXsXbtWqxatUpPtSocmQOUERH6qQcRERHRx8DFxQX379+Hl5eXWvqJEydQokQJ/VSKiIiIiDQY1AjKT52bm/rzyEjg/6dKIiIiIqJc6t+/P4YNG4azZ89CEAQ8f/4cv/zyC0aPHo1vvvlG39UjIiIiov9ncCMoP2UeHpppT58CPj6FXxciIiKiD9WjR4/g7e2N8ePHQ6FQoFGjRkhISEC9evUgl8sxevRoDBkyRN/VJCIiIqL/xwClASlaFChSBIiL+y/t338ZoCQiIiLKDR8fH3h6eqJBgwZo0KABbt26hbi4OMTHx6Ns2bKwsrLSdxWJiIiIKAODCFB26NAh2+2f0iTmHh7AzZv/Pf/3X/3VhYiIiOhDdPToUYSFhSEsLAybN29GSkoKSpQogYYNG6Jhw4YICgqCs7OzvqtJRERERP/PIAKURYsWzXF7z549C6k2+lWsmHqA8ulT/dWFiIiI6EMUFBSEoKAgAEBSUhJOnTolBSw3btyI1NRUlC5dGjdu3NBvRYmIiIgIgIEEKNevX6/vKhiMzPNQcgQlERERUd6ZmZmhYcOGqFOnDho0aIA///wTK1euxO3bt/VdNSIiIiL6fwYRoKT/MEBJRERE9P5SUlJw5swZHDt2DGFhYTh79iw8PDxQr149LFmyBPXr19d3FYmIiIjo/zFAaWAYoCQiIiJ6Pw0bNsTZs2fh7e2N+vXr4+uvv8amTZvg6uqq76oRERERkRYMUBqYzAHKJ0/0Uw8iIiKiD9Xx48fh6uoqLYhTv3592Nvb67taRERERJQFmb4rQOo8PdWfv30LxMTopy5EREREH6Lo6GisWrUKFhYWmD17Ntzc3FChQgUMHjwY27dvR2RkpL6rSEREREQZMEBpYDw9AUFQT3v0SD91ISIiIvoQWVpaolmzZvj+++9x9uxZvH79GnPmzIGFhQXmzJmDYsWKoXz58rkud+nSpfDy8oKZmRlq1KiBc+fO6bTfli1bIAgC2rVrl+tjEhEREX0KGKA0MHI54OamnsYAJREREVHeWVpaws7ODnZ2drC1tYWxsTFu3bqVqzK2bt2KkSNHIiQkBJcuXULFihURHByMiIiIbPcLDw/H6NGjUbdu3fc5BSIiIqKPGgOUBsjbW/15eLheqkFERET0QVIoFDh37hzmzJmD5s2bw8bGBrVr18ayZcvg4uKCpUuX4uHDh7kqc/78+ejfvz/69OmDsmXLYsWKFbCwsMC6deuy3Cc9PR3dunXD1KlTUaJEifc9LSIiIqKPFhfJMUDe3sCJE/895whKIiIiIt3Z2Njg3bt3cHFxQYMGDbBgwQIEBQXBx8cnT+WlpKTg4sWLmDBhgpQmk8nQuHFjnD59Osv9pk2bBicnJ/Tt2xfHjx/P07GJiIiIPgUMUBqgzCMoGaAkIiIi0t3cuXPRoEED+Pn55Ut5r1+/Rnp6OpydndXSnZ2dcfv2ba37nDhxAmvXrsWVK1d0OkZycjKSk5Ol57GxsQCUo0EVCkXeKp4DhUIBURQLrHzKO7aNYWK7GC62jeFi2ximwmqX3JTPAKUBYoCSiIiIKO++/vprvR4/Li4OPXr0wOrVq+Hg4KDTPrNmzcLUqVM10iMjI5GUlJTfVQSgvGiIiYmBKIqQyTjzkyFh2xgmtovhYtsYLraNYSqsdomLi9M5LwOUBijzFEV37wJxcUCRIvqpDxEREdGnzMHBAUZGRnj16pVa+qtXr+Di4qKR/8GDBwgPD0fr1q2lNNUIAmNjY9y5c0fjdvMJEyZg5MiR0vPY2Fh4eHjA0dER1tbW+Xk6anUSBAGOjo68aDQwbBvDxHYxXGwbw8W2MUyF1S5mZmY652WA0gAFBAAmJkBqqvJ5aipw+DDQvr1+60VERET0KTI1NUWVKlVw5MgRtGvXDoCyY3/kyBEMHjxYI3/p0qVx7do1tbTJkycjLi4OixYtgoeHh8Y+crkccrlcI10mkxXohYMgCJABkEVFAaamgJUVwAtIgyAIQoG3P+Ue28VwsW0MF9vGMBVGu+SmbAYoDVCRIkC9esCRI/+l7d3LACURERGRvowcORK9evVC1apVUb16dSxcuBDv3r1Dnz59AAA9e/aEu7s7Zs2aBTMzM5QvX15tfxsbGwDQSNer1FRYLlgAYf16IDJSmWZlBXz7LTBmDHDnDrBpk3Jbly5AUJBeq/uhefcO+OMP4Phx4MEDQBSBzz8HevUCBEHftSMiIjIsDFAaqJYt1QOUf/6p7NSwM0NERERU+Lp06YLIyEhMmTIFL1++RKVKlbB//35p4ZwnT558WCNDrl2D0Ls3ily6pJ4eHw+MG6d8ZLRihTK6tn49kIvbtT5Fb98Cq1YB338PREerbztwALh/H5g+XS9VIyIiMlgMUBqo5s2BDNMQ4flz4OlTQMsdQURERERUCAYPHqz1lm4ACAsLy3bfDRs25H+F3sePP0LIHJzMyZYtQFSU8tYeY15GZHbhAjB8OHDyZPb5ZswAqlcH2rQplGoREeksJQVQKPg7FOnHB/Qz76fFzw8oWlQ97fx5/dSFiIiIiD4yc+dCdHfP/X4HDwLz5+d/fT5wv/0G1KmTc3BSZcoUZRCAiMhQbNsGeHkpZ/po2hRYvhyIiNB3rehTwgClgZLJgKpV1dMYoCQiIiKifGFjg1VfKzubNxyBRVOCkbpsCWBpmfO+ISHAP/8UcAWzFh6unNvxjz+ApCS9VUNy6xbwxRdAcrL27U2bAt7e6mlXryqDAURE+iaKwKxZyqmGX7wA0tOBQ4eAgQMBHx9g6VJ915A+Fbw3w4BVq6Y+DyUDlERERESUH7Zc34IBij042Bn4ww9IkR3A8vRwjFvSD403HofbpXswkpsB48crO6X16yuvYgFlVLBVK+UVbKlSyvTUVOUq4Fl5+hS4eVM5AePjx4CNDVC+vLJsc3Ng61blMEQLC2DECKBWLbXdFQogLEy5fs+pU/+llyihDPRVqfJ+r0dcnDLg+eSJ8pRatgRMTHLeLyVFOTXnu3ea2z77TDmVZ7Vqygv+ChWUwUyVkSOBRo2Aa9eU51S7NtCw4fudBxWMV/GvcOrfU0hOS4aHiQccHR31XSUyBHFxyollHz9W/m1LTlZG9D77DPD1BYyM9F3DbImiclB8aChw5oz2PPHxwODByj/hM2dyTQwqWAxQGrBq1dSfnz+v7Pvp0lkiIiIiIsrKtVfXAAA7y/6XdufNHXz55g4QBCAIaOpTC6tbd0TxosWBIUOAH3/8L/O//wKlSwNlygBv3ijvA2zQANiwAShe/L98MTFAv37A9u26V+7XX4EiRSCWLo2YiGREvDbCxaRyuJFeCuXghHjUwD+oCAB4+BCoWxfYvVs5UjEvnj1TLlB+//5/aSVLKhcy79FDGT/NyrRpmoNJO3UC1q1T3iapYmQEfPcd0LHjf2kvXgBOTur7tmihnIseACwt5WjYUPlyfspBgQP3D2DdlXW49OISktKSEOwTjPnB82Ett85dQaKojCSbmeU4h+rbxLdYfmE5zjw9g9uvb+Ne1D217S5WLviq8leYWHci5Mby3J7SJyM2FvjrL2X8ztZWGZB3cdF3rfKBKCoXDBs3Dnj9WnP75MnKi/YyZZQ/5lSsqPwgV6yoHIH04AHg6an8NcTFRfl3MilJ+YenkOb3jYkB+vdX/rnVxfffK6egGz++YOuV31JSgEePALlc+Ruas7PBx40L1e2o27j27hqa+DTRd1UAAIIoqn4KJZXY2FgULVoUMTExsLbO5RefjhQKBSIiIuDk5JTlio/PnwOZpwbat++/TgvlP13ahfSDbWO42DaGie1iuAqrbQqjP0MFpzDab9M/m9Dv935ITEvMMo97EXdc+voSnIyLKgOQp09nX6ijI/DLL8qI36lTQLduyghgPtuH5uiOn/EWdgCU0yNNnKgMGOYmmPfmDVCvnnJwpzZOTsrRmf/8o4xtlSwJlPRLQ+Uq6ahUQY7mzZWjI1UqVADOntUe1BRFZQBy//5cnCiUMY7Zs5WjOgv6z3lEBHDunDJGUro04OqqvLDXh3RFOkYdHIVFZxdpbCvvVB6/d/0dXjZeuhW2erVydaLHj5UnVLs2MHSocqWiTC/qqX9PodOvnfA87nmOxZa0K4kRNUegZ8WesDK1yjF/QRNF5Wjj9w3AiKIyYG9lpXwP5JZCAaxZA4werRxkqCKXK28bHj9eMzifm7pdu6b8U2RtDXTooCy30Po9O3YA06cDV67kf9nW1sCAAcohjdn9MvKeUlKUf84zjkbPyNdX+bfuzz81t23cCPTsmbvjabTNo0fA5s3KqHXXrkB0tDJ66OaW63PJSlqacsTnokXKtd1U7O2B1q2Vf1djY5Xx5ago5Z+B8uWBL7/M12qoEUXlSNWjR4FXr4DISOWfpAcPlHHqPn2AQYOyHpB2981dhIWHQRRF+Nj5wMPaA772vpAJuX+///PqH4SGhWLX7V3wLOqJe0PuwcSoYEbC5aY/wwClFoYSoASAmjWVnRyVXr2UP0xTweAFveFi2xguto1hYrsYLgYoSReF1R/9+/bf+OrIVxojxDJq6N0Qu7rsgnVssvKX8osXC6Q+uXUfPmiD33AL/w0DHTdOeVGqy0crPh5o3Fi9r61BlgqUPADY3wUsIwCfg4DTNcAoDYh1By5+BZwaDaRawNhYecdTpUpZF/f0qTI29u+/Op+mxNdXOWgrMFD5/OLzi1h6fimuvrqKNEUavG280a50O9QqVgtJaUl4Gf8S4dHhOPvsLO6+uQtruTXalmqLNqXawLWIZtRp1y7ltUbGgJIgKO+2Dw0FmuTXAJukJOV9+W/eAN27K4PamaQr0vHlb1/if1f/l2Ux9ub2WNRsEbr5d9PcePKkMpIikyknLT1wQGsZjz1tEDasLaJqVkQReRGceXoG6y6vg4jcXSIXlRfF+DrjMTZwbJ6CBe/j0iXg55+Vp3zzpjIwY2OjDHi8fq18j4eGKgfsqfz6q3KUb2SkMr1pU2WQ7/595ePCBWXQRGYkomOnNIwYnYbKlYxgapTNNA7/79Yt5bQH2U1T6+QEHD6sDOjnxunTys/48eP/pZUrpzwX3/Jvsf7cekSkRiApPQlFTIugiU8TuFq5Ik2RhuJFi8PSVIc5drMSH4/Er/vCfFMhTB7r7q6MonXooGzATL+6iKLyxwRra2WwMSZG2d7ZzbKR0dChwOLFmune3sqB8q1aKZ+vX68M2GVkbKwMsuVmWg2p3yOXQ7Z4sfKPtLZJewMDlRWoXFn3wqH8kej1a+X7ShCAly+V9dYWYM2JqSkwYQIwahRQpEju98/K778rfyPJ6vtGhnQ0xmEMKX0YLYPeQfAtqQwGVaqEi9G3MPrQaISFh2nsV8y6GOY0noOuFbpqLTchNQFXXl6BAAFOlk648+YONl3bhM3XN0Mh/rdS29o2a/FlwJday3hfDFC+J0MKUC5YoJyfRsXaWnk7iIVFgVTrk8cLesPFtjFcbBvDxHYxXAxQki4Ksz9qVtQMc07NwaqLqxCdFI10MV0jr3sRdyxuvhjtizdVrgjz22+5P6C5OVC2LODhobyavHJFGSUElEO+ZDLlfEa5EIsi6IrN2IeWUlrdusDatcqA3rVX17D//n5EvItAWceyKCN2wqK5VtiyRYfC7e8An7cHHG9ln+9FAPDTAUwb74hvv8252KdPlXG5v/7SoQ6ZmBS/hNqj5yHG9CauvLyS+wL+X2u/1ljUbBG8bZWr92zYoLygz+rKsCrOY7T3TrTqYQvLNo2yjE4oRAWOPz6O88+Vk+ebGpnC0sQSdYrXQSmHUsqoVbduwPXryh28vZVRXXt73LqlHNEVnfIae2Q9cfylbtGFwdUGY0GzBTCWGSujNhMnKt8AuRDmCdx0BEpGAW/Ngd2lga3lADHTn+gipkUQlxKnvRAA4wMnIDBpJn7/XRkk9PZWzpNaq1Y+BTsyzPeVlKS8Tly+XLddK1VS3mEcE6OcEiFLJglAuW2A3++A75+AcRIgiBBEGco7VEafCgPQOaAZXIu4QibI8OqVMha8cWPWI5G1cXNTNr1qtJpCVODM0zO4GXkTjhaOqFGsBpwtnSEIAkRROTXtoh/TAQjqDWMaD6H6Mpg2no5kZN02AgTU9ayL+p714WHtgUYlGqGEbYks8z96+whHHx2FV2Qqymw+DOsdv8MqPkVr3vBiVvCo3wZGMiNlVEzbbd955eamjOS2bg188w2ePJWha1fN0Y+ursqBwi1bZtp/2zZg6lRlIGHxYmy5XxVdM8Wy7OyAOXOUIyMzj97LHI8AgIAAZaBN16nnFLduIXnCBJjt3QshLS37zBYWytGVbdrg33+Vxz97Vvn1UKeOchR69erK97+FhXIBn9mzlUFJLy9lwPr4ceXoyPdhbQ307aucf7NE1m+THKWlAcOHqy80VAz/IgRT0RBHIULAfZREOdxAMWjebZBqaY6J9VMxr2oaRBlgkwikyYD4TCPbx9Qeg1mNZinfgwDSFGlYcm4Jphybku3fLBUfWx/cHnxb+Xc0nzFA+Z4MKUD59KlyuoqMrbR+PdC7d4FU65PHC3rDxbYxXGwbw8R2MVwMUJIu9NEfVYgKiKKIl/EvUXV1VbyMf6mWX4CAzZ9tRnWXKsCaNXC78QRyRxdlFGbpUuDOnSyP9cqsOLYPOIySzXzRqNH/T7OWnKwc/qVQKK8qbWzw5PIbbAz+BXGRSXgDe6TBGEUQj/41/0Epy2eQ/30QQqYgZiqM8Rl24He0kdJMXe/CY+AAPEg/pl6ROFdg50/Ao0YadSxWDDhxQnkaY9btxD8legPynC/sAMAi1h/PvzuNojqOIlDdQvvokbKv7+AAzJ+vXHcoPR0wMxNx9aqIhIQMfyM8TgE9mgCmCTodIyOzVMDxHeCYAIgArjsBxmbmGF9nAmzuDMWIb4pCodC2p4hxmI0ZmAQj/Jch3LUmYms1g0NwFaRUqYV0G3skWN5Evz/64NyzcxqlyBTA/JvFMGTPK8gytV9c/VboIOzC4TAjoNRvQOuvAKsI9QLSTVE2/QtE2uxFZEKkRvm9UspixLEklD33CCbp+XN5u98HmD7EH43828HBwgEtfFvAq6gX9l3bh6nnpuLiiyxGEp8YBxyZCUEE3PEMqTDBO0tnzJ4Ui35DzGFqpRzmlpKivL3z1i3liDSZTBnITE5WzopgIqShSsQ+BCSfhfHTcOXcAFFRQMmSSOzeD+2PDsWBv7O4DdgoBbB5BNg8BixfAXFuwFsfIN4FSDPTzG/yDrB7ALhcAYJCANvwHF8fIV0OoxhfpF1rD5wfCCTZAOW2Am4XgFQL4HY74KlysauSJZXzxSoUUAZArV4o8yfaI2RmLFzq/4yTW7+H0+1/oRCAaDPgcAnAxPP/2Lvv8KiKNYDDv930XkijJfTeQSAU6U1AkSIIShVRQEAsFxuI4gV7BQSVIsIFUbooIFV674L0DgEC6X3n/jFmk002DUiywPc+zz4k58w5O2fnZJn99puZ0gyoOYhf/jzHwZs7IOAwpDjC9SoQXh7sEnRW8x38TRgNRnpVGEilc5+zcok7J0/qxMXHn0wmou5bTNn7KX33pDD5N3DO/J0NAJc8YNhjsLQytCvbjsU9F+Ni76wDlEeP6vnZdu7U6agZ06b9/XUKax6crtON5mdncyE8i0xQtzB8S58jISyEUv4BfFxjDh3+lzYe2+ThyVOmBfwa0w7QWZnOzjozNbvM79GjdaAwvX7PxdHohZ/46/wmDBjwdPLE28mLdnHFaWQMwc63CHh6ov5ci3rjDYyxVlYSy8afJQfw5rWR7Emshok7mLPAYAKnCEjwyvxNQx7Y2cFnHyUzImSpXkXt/Hn9BcuRIxAbq2/uxx4jvGV33ppfnSVLDYSH67WSGjXSmc3HjqWdrw57+JPW+HA7T/U45wVJRih3C0zA9hLwUWNYWglCboNzMtT1rkQf/9ZccIzj/djfuRSd8zQVAN7O3oxsMJL/NP4PLg73fmoBCVDeJVsKUIIeSZN+rpqGDXOe/kfcGflAb7ukbWyXtI1tknaxXRKgFLlR2P3RbRe20XFeR27F38r2HD2q9ODHJ3/E2eAAX3yh07lOnTLvT8HIelowkBlcQC+eU7SoHkrcrZv+bOftrctGRuqRfekOB+Ctt/SUb4CO3HTposegppOAI51ZzhraQN3voP0ocMh6bk0iSsC1mpDoBpfq43v2ebas86BEmWhe/uNlvt/3fbbXbU3fmn2Z9cQsDPdgRRuTycTVq2GsWBHAxIlGziZvh2fagXPu04KComDsRuh8HEpkiLOGO+sPtx81BpXgDTtGwPZREO9jLuNOFDMZQHd+zfZ5EnBkksezzOnzM5eKRBH/b1aVfQoExECr0/DmX1DpZtbn+DG4FP163wDn6Mw7kx1hwWI48RgDXowgsvkgfv37V/NzDN8JH68G+xw+1UY5wn+bwnkvXZ+quYgNqaAgDM2a6XGfRYti8vMjJjISt7JlOZxwnolsZv7VNRkOghfnduI/Jw8SwnmLXSYM3HApyUXHsuyLLMsBVZ3f7Fty2q4kxgR36rOTElzEh1u8yFRqsz/Luh2iGl1ZxEnK6yf1OQPFdukpCSr/av1eUQY42Q7WfAxh1cD3BLR6Cyot1tMW3KkUe0hyA+cIi80OERVpUuYRgkvYcfFmOJuPHyXB9TQYdGM5JkP//TBmM5S+neGUBviwMUx4FOJyOXT5joRVgdnrISYA7OOw7/oULR1X8MJuePKY9UMiHWFUe5hTE5LTxc6aBjdldOhodl3axe3429QMqkmvar30gk4nT+rgVvHiUKGCHoscHa3nX3R11ZlJI0bkmFq9l9o8wVIuUjJto8dleGwYVFqqX9skZzot6M/ik99hT+bo6jmCeY2PWchTzJih5z7MTmIi1K6TwlH7uVBhBfj9rTPLjSmUiICqYfrLj1e2Qq1r2Z8rr27jxVz6MIkxOJJILfZjwshmmnCDdNNDGJOh5o/6Sw7fkzoI7hqOIdGDFsU78W6b/xBgqsnKlTrDMjZWZ0kWKaIfp07pqQ/SzymMIQW/kF9YcuV9GiccybGuWwnlv7zJ73SgMn9ThtMY//1ipxiXeYRd9GEujuRtpMCdWFUWnukKN3KY1WB4reG81+Y9fFx9si94FyRAeZcKu0OY0eLFevqJ9M6f16NjxL0lH+htl7SN7ZK2sU3SLrZLApQiN2yhP3r29lkGLh3I+rPrrRydJn1gLuya4q3HD5G4cx/7qWVebTs7tWvDp5/qUUJz5ljuq1pVT3lpsVBLXJxeGXzePIuycQZn2reoy6ZHt+T4nBkVcQrikZK12H91f6bMUdDZox3KdyDEK4QnK3Xl5El4Z9dz3Ew5Z1FueqfpDK47OM/Pn1H6tvnr3DbazOpAktFKNme8F6VvvkjVomU5nPIrZx10VkOtK7B8XubAZEZryuiHWxJccfDk9/0LGPB0fcZ5fYFpylTswvM2VDXZoLMzPROgaDS45CHmNaodfBmaYWOMH/z8K5x71LypfXto99pPLP3fAL5ZmpxtoHF/IOwqDnuKwoJqcNsFPJ08qRNQi6fWhzFwwT84JVlNHc0dR0fO1yrNNPfj/K8aXPKEub9C9xxmBchoXxAUjTIQFJO3j+bJBgOfl63CGx0jSfHJ48Sm0YHgfo+jSXlQJhx+m5t94BrgljNMqwvvN4PYnAKVygBnm+v5YYP2Y/A/TPEoqH8JfOL0vbm3qA5O998PNa7peJ5Sdqg4XzyMEVS/kZhlxiTA6vJ2/Ph8fW4EebLm9BqLefysKe1dmplPzKRZqWY5VP5f+/fDqlXETfsRlzPWx8xfdXDnidIj2GlfG9yvQrP3wC3tD6HpWVj1U85/fyvLjeCxIx/nOHnllagrtJ/ZnYO39Ljyitfh1a3Q+jSUisj20EwScGQ1bXmbCVygJLXYTxIODOcbepL7+T0TDXZs9inKIXd/lhYrzvoGh3PM/O1YviNdK3fF08mTQ9cO8U/4PwS5BdGtSjcalWzEgf0GXp90ij+33sAjcCPt/T9n0vZrlLmdt2u8G0epzEFjJULdlhMSdRdfGqC/lJldEzaW0lNYnPAFB2d3gp1qEOJegdHNB1DdqwKurgE4OhrzbV2mhz5AOXXqVKZOncrZs2cBqFq1KmPHjqVDLpe/toUOYXpJSRAYCLfSfYH9/fd6TgRxb8kHetslbWO7pG1sk7SL7ZIApciN1PabO3curq6uhIaGEhgYaN4fHR3Nn3/+CUCJEiWoV6+exfEbN27k1r+dxy5duljsO336NAcPHkQpRenSpalRo4b5XkxKSuK3334DICAggNDQUIavHM6U3VMAaEhDgggCYCUrSUTPydamTBvq0plzWw3EXffj8IF6nDxZzuJ5H398GUajidu3vdiwoYXFvtq19xESooN9a9e2JCrKk6pV9SgiZ+cbbN68GYDy5ctTtWpVPVb0+efhhx/4o1074l1ccI6Lo/2qVUQ7wDlvuOkC22tXIaxEVS67G1jjso4b/BtwUxAc50Edl1ZggHOcYx/7LOrUnOZ4401pn9JMeHECrg5pw7dPnjzJ1j1bWf7PcraatnIZPZTOyc6JnQN2cnrXaQCCgoJo2LChxXm3bt1KWJgewtyxY0ccUidyU4oLv/3Gnh07ICGBMm5u/F4njrf3f0qySX9Q7YJuy/A4E5t+GQrnmkGyM0W4wdN15+ETHI69IYEhq34gMCYtYHEtIIBtjRoBUOnYMSods0wNW9mhA4lOTrjFxNBmjWVG4KFq1ThVTrflo5s24ZtuSdzb3t5saN4cgNKnT1Mzw8ooa1u2JMrTE/vkZDqtWAHo4Ym7i4GnewWOVdELHDXYvp0iYVdp0R+2BoMLLrSLGAQXGnHpXFl27apvPmdxLjKhyTt4+90GoPOyZdilG59+vEwIy5vVZl9R2OK4j2v212gS3ARfF1+qFqlKuRvlcHVwxc/PjyYBATB9uh6O6+vLTh8fLl+5AikptFu1Cpe4tCzcK0FB7Pi3LascPUqFf/6xuNYVnTqRbG+PR2Qkrdats9h3oEYNzvw7kV3zDRvwvn3bvC/c15dNj+oAbNmTJ6meOj/nv9a0aUOMmxuOCQk8lmHVj2OVKnGsUiVO+sCnJbdy1Zg2NN4NN9qgVza6wAX2mvbQ8CJUuQ72Jrhd6VESPHwBWMISi/OWpjQtXFpQwbcCrl5VmT6tJIdv7IdHpmAfdIRO9vq8YYSxla0YTWD697+zrN4jAIpRjPrUxy8Gnvv9MA0On7R43mWPP47JaMTr9m1abNhgsW9hy9osbBxCkh2sZS1R6eab9DMF0CSqB1yryT/7G3HiaHle5nNG8TlFuZbpPSK9I1WqcKJCBQCabN6MX7r5IyM9PFjXSk8Hcc54kVOtgni1z2SCvXQ2+Nxlc1l6YCkJpgSWYTkvbznKUY1qAOxkJ7XK1aKey1PUdumCW5k9HNq6HwejI8mJxYkN64JSBq4knGD7jdV4pVzFyyWGwJRbNFyxjf7n0qbPuFCyJHv+nQPW7/QBPK+e4Zgf7A+CA0FQ26sLta+C381wmm3aZFGn3XXrcvHfLKfWa9bgHhOjszm/+45rFSvyx3o9X6/R30ho7VCC3INYfnw5H/z1AfVi6+GEE6WuxfDhtDU4povL5vY94vLporx58L9E4G3e37LlWjw9o0hOtqfYikt8zsu4ov/u/qlQgaPp3iOKXk378ijOxYVV7doBUOzSJW6F72JODVhZHm65QhOa4IcfAMtYhindFBUhhFCb2gDsYx/nOIezvTOBcXb03BlHxSKP4xMP/jdu0OTf/3tS7XzkES4XLw5w9+8R1avr9NV//mG/vSOLwoM5HFua9S6LuF1xLs3PwOzF4G5/9+8RAPW37uBWmA8reYztNGSHW30atNuDyQStWpVg+PC89yMA6tatS8l02XMZ+xHVqlXLdX/03s+AaQNKlCjBpEmTKF++PEopZs+ezRNPPMG+fft0h+Y+4+CgF+9auDBt26pVEqAUQgghhHjQxcXFYTAYMGWYIFApRdy/H4wSEzMv3JCQkGDen1FycjJxcXEopUhJyZwqlHpcQkICBoOBrzp8xY24G/x85GccccQFnWZhIG0o85rTazjOcRoHNcYl0IC9z0q4PgEi9Af5J54AP784YmNNxMc7ZXpOR8dEXFz08xqNCk9PWLFCzwt57VqKuU5JqfMXGo0wbRrExREfG0tcutQP96S04buePg787WpP3XAYucUHv8hwLnuaCIyGwBQjvzzhwuEAuOKeOYPIw+hBQ/+GhHiHWAQnU19DB5MDDQMbsvNK2pyLCSkJvLDiBYZ5DMNoMJJgZaXahEuXiN28GW7eJPyl5/i7ojeO9k6EHL1Msp03caE6jdB5925eGX+CgS4682V7CYiq64JjgguO570Yf2oiRXgVA4oynGa/Yy3OOZcCwNVg2fYmOzvza5RkZWWLeBcXEpycsLNyPyQ5OhLr6cmN4DqMLtcPdX4cNc8fpNUZKJloMJ830UoWVryLC3EuLtj/uzDG3iA9NPavUvDMJXsei9bHptjZ4WiCLTPg6zaepAz6L/bXfTCWg7//TmSXXnOH0pxmK4047FSN6y4BmZ7vm3K1ODiwHUXdnCmZEs+AUrUY2WEk3s7e+nlSUvj555+JS44jPj5eB2c++cR8fOLmzcT9/Tfs34/KMFQ/Jf1raJ/5Y3SciwvJ9vY4WFnoKdHR0XysKcN5TUZj2nmtvIYRbs6cKOpCtNHIvCfh7U1pWYdJDg7EubhQPB4+WWPH2EfgtI45YsBg/lsNiXTkh7lQM13C5OokZ06UcOGqO6xzhUhnKOZRjFpBteji1wW3G24YDAYa1qvKSz1KExZWk5iYflw7fpWdG35AJUThfMOOxYsgIBb+9tOZjueb+uFh8iTJlGTxHgFgjz1F41xodAm8kjO/hmFuvmDnhFN8fKZ9wTGOtL3mT1y9qjSv0JwrSVcwYKCsb1lKRjfmz+X7uGYHbg4xrKATbUkLtKfeh9akvoYAKRm+NFRGI1GuHqRUq06DFk0ZmeHLBl8HX1qUaMHWi1shQyKlPfbm198ee1aeXMlKVgLgtNOJrughkpe4xKYbb0B4OSi/EoIULWiBC0FEAs8POM1/b8Dy/0HFm5Cc7j4MiLOnQhjUCIOn/k22/N/TLiQ6Q4Jz2lyjc2roLwXaJqTdh+b7+59/MLVqybR+NTlWsiIAhy8e5tV9r1pcj4tyoWq4E49cTLEIToK+b83397+vYYIdOKVApKOBM/4uXPCEv5yKEnHIU0+Em3pel3hcXOJITrZnepHmbKzbjjHXltD8rP47M7eNneU8lMqQ7r3HyYl2p6DdKT01wOZgWPSYE9cDXMhwC1ImHJpH2OPl58ItFwiOteepg/DUkXhqXAM7jPzc04V4F4h3yvx/1cFiTlws4UKUE8zoZqDZKXjqCJSMzON7RIcOMHcu+Oih1Um7duG37STOh89iKK7vkw2lodJwePFqUWoVr4q9ozNJvXvriWrHj9fTBgDxzs7EubiYX3uLtkl3fxvsoCpHqYq+Wa4bijDdaQgnqEB0dOb309z0I1J/znSt6foRefFABig7d+5s8fsHH3zA1KlT2b59+30ZoARo184yQLlmjV4Rysp9L4QQQgghHhAuLi64uLhkyrY1GAy4/Puhw9FKUMPJycm8PyN7e3tcXFxQSmGX4UNf6nOmngPAzmjHvK7zaF+2PScPnqSooSgpKoXFZxdbHJdMMnHEgQGSSxyGF2vwhP0UFo7tjYMDLFvmQni4iePHnTl2DP4d7AQoEhMdiYv79wOuycD06XpFVgA7OztznRzSB9fs7GD2bCKeexqviDicrXyQckhKMme4lI29hd9tE2Vv632RHiaKRMbRLBIaRSYy118vOIABWpVuRX+//hgTjVYznVNfw7IuZenm043Pj6atILHn6h5OcYoagTXMr2GqxMW/YP/1V7gW0Zk9vlcjaH4xbYzrhZIe5vraJydjr/Tcbv6x0OgiLAmKA+LwiT9JMyyzoxwTE83HGtINkttCI1bUfQ8VeJPjZx1ZmdSerxhpsUiDc1wcRpMJZyuBoUu+Bo5X8+N00m5W2r9HeJVw0ElNNL2qeComjuJR+vkzco6LI97BmevJfnR2m8aKZr9BKZ1ptrN4MuUvx1EjDIvA6EtrIonb+h9WPf881K1LvWcdadsWhg5J4ccrfQniGicTyltkLu2lNq/wKRtOtiBk2lkaNtxPnTrQrUYtc3AyVeq95JwugGN+DR0dcfH3hzZtMDzzDKxeDZcvg6srdjdv4uLuTkpKCvZFimQ61iUujmR7e/N9GOWo54DbUxQeSUnEyyUOOwXLOiiCw3Qm4xPHwGgyma/FITGRs15w3Q2uuOuFYk5Vi8fR3o4EEvjdH34vDx+ugef2Wd7foadS6LkDjpfzxr5EMO6turDTww/D8eOU2P6PRXASwCk+nuLhcRQPh7CVdhiaNcPxsc5QvxOnjUYOxursKHt7e1CKgEPr4IcfKLFsGVfb6AzKgLAwAv5dn6byDfhiFWyPuMGVBmWJKFeCR2o9wtmoszjYOeDq4EpIclG852zFLjbaHLQG+NupKC8lzMItJh6j0YR7vM6O7EDaQgyOiYn4hV2HM9do2XNMWibWlSvcmDGDOFM4yT4pvOTyPY9gOS1Fapvk9B6RPhN3l30VfjI9z9XYIEx7nbiCE7VrW0434ezsTFGforR3b0+KMYVlx5cRlxxHae/SOEY7Epf0bwAHywCOQun3StAZpn7H9eNfCSSY9ysUJ/yg4XOwYCFUTkqxeI/IKHVfapB3USUY8ASk2MEWlUjLm3E0uGT5HmFMTmHI8otM6xLMvmKQlH6ORAWPnoNOMXH4JFl/j3BITMQ+LolY3Jhs15dJjd2Jf/QTPFUsdo6KZsZ/r7XyL/hN+oZaTl2pYOxAl8qduBaewp5jV7icdByqPsVxIHVKzMbRyXQJj6NceNp7RLSD/hLKoJT5Wh3TBcHsFDQ7B3aHErhczZ0TVYMIjPCh6f6bDN4Drc/A2ZBk9tfWx9bal0ypdDN1pBjTXkPndOfdWQwefxqCPRIo/m/b/OmhWF4OxrSGXofh+eNpbeOQnMxld4hx1PHYJDu46BJHvLcHlVp2gOeGWbyGjo6OFC/hSNLNfah0Edy4FB9m/bmAF545Qa2K4FCpLFSvjqlHT+a9vIvl313FGJ+AsjOSmODASL7gM0bTlUXYYbK4v40ZvoByUfFUjTtKOU4Se7gOJNazGO6fm35E6s8ZZexH5NYDOcQ7vZSUFBYuXEi/fv3Yt28fVf5NEU4vISHBIrIbGRlJyZIluXXrVr4O8b5+/Tr+/v65Gt518SKEhFiW27zZRGjGeVrEXclru4iCI21ju6RtbJO0i+0qqLaJjIzEx8dHhnjfp2xtyqGMEhPh6bdXs8jQG1yzn0Quq3kZI6ITafPBu+wyfgkpTnC2GZxpxbBGA/jm8xxm9v/XsRvHqPllZSb9CS9vz9MlWLWkcREcpn3PY1WeyPViN3FJcdSeVpvjN3WAwT0B6l010L/OILp2eQOPEnpo783Jn+A14jXs72LKw7z6jucYxmSSsAxi+xDOIH6gR4lt1Klwm2O3/qHcoUs4/xvvMKHnbJzYBA4FZf8crg6uPFuhBwMDR7L50yhiN+ykTOQ+bisvZjCQ/dQiJX1eTInt0PgjCNmEnV0MO352pO6pLBb/8fKCJUugeXPiJnyKyzuWWV2XKMYnvMo3DCeZzJmhZcroFYiffVYviGHN6dNw9areX6JE2qJN1phMJq5cCeP99wP5fdp5mrOB+uzkGX7Ci7RruO0EXXvC+jIZLsfJiyF1h1DGpwynbp3CO07R4UAsJa/GEu5qZFNNb5ba/cPJ8JOcDD9pHtqfnoPRgcr+lWl5LIEPp5zA8W7m0MxK27YwZYpeivjoUT2dwpa8z+1KtWr6xe/SRS9lPGaMXrY8nYM+zXj01hKLYb+pHmEnq50exzshQ3R10KC0lWQXLICo7CdbjcWFSDwJIu08Sa6exPV9nrBiyey7vJfwuHB8XX2xK9GGlyYO4nJ00UznqVABli2DihWtP49SihSVgr3Rnq3bk3h8/HRuVnsv86r0d8guBT7f5sGL66Oxz8Vq9csfK0f/ercJN1rOJVvrCkxbDvWtLPL8Ugf4poH+2SURvvwDBu+1fv4jjQfzScTz/HaxJp5FHLh0CVLjl05+l6j48kscTFps/eA8qHQdgiPggicc8wPveJ212POKD42OROKUkM2koQYDys4Og5Vgbk6S7YxceqQCZ1vW5dVih9gffjTT36TRYCS0RCiV/Sqz+/IujMeOUzbBnTP+9iQU9edQ2KFM53W2d+bHLj/So2oPi+3vbXyPcRvGWRb+ZR4cfhrQi8rNm6ffpwYNggyjuS0Ec47n+J4G7KAKRynBpWyvNdI5AM9rJ7J+o7wLD/0clACHDh0iNDSU+Ph43N3dmTdvHo899pjVsu+++y7jx4/PtP2ff/7Bw8MjX+pnMpmIiIjAy8sr1x3C5s2LcPx42n+8o0dH89prVla6E3fsTtpFFAxpG9slbWObpF1sV0G1TVRUFBUqVJAA5X3KlgOUJhP07q3jAjjfhhpzoNQG8DoPAYfBwTLDxsXehe3PbadGYA0ArkZfZcquKfyw7wcuR2X+hFzVvyrLnl5GGZ8ymfZl1PvX3vzv8P8AqHYNQm848/FT3+OVbAfHj8P27bBunY6o5kWbNtCihV7Bp1KltHTOLOy4uIMm34fy2l+KdzZZLk5x09OBs3721D2dzari6ZzzgsseUPcymYZS5qhIEahUiR2JtXhx10D2USfLosHBsHevPgTg8JENzPiqP2HXz7G9BJzKnCRoIcg9iP41+/NKo1fwc/Wz2JeYqFfKXbMGbtzQCxanPk6f1ivmennB229Dt9YRMHy4/uRtsnLBDg5Qv36mANkNj1KsHLeTai38ef11WLs267q6u8Mrr+j4WGriZHw8vPwyfPutZVmjEUJC9NPa2+vYl58ftGoFQUEmfvwxhYMHLYOhLsTSOugIM75N5FT4egaf/4ZD6YJhjnaOjGs2jpENRuLmmLvge2xSLHsu7+F67HUuR10mIj6Ckl4leaz8Y2mv96pV8NRTEJn71d1xcNAXmZuhl/Xrw6FDemGq/FCnDsmbtjJmnBOffpp594AB8N3zu7Br8Wha5CsPlI8Pe19fwDpjaxKTDPRud5PSN3bpxR3atNENa8W2bdCpE6SbStGsZEl9K2ZcsPb2bZg/X9/3ly6lW4zbMQpqzoGqP0OpdCt0RxbTixQZLYNrdsqZqi6t8fd1IMXhFikqBYPBQMtSLXmuznMU9yyuV8xdt04vVhERof+QDx/WbQU6sPzhh9CtG0opLkZeBKC4Z3G+3f0to/4YRXJyEv9dC2MyxJ1TDPBtG2/Urdv0OAqBMVZeIIMBPvsMRo2y2BwZqRc1S06G0FBwc1N88NcHjNswLsfFhHJSM7AmnSp0wtXBlVpBtahfvL7+O4iJ0UNNFy2CP/+863tVOTrCU09hGDQIHnkE3NL+Xk3KxIGrBzgcdpjoxGgC3QNpFtKMIq5Zv1lejLxIox8acSHSchErAwZ+7vEz3at0B2DXpV00m9WMuOR09T/7KMzaQKZx6lnw89Prx5UqpadV8fKCGTPg3DmoWSqCR30P479hIc57tqD27cOQLqMy/uvvcB7+XK6eJ68kQImei+f8+fNERETwyy+/8P3337Nx48b7NoMS4LXXDHz2WdrN2aCBYuvWB7L5Co1kHNkuaRvbJW1jm6RdbJdkUIrcsNUApVL6M+lXX1nf/84HtzhbeSRzDloux+3p5MmAWgO4Gn2VFf+sICbJ2qfeNEVcirC452KahjTNssyOizto+IPlnHBvNnmTD1p9YFnwxg0dUYiK0kGZs2d1lsixY3oI78WL2dYF0BllU6fqlDyldDDtyy91ENTPD0JDubrtT4JO535V5I0hcLKUJ23PGAi4GsWpCv5836kYsz3PEJ54m/IOxZgeMoxmKhjD9u06ApKUpOvu7Kwji+XL6w/oXl46oBqQNo/Y1Km6razFZkuV0gG9MhliwCZlYvHfi5l/ZD6rT60mMiFz4Kuqf1Weq/McQ+oOwcXhHi77evgwvP66bhMrc2FaMBhgwwb4d+EIpWD2bH1f7tuX9WF16ug4xpkz0L+//uB+L7Rvr58/9eWPT45n7em1HLtxDF8XX9qXa09Rj8wZefdEWJieR3PyZIiNzbqcu7vOZvzwQz09wpo18OOPOkPVWmA4Ow4O0KiRTuWqUkVHcufNgwMHcn8ONzcdCaxeHdDZYNOm6QB2yZK6fbp3103N5Mk6iJ0X1arpb1GsfP7PjcuX4c03dbtmVLmyDkD6++t77/PPYdw4iM4md8jVFV74zzmU91kuHCiHfWxxKlROJLLUPCK9tpKiEqkRWINe1XpRzKPYHdWZ27f1PVC06L8vnHWHrh1i0LJB7Lq8i8G7YfqKPDyHoyPMmaMD47m0/sx6+vzahysxV3Is6+fqx4BaAxgdOhofZx+OXD9CiimFusXqYjTk8P9UbKy+n195RadFZ8fBQb+fgl6RuH176NNHB+W9vHJ3Ybn09/W/aT+3PecjzltsN2Cgd/XexCfHs+z4MpJMaUPrjQYjnS7u5Y/ZNXP8fs1g0EnO771n8V9A9s6dQ40Zg2H+fFTNmhj27NHvC/lAApRWtG7dmrJlyzJt2rQcy9pqh3DNGt03SmUwwPXrad96irsnq97aLmkb2yVtY5ukXWyXrOItcsMW+6PXr8OLL8Kvv1put7ODbt1g2DBzvIgXV7zIt3u+zXySPHBzcGPNs2sILWl9TqOWs1uy/mzaXHNeTl6cGnEq22yWTJKSYOZMGDkydxlaFSrotKobN3Ium43v2vhSYdqvNC31aKYP3UopbsXdIu52HEWDit7V+8TJkzBpEuzYoWOajRrpR4cOOY/kS0pJYsuFLaw8sZKT4Scp6VmSZ2s+S92idXM9/P2O3LgBgwfrQENWXn5ZZ3BZsWoVvPMO5oV18pO9vQ5g9e6d/8+Vo1u3YPly2L9fX/yePTqbrHx56NtXv2ZuVrI3b97U0epVq2DxYn2erJQpo/9Weve2nn147Zp+Qb78Ukf4suLqCkuX6pVgc8Nkgpde0sPOs6BatyYuMBAXDw8MzZtD1646CHWXdu7Ut2OGBeopU0YvAP/dd/9mk2ejcWP9spQte9fVuWdSTCmsPrWaW/G3aPbjRop/Mj3ngwYN0lHbjN9s5MBkMnH20ln2RO7h+M3jzNg3gzO3z5j3uzm40Ti4MQNqDeDJSk/iZJ+3eQszuXRJR7e3Z5j3o0EDGDFC3xsmk/6CydVVv6/n53saEJUQxehVo/l+3/e5Kj/8keF8/djXnDypv1fIeCmpgoL0WjstW+a9TiaTiVu//YaPnx/GfJw7UAKUVrRs2ZLg4GBmzZqVY1lb7BCC7jP5+lpmLS9YkKcvL0QO5AO97ZK2sV3SNrZJ2sV2SYBS5Iat9UdPn9ZD9sIyTKVmb69X227XznJ7XFIcbea0YcuFnOetK+VdirZl2rL69GrO3j5rsS/QLZAjQ49kCjpuOb+FJjObWGz7uM3HvNrIco7CXNu3T38KPHLkzo7PJRPwcY9i9P1hd7YZdQ/9e3hKig4kWAtIDRyo00OtLA6VSimdYPnVV3rOwLwmCOZG2bI6AbFRo3t/7ntCKZ0+m5dFKm7f1h8u16zJvK9vX53iaGVxoUxSUmDrVj1+/o8/dFDfYNABoo4ddRsWu4MswV279Iu+aJEOgPr56aDZkCGYQkLy7W/mxg1o2lQnXedVq1b6PTI3L1uhUUq3SVaxkjZt9BcC1ard0ekzvp8lJCew7sw6FIpKfpUI8QrBzniPs/eSk/X9unq1/jbmscegZ089vUEhUUoxePlgftj3Q7blWpVuxYreK3C21zdNUhK8/z78979pyeX29jrLeMIEnQB6J2yxP/pABijfeOMNOnToQHBwMFFRUcybN48PP/yQVatW0ebfFceyY2sdwvRS5wJONXAg/JD9/S3y4KHvDNowaRvbJW1jm6RdbJctdgiF7bG1/mibNnp6r4x++kmPirMmOjGaZxc/y5JjSzLtMxqM9KvZj5ENRlIzqCYAkQmR9P61N7+d+M2ibM+qPflft/+Zs/aUUrSY3YKN59LmcyvmUYxTI06ZP9DdEZNJB0BOn9bBlb/+0mlTef24VKIEzJvH1RplOLx1CaaDByh29iaesSaSu3UhpNMzOX4Yl/dw9Ov+4Yfwxhv6dwcHfcPlMTvjwAEdlzh+3Pr+0FCdCVe8uM5yu3kTatTQQamoKD1k8q+/9DR/9vaK8uXj6NrVmccfN2YXI71/paTowPDkyTq42b69TpGuX//Ozmcy6XkSXVzubZQuNlaf89/3hfz+m7lwQQcps5sWwGDQX9YEBurEvObNdSLfffEnnJiog3ipk7k6Oemh0n37Zr0qUC7J+1maxJRERvw+gul7plus1g1gb7RnSN0hfNTmI1wdXDMde/iwnlPS3V3Pz1q69N3VxRb7ow9kgHLQoEGsXbuWK1eu4OXlRY0aNfjPf/6Tq+Ak2F6HML0vvtDZ+amKF9dvlvmckfzQkDdP2yVtY7ukbWyTtIvtssUOobA9ttQfPXs28wehIkX00MYnn8z5ebZe2MqCwws4EX6CALcAmgQ3oU2ZNoR4h2Qqm5iSSKd5nVhz2jKDq2GJhoSWCMXP1Y/dl3ez+JjlyrBftPuCkQ1H5lyZvAoL0ylQhw9n3ufnB//5j/4gf+KE7pDXrq0jEu7ud/W08h6ezp49OmDctu0dB0oiIvTiEb/8Yrm9d28dlLS3t35cRtIutqsg2ubKFb0g+c6dmfe5uen7q337fHnqgpGQAN98o7NTn3tOT7Z5D8jfTWZHrx9l+fHlnIs4h6uDK1X8q9CubDu9CFIBscX+aC7fiu8vPzzAKYXt21sGKC9d0nML2+zQAiGEEEIIcV/78UfL33189Ejo3A4ra1SyEY1K5q6z6mjnyOwus6kypQq342+bt2+/uJ3tF61PwhXiFcLguoNzV5m8CgjQk3/99JOOckVF6ck4/fxg6NA7G6Yq8qZuXf24C15eeqHfw4f1Z6fkZD1atUkTSfQQuVe0qF5va9o0Pc9p6nSdZcrodWPu+8/kqVmTIt9V8a9CFf87W8DpQfZABigfZBUr6nmOT5xI2/bllw/Am6EQQgghhLA5cXGZpwF8+uk7n/MqN4p6FGXG4zPo+nPXXJX/qsNXVofD3TNubjBkSP6dXxSYatXueBo9IQCdbTtsmM6+XbVKf4fRrFm+LYAsxENF8mvvMwaDngc4vZ9/1lOECCGEEEIIcS998YVemDe9gQPz/3mfrPwkn7T5BDtD1p/6DRgY33w8j1d8PP8rJIQQ6fj4QK9eevVkCU4KcW9IBuV9qH9/ePttPTQh1YgR+s3xHk0TIYQQQgghHlJKwe+/w9y5MG+e5b42be56tG2uvdLoFR6v+DjLji/jXMQ5zkecZ/Wp1cQlx9G6TGs+aPkB9Yvf4cIdQgghhLApEqC8DwUGwltvwfjxadtMJr30fMZOpBBCCCGEELl165ZetHXFCuv7x44t2PqUL1KeVxqlzYmWkJxATFIMvi6+BVsRIYQQQuQrGeJ9nxo3TmdSpjd/Ppw6VSjVEUIIIYQQ97nwcL1QclbByTFj9KIihcnJ3kmCk0IIIcQDSAKU9ymDAT79FDw80rYpBd99V3h1EkIIIYQQ95/ERD3XZMWKsHt35v0ODnrF2v/+t8CrJoQQQoiHhAQo72O+vpknKf/hB0hIKJz6CCGEEEKI+8ONG7B4MXz2mRtVqxp4+WW9Lb0iRWDSJDh3Dt57T39BLoQQQgiRH2QOyvvckCHw5Zdpv9+4AatXQ+fOhVcnIYQQQghh2/buhe7djYCH1f1FisDmzVCpUsHWSwghhBAPJ8mgvM9VrgyNG1tue+kl2L8fRo+G7t3hP/+BI0cKpXpCCCGEEMIGVamS9b6OHWH7dglOCiGEEKLgSAblA6B7d9iyJe33c+egdm3LMp9+Cq++Ch98AHZ2BVs/IYQQQghhW4oXB09PRWRk2rjtihVh5kwIDS3EigkhhBDioSQZlA+Arl1zLpOSAh9+CN26QWxs/tdJCCGEEELYLoMBmjaFRo0SGDpU8eOPsG+fBCeFEEIIUTgkg/IBEBwMrVrB2rU5l126FIoV0ysxjh4tk50LIYQQQjysli1ThIXdIiAgAKNROoVCCCGEKDySQfmAmDnT+sI41auDg4PltogIPdx79uyCqZsQQgghhBBCCCGEEFmRAOUDomRJWLYMwsN18LF7d72a98GDsGYNeHtnPmbAAHjlFTCZCry6QgghhBBCCCGEEEIAMsT7gePjAx9/bLmtWTPYvBnat4eLFy33ffYZBAXBa68VXB2FEEIIIYQQQgghhEglGZQPiapVra/uDfDWW7B1a8HXSQghhBBCCCGEEEIICVA+RIxGWLIE/PwstyclQYsWsH17oVRLCCGEEEIIIYQQQjzEJED5kAkOhqtXoU4dy+2JiXq4txBCCCGEEEIIIYQQBUkClA8hOzv4808ICLDc/vvvkJBQOHUSQgghhBBCCCGEEA8nCVA+pHx8YPduy23R0TpIGR0NShVOvYQQQgghbNXkyZMpVaoUzs7ONGjQgJ07d2ZZ9rvvvqNp06b4+Pjg4+ND69atsy0vhBBCCPEwkwDlQ6xkSWjc2HLbk0+ChwdUrw4bNxZOvYQQQgghbM2CBQsYPXo048aNY+/evdSsWZN27doRFhZmtfyGDRt4+umnWb9+Pdu2baNkyZK0bduWS5cuFXDNhRBCCCFsnwQoH3JPPWV9+5Ej0KoVvP8+jB0LP/4oWZVCCCGEeHh99tlnDB48mAEDBlClShW+/fZbXF1dmTFjhtXyc+fOZejQodSqVYtKlSrx/fffYzKZWLt2bQHXXAghhBDC9tkXdgVE4Ro8GGbOhP37M+9LSdHByVSHD8NHHxVY1YQQQgghbEJiYiJ79uzhjTfeMG8zGo20bt2abdu25eocsbGxJCUl4evrm1/VFEIIIYS4b0mA8iHn4gIrVkDLlvDPP9mX/fhjPXflxYvg5wevvw5ubgVTTyGEEEKIwnLjxg1SUlIIDAy02B4YGMixY8dydY7//Oc/FCtWjNatW1vdn5CQQEK61QojIyMBMJlMmEymO6x59kwmE0qpfDu/uHPSNrZJ2sV2SdvYLmkb21RQ7ZKX80uAUlC8OOzbB599phfJ2bo167Jvvpn287p18MknMH68zq7094cvvoCmTfO9ykIIIYQQ941JkyYxf/58NmzYgLOzs9UyEydOZPz48Zm2X79+nfj4+Hypl8lkIiIiAqUURqPM/GRLpG1sk7SL7ZK2sV3SNrapoNolKioq12UlQCkAcHWFt9/WD6Vg0iT9c3bB7s2boWHDtN8vXIA2bWDCBAgJgdatdcalEEIIIcT9zM/PDzs7O65du2ax/dq1awQFBWV77CeffMKkSZP4888/qVGjRpbl3njjDUaPHm3+PTIykpIlS+Lv74+np+fdXUAWTCYTBoMBf39/+dBoY6RtbJO0i+2StrFd0ja2qaDaJasvZq2RAKXIxGCAN97QwcaZM2HKlNwfm5AAr72mf3Z2hv/+FyIj4do1eOIJaNtWn18IIYQQ4n7h6OhI3bp1Wbt2LV26dAEwL3gzfPjwLI/76KOP+OCDD1i1ahX16tXL9jmcnJxwcnLKtN1oNObrBweDwZDvzyHujLSNbZJ2sV3SNrZL2sY2FUS75OXccneILNWrB5MnQ3y8XhzHwyNvx8fHw+jR8O67MHUqtG+vg5QxMflSXSGEEEKIfDN69Gi+++47Zs+ezd9//82LL75ITEwMAwYMAKBv374Wi+h8+OGHvPPOO8yYMYNSpUpx9epVrl69SnR0dGFdghBCCCGEzZIApciRk5POijx9GhYsgJdfhjtdgHL5cnB3h6efhjVrsi6nFMyeDT16wFdfZT/UXAghhBAiv/Xs2ZNPPvmEsWPHUqtWLfbv388ff/xhXjjn/PnzXLlyxVx+6tSpJCYm0r17d4oWLWp+fPLJJ4V1CUIIIYQQNkuGeItc8/ODp57Sjzff1FmR69bpgOMbb+gsy6VLc3eu+fP1o3Nnfa6GDeH8edizB2rU0PNYzpqly/7yC6T290+ehOBg6NVLZ3jKcHEhhBBCFJThw4dnOaR7w4YNFr+fPXs2/yskhBBCCPGAkACluCN+fvDOO/qRaskSOHMGEhPh1VdhxYqcz7N8OaxapVcQHzMGshr1NGmS5e+ffQb168O8eVC27B1fhhBCCCGEEEIIIYQoZDLEW9xTpUtDxYrwww/w74inHCUmwvDhWQcns7JzJ3ToALdv57maQgghhBBCCCGEEMJGSIBS3Bu7dsGBA/D333DyJAHx5/nzp6s0r36TisWimDE1gRbN7v1EkidOQO/ekJJyz08thBBCCCGEEEIIIQqADPEWdy8lRY+3zqAasD71lxdhAJBisCMRR3B0JCbRgQTlyBgm8RPPWj31fHriQhyJOJKIIy5ejsQlO3ArRv+ehAOJvzuytb0jTVs5goMDOOrzU78+1K5tvc7790Nysi6X/hg7OwwREeDqCi4uYG8vE10KIYQQQgghhBBC5CMJUIq7l5SU66J2KgUX4iAhDpd/tzmRYFEmOBimTIHLl6H983/gRWTazogsTvznv4/0JkzIOkDZo4decScDI5BpZHpq8NLRUS9pPngwjB9v/bz/+Y9O63RyynyctZ/T/162LISGWj/vxYsQH2/9HBJEFUIIIYQQQgghxH1MApTi7iUm3tXhRYIc4SqUKgVvvQXPPae3JyeD6cVEuNPh246OWe/LQ1CVxETLa4yJybrshg16csw78cwzWQcohw7VKwpZYzBYBjwzBjCbNtURX2sWLNDD852d8/5wd4egoDu7ViGEEEIIIYQQQoh/SYBS3L3kZD0cOjHxjiaD/OBjR8Z0BG9vy0RAe3tA3UXwM7sA5d0EVW3tvEpBQoJ+WBMcnPWxf/wBs2blqnqZ1KkDe/ZY3zdxInz/vfXAppNT9oFPPz8YONDqaY3XrsGlS7qctWzU1CH7klEqhBBCCCGEEELcNx7IAOXEiRNZtGgRx44dw8XFhUaNGvHhhx9SsWLFwq7ag8nXF2Jj9c8mk85OTM06zMXP9tWr4+Nj5bxKwYcfZnnc8SNJbN+UOjtlIg4kmX/2906ign8JnLKq87/BMJWUhCGvQdXsAolZBQnv9rx3E/h0dubYMR2HrFgR+vdPF7+Lj7+r82bp2jU4ffrOzlumTJYBSpf58zFOmpTzOdLPK5oayPTx0XOPWrNmDUyfnjnYmdWw/IyPbt3Azi7zeaOi4OpVCaQKIYQQQgghhBDZeCADlBs3bmTYsGE88sgjJCcn8+abb9K2bVuOHj2Km5tbYVfvwWY06mCMU5ahwdwzGODVV7PcHRIP3erBkSNWdt6GfqthVu+0TTdvwhdfQHQ0jFp/mpAQGD4Mvp2SYg5uOpDEkx1u881nBpyNyWmB0YSEtJ9Ll866zqNG6eBcxmOy+jn979llOt5FgPLCdSdq1kw7xfnzMG7cvzvzK0B5N4HabM5ryO3Q/KQk/Ug/HD+7ofknT8Ivv+SyglYkJ1vfvmaNDl7eqVu3dGqxtfO2bXvn5/3nHyhfPvP2/fuznrc1G0Yg0MEBtXw5tGuXuUBcHPTsqTOts3o4O2e9r1w5rH+LIYQQQgghhBDiQfBABij/+OMPi99nzZpFQEAAe/bs4dFHHy2kWol7zdlZx2kefxx27868f/Zs+PFHKF4cvvwSvvtOj2gGmD8fVq/W20zYkYAdCejA2A+/F8EUpPjhB0OOCW7JyXDwIJQoAQEBwPPPm/edOgVFiliPL+XZpk2WQc3sAp3//myKT2TujASm/RFM+vDmu+/CV1/pbMouF1pS2tULT8d4GtSMx9s5XgctMz4SEix/B0yOzoRdBS8vHUOykF+BT1scmm80Ws+evNvz3mcMSUkoBwfrO6Ojs55DNTfmzYOnn7a+LyhIzweRXfDT2sPVFVq0gOrVrZ/3xAl93tQpCVL/zaqthRBCCCGEEELcsQcyQJlRRIRe+tnX17eQayLutaJFYcsW+PxzePvtzIlsSukFsDMmsV29CjVqZH3emTMNxMTAyy/DJ5/A5s3QtSt8842ORwFERkLjxnD4sE72LFUKmjXTi4cPHQrLlul4xqJF0KFD7q5HKT2t47VrOnaSkgLvvAPz5xu5dcuZ55935vPP/52fMxuJidC3Lyz4w/r+8HDYtg228ZLeEAshZ3VMJn2MKSVFX2/6QO2pk4rXRyWy+vcUoovquN/gwfD11+nKjRoFXbpYD27m9ChbNvvXyMEh95mU6d1vQ/PvR5ki1f+Ki8uf8yYl6T+WOzVlStYByvr14fbtzNutBS0z/vvcc9Cnj/XzfvWVfvPIOCdr+rlZs9rm6QkyCkAIIYQQQgjxAHrgA5Qmk4lRo0bRuHFjqlWrZrVMQkICCekCFJGRkeZjTSZTvtVLKZVv53+Y2NvDa6/pR79+Bn766d7M6/fzz/qRaupUqFDBxIgR+vfPP4fDh3W0Uik4c0Y/0q85Ex8Pjz2mf65RQ9GwIYwZowgJ0duOHYMxYwzEx8Orryq++87AL7/o+jdrpggOhjlz0q7nm2/A1VUxcaLKst63b0OjRgaOH8/b63DuHPzyi4mePdOuf8QIA5GRetTuxImKpCTo1MnAxYtpQ/gTE2HyZP14910Tb78NhurVsw785IaVvwuTyUTUG2/g/NlnGA0Gy7lJrWWUZnw4OFg9LwANG2J4553sj8/q4eiIyuq88fEY7/xV0O8P1s5tMtnmeZ2crJ83Jub+Oi9gSEjA6l9QcrJ+ZDNlgKldu6zP+8UXGM6cuYPagho5EvXZZ9bP+9RT8PfflsFNR0e8DQbw9ESlTr2R7mHeVqNG1t+iHD6s51K1crzFQ+ZTzZOC6gNIH0MIIYQQQtwvHvgA5bBhwzh8+DCbN2/OsszEiRMZP358pu3Xr18n/m6GqmbDZDIRERGBUgqj8W4+Yov0HnvMkZ9+yr9M2c8/N/Hkkzc4dMiBd98tkqdjDx40cPAgzJunGDkyhn79YnnssSKcO6f/DNessfyAv3Gj9Q/8H31koFatW7RokUhyMnzwgQebNjmSkgK+vopt27LJ6MtB795GXnjBRIcO8Sxc6ILJpOuwbBksW5ZzAOLdd41UqhROs2b3NnPwxAk7Pv/cjaQkV15//Sbly2cI0BqNaUN3sxMWZn17hQr6caeyOm/nztC+vc74TEzU/yYk6H9zkQWaEhdnNQvTUK4cxg0b7ri6Kc7O1uvs64vdHZxXmUzEhIfj5OWF0cp5DSkpuI4ZgyE+HsO/mbKpPxvi4tJ+zrgvPh5DQgK34uNJsnJe4/XrBOS5tmkik5KIt/Y6KEXgXbz3RyclEZvFPeEfG8udDhKPSUkhOovzFjl+HIejRy22GYBsJkwwB2Bje/Uism5dq2V8Ro3Caf36HOumHBxQ/y4ApZycUI6OJD76KJEff2y1vMuPP+Kwb58u/+8xuf3Z5O1NclZfgKR+2ejoaNNB04LqA0RFReXbuYUQQgghhLiXHugA5fDhw1mxYgWbNm2iRIkSWZZ74403GD16tPn3yMhISpYsib+/P56envlSN5PJhMFgwN/fXwKU91C3bvDII4pdu3L/wdTRUTF2rGLIEGje3MCRI1kfe/68PaVKBd1VHaOjjXzwgQcffOBxx+fo3duXV15R2NvDt9/mfK2vvKJ45BHF5MkGbt2CCxf0/JinT2c+NjLSyIIFrndct40bfejRI+sMz7yKj4e+fQ2cPavreuyYJ4cOqWxHVj9Itm+HJUsM1Kun6No1bYqBbBdruhvZLdaUBZPJRMr161m/nwUEwAcf3FF1FJDl8jheXpjWrtVDyNM/4uPNPxsy7ktXxrNyZTwDrIQ4k5IwqDu/h92LFMHd2nnJw0JPVrj5+uKa1XnvIlPOxdMT56zOm8tzGJKS9LWlyyy1i4rK+rx792JYsCCvVQVAhYaisvjS0fDyyxi++kqX+zdgapHpae331G3BwagvvrD+pNu26T/GjOfMzfmdnfUbbjoF1Qdwzm5OXyGEEEIIIWzIAxmgVErx0ksvsXjxYjZs2EDpHD7IOzk54WRl1Wmj0ZivHxwMBkO+P8fDxtERfvtND7N2d9cLBx8/Dh9/DIsX6zLt2ukgT3i4Hn49ZIiBwED9MXzGDBMNG4JStpt5k+rTT3NXx0GDdNal0WgwD99OlZSkp9rbvz/vz1+mDFy/rkd/prdhg36uuxETAwsX6uTBJUvg7Nm0fSdPGvj5ZwN9+1o/NjxcT0tYqZJNJ1Dlyp490Lp16hSOBmrUgE8/1dsK2r+j2bNUKO9nLi7QsuUdH57l7eHkpIdxp86fmtO/GbYZH300XSQ5g06d9DwMuTl3XJzFUHGDiwuGrM57FxmfBmfnrM97F/OzZnveu5if1eDklPV50wWADanTMERH5+7ElStnfd61a2HcuDzW9F8+PvqNKYOC+JuR/oUQQgghhLhfPJABymHDhjFv3jyWLl2Kh4cHV69eBcDLywuXnIaAivuev7+ejzJVaKheqObqVYiN1YG1rNSrB6dOXWPp0gD+8x9jrj5DG42wcqX++ckn09YDadlSB5j+XaPprjg56eu6eDFvxzVpAt9/n/V+Bwdd965ddXJQbnXoAHPnwo0bmUdG//23fq2D7iDR9OZNPbdnTsl2n3wCzz6bOQC5fLle7DkmRgeily/XsSBHR3jjDdi6Va+43r+/zrbNzwDmrl1w+bKOl9jZ6dcsL8lMp07p+zG9gwehTRu9KJGdnV4hvnNnaNTonlbdwq1b0L07bNigA6O//AIed578e/+ws9MrfbveeTaxVbNn5618+kBpdjfQ1Kn6Zk8X5DTFxRFz8ybuDg46WJeQYP2RxfzMgA4Cu7vrcnnN/rTyxZ9Zfi1MdTfnza/6ZndeIYQQQgghBPCABiinTp0KQPPmzS22z5w5k/79+xd8hYRNyG3AzMUFRoyA9u11sK1pU/jpJxg9Wi+Gk6pSJWjbVi/Ymzod2tGjOnBXvjz06KG39esHc+bonzt00IHAW7esP3ejRvDqqzrrMbWMhwfMmwd160KvXrBpU+6uw8EBrEytmknRonqV8qVLM692DrBunQ5K3bypg2OPPgrlyungno+P9cy6oUPh119zHwDcvFmvVp7b6Q8PHYLVq3UQEuD8eR0c3rs3rcyqVdbjGAcO6CzbwYPh22+zTnS7G5Mm6YBoenXqwPr1eiHm7KSkwH/+ozMls/Ldd5bP9dRTMHPmvY+lAQwfru8B0K95tWqwZYsO9O7ape+x3bsNVKjgw1dfQa1a974ODzV7e/3IafXutm0zbzOZiAkLwy0gIOvMwJz8+afF+cgu0JnxUbx41uft2hUqVsz9udI/imQz/68tBj4flvkohBBCCCGEuAsGpe5ioq0HVGRkJF5eXkREROTrHJRhYWEEBATIECwbkl277N2rA0ylSumgZRZTq2WilB5m7uMDgYF6tew339QBQQcHGDkSmjfXn/tbt9YBs8hIPbT52jWdEZh+CtW9e2HgQB1oS9WmjQ4ifvaZPq5WLZ1FWrVq3q7/wgUdHF2zRgcX330Xxo7N+bjOnWHFCsttY8fmLkB64QJUqZL7UZjpffGFzqTs3FlnR+bVpEk6sLl6NTRuDLVr5/0cGYWH67iMtRG3AwbAtGm63bMyY4Zug7zq318HKe+lDRugRQvr+955B/77Xx1QTeXoqFi61ED79ve2HuLOPJT/z1y/rjNJMwZSc/N7UJD+xsma6dP1ty65PV/6OUHLl4d//rE4XUG1TUH0Z0T+kf7ow03axjZJu9guaRvbJW1jm2yxPyoBSiukQ/jwKsh2UUoHd+zvII/ZZNLDl/fu1YG11MDmvRIWpkeWFiuWu/I7dujh5MnJlttXrICOHbM/9v33sw+C2tnpgF94uCI6On8nlRw4UNe3XDmdFXsnQ8AnTNDBu+x07qyDo1WqZN7XqJFejyOvjEadSZpd0lpWEhP1sPmjR3Wgs3VrPZq3Vi29LS/8/eHIEf1vUpLOvixSRGcAF8acoOvX6wzo69d1APr113NORnxQ5Pf72a1b+n3Mzg4WLNBxuSeftPxC5aGVkpIWrExJAT8/i9222CEUtkf6ow83aRvbJO1iu6RtbJe0jW2yxf7oAznEW4j7gcFwZ8FJ0MGoJ57Qj/yQ2+zQVA0awM8/67kK0ycODRmig1VeXtaPU0oPX7dm4ED46KO00ZyxsYoyZUxcu2aXt8rlwYwZ+gE6UDhvHoSE5O5Yk0kPvX733ZzLLl+uA2dHjlgumn3yZObgZKlSOlOxTBldp6wWazaZdLD6n3/0/bFvn87UzM2Q63ffhYkT9c+//qrnTv3zz7wHJ0EHAsuW1dd15Eja9ieegB9+yH50bl5ERek5OWvW1Nc5ebK+B8+d03N3DhwIy5bpBbNSLV+uh/fPnp337OJUSumh7SdO6CkbfH3vyeXYPJMJ5s/Xr2lMjM7y/usvy2kvAF5+WU9R8N57OnD50MqvOUyFEEIIIYR4QEn4WghxTzz5pF7gJr1Ll/Sw86wcOADHjlluK1pUB5kyBrOcneGLLyKws8t70ne9ejpQlZSkg6a5sXWrDkBlXKU8K8OHwwsvWA55hqyfLzpar2uS3vTplr/7+emA49NP6yDwwoV6btQXXtCvXcah4OfO6fU4SpXSq7PXrq0D4V5eOuhcpozODn3vPT3twIkTev7P1OAk6KHpr76qpx64U1FRlsFJ0FMatGxpdTHjHCkFGzfq623eHF58UWeKNmmi52h1doZXXtGZvFev6szdrl0tg5Op9uyBGjX0Ned1/EBMjJ5btkEDeOYZfX+GhMCXX2Z/3Jdf6rqOGZM5y/h+EBGhp7js00dnSq5YoefCtfb6paTogHrZsnrqBCGEEEIIIYTIDRnibYUMqXl4SbvcHaX0wjVr1qRtc3PTmYHWFil6/XX4+OO030NC4MwZ60OBU9tm8+YAevSw3jYhIXrY+6JFsHu3nvPzmWf0FHCpoqP1EOvcLsjzxBP6fNndDosWWV9g6NlndbbeJ5/orDJr77b9+unV5WvX1sPDY2PT9g0fDl9/nfXz7tgBDRvm7jru1qZNesX2Bg10IDRViRKwYoWJunUNpKTkPIa7eHH48UcdrMyNU6egSxc4fPjO6p2dWrV0+9SokXWZiAg9lDkwEHr21FmY1jRooBe77tlTL8CUavFiHSxN9dFHen7YgpCQYOKVV2I5dMiNPn0MPP983s+xYQM8/7wOZueVnZ3OFG7aNO/HPuhscUiNsD3SH324SdvYJmkX2yVtY7ukbWyTLfZH5e4QQtwzBoNeuCa9mBgIDdXZlOmZTPC//1lue/rpnOcp7NpVZ6HNnp02hLRcOXj7bT3foa+vXufi22/1Ij3pg5Ogg0jr16etsp6TpUv185QvrwNar75qmQWYlAQvvZT5uKFD9XBxg0EHpPbvz7yyN+jrWLhQL5yUPjhpZ5dzFmODBjpzML81aKAzAAMD9dDvChX0dn9/vThT9erQsmXuVjm+dEkv6pQxW9Rk0tefkpKWhaqUHqqdH8FJ0G1SsyaMGKFXqc9o/ny9uFXp0nqkblbBSdDB4rVrdTBv1Sq9LSJCB53T+/bbe1Z9q1JSIC5O//zRRzB5sjubNhkYMiT3QXmTST9GjdKLJN1JcDK1Lt266SBzQYmJ0dd/+LDO4pWvYIUQQgghhLg/SIBSCHFPVamSeW7Ms2d1ZllSUtq2LVvg4kXLcr175+457Oygb1893+G5czqA8v77eghzbk2dqoNToDMvT53SAc6//rI+r+DJk3pY9aefWg6tXrYMLl+2LPvVV3pOxPRzjNaooYe+5jbjsU8fHXjNySefpM2bmV/efjstcFyuHBw6pIdKHz+ug5cATz0VZ/VYa/MQmkx66LvBoLNZx47Vw6Xd3PRr5uKi7yOjUWdu5lbt2noe1Dp19LBv0MPbFy/WAcjRo60f9/XX+rj0wayICJ0JeScBrvbt9bV5e+uM3vROn9bXntVcondjwwY9tNrVVWedjh1r+V/85MmZjzGZ9IJNNWvqqRAMBt1mdnY5D113dNTP88ILsH27fv6MmdLXr+t7ZtCg/A8WfvaZ/tt1ddVB82rVLAP44eFw4YKeBuD0aQleCiGEEEIIYVOUyCQiIkIBKiIiIt+eIyUlRV25ckWlpKTk23OIvJN2uTdOnlSqSBGldAgg7fHpp2llXnjBcl+1atmfMz/aJiVFqdOnlUpOttz+559K2dllrn/6R79+SvXvn3l7aKhSJlPWz7l3r1Kurtmfu2xZpa5ezdu1fPCBUo6O+nhnZ6V69FCqaVOlPDyUCghQ6umnlWrVKufrSv+oVUupH37IzeuYoi5duqJ69jQpUMrNTan33lMqKUnvP3dOqTZtcv+8d/IYO9ayTtHRSu3bp9SNG5bbly5VqmRJ6+dYvjyt3MSJ+Vvf0aPz1r4ZJSQo9c03Sr37rlI//aTUf/6T1v7ZPW7eTDuHyZT57zC7R4MGSu3ape+1L77Qr3FGyclKNWli/filS3N3bfv3K/Xxx0pt3Gh9/4IF+jn69NF/T0opdeSIUgZD1s87fbpSLi6W23v1UioxMW+v+71SUP/XFER/RuQf6Y8+3KRtbJO0i+2StrFd0ja2yRb7ozIHpRUy58/DS9rl3jlzRi9ocv685fZq1fQiLitWWG7/4AM9zDkrBd02v/yi569MyN3IZbMff9RzT2bn2DGdzbd/P1y5Yrlv8GA9L2dWK59nJzZWZ6z5+2e9eHBysl7Exs1ND8F1dtbZrGfP6sy/HTv0wjydO+cugxMs2+bKFSP+/jq7zrIMvPWWzta7Uz/+qDNRn34a/v5bb+vRA556Sg8lzml6gFQxMfocGYdsFymihwYfP65fi/j4O69rTgwGnXVYv76+x9av19mWxYvr7Fl7e91O//yj2zN1tffERD2EuW9fnb17J1IXUfr559yVd3DQmcNDh+ZuZe7Dh3U7ZexdtG2bNvw9Kxs3pr32BoNe6Khv37T9kyZZTpVgZ6eH4i9cmPvrSe/ll7NfyCu/2OKcP8L2SH/04SZtY5ukXWyXtI3tkraxTbbYH5UApRXSIXx4SbvcW1u26LkLc+P0aT3XX1YKo22OHIGZM3Ww7Ztvcl7R289PB2RdXPL2POvW6dW027YtuEVv7qXcto1Ser7Hb77J2/mNRhg2TA85Nhh0kPXGDT0nZm6DktYMHZp5JfWsODnpocyNGkGrVnoOza1bdZDRx+fO52l84gk9jP3WrbRtjzyig8OLFqUFyDt1gpIl9er2iYl39lx3okgRPYXA44/n7biXXrLezsePp81hmlFsrB6affp02jZXVz3vaWho5gWH7pUFC3SQOydHjqQt+lOzpg7wx8RAdt2E5GQdZPb01MHn/fv19po1ba9DKGyP9EcfbtI2tknaxXZJ29guaRvbZIsBSvts9wohxF1o3FhnQ/3xR/blQkOzD04WlqpV9RyPoOfQrFUr+/JTp+Y9OAl6Revcrmp9PzMY9PycdevqOUPTB6KsKVlSB45efdVybkN7e+urwufV11/rIOOBA9mXe+YZmDPHclvGBY9u3IAOHfTq8Rm9+abOKOzVK/O+pUszb9u1Sz/Sy5hxXBBmzYInn8w+AJeVL77Q84gOHWq5fcqUzAtppZo6NfM9ERurM7EXLbKc+zUn3t5w+3buyg4aBI8+mv09tW+fLhMdnbbNaEybS7RWLR28HjIkbVX4GTP0vXvrls70TF38CWDwYAOvvWZAKR1ol766EEIIIYR42EmXWAiRrz76SAcLsuLikhYEtGU1a+oVmVOHLjs766y6VGPH6oVWRPYMBujfXy9KlJKis+KmT4fWrdPKVK2qM9POn9f3xr0IRlpjZ6dXks9qODzoRWB++CHnc/n56aBibKwe4jxlStoq0h98oAPccXE6GJUfKlbUgbYdO/RzxsbCl1+aaN0653HqDg56tfT0+veHfv3uLDgJ+rV98UW9gn16X34J77yjh7M/95ye8uHdd3V28rRp1s+VmKgzSNNnmWbFYIDZs/XCVT17Zt7fsqVenTy96Ojs2/i33/TCS+mDk2C50NH+/brNa9fWAekhQ3R7pNY5fXAS4LvvDFSoEEixYsZcXZcQQgghhBAPOhnibYUMqXl4Sbvkj5Mn9TDWH3/Uc8ylqlkT1q7Vw0hzYittExOjV4QuWlQHdk6f1sGYkJBCq1Khuxdtk5yss+SuXtWBpfwK5Fmzdi28/jocPKjrAToQPXIkTJyYu3kXc2vCBB2gu1datIB586wHcVPbJSUlgFdeMbJ5s76ujh11IK90aT08vUkTPaT8iy90sLh6df3vncyDmtGZM3pl8Xvd02jQAI4etZx2wclJZ7r26JG2bdcuvSK7r6++1tRM7V699NDuVKVK6UzvHTv0MO6zZ/V5TpzIOuPzXrlwAUqUyJ9zyxDv+5v0Rx9u0ja2SdrFdknb2C5pG9tki0O8JUBphXQIH17SLvlLKZ0lNXeuDk5OmgTu7rk7VtrGdj0obZOQoIdox8frYbr+/vf+OaKi9FDh1LkIq1bVi8A89ZQe5jtpkg6YJifrqQ+OH7c+bDzVypV6aLk1ttIu3bvDr7/m7ZjixfUQ7ZiYzPuMRr3Q1O3bOoC8e7cO0H7xhR5mnRs7d+ogpy345x8oXz5/zi0Byvub9EcfbtI2tknaxXZJ29guaRvbZIsBSpmDUghRYAwGvdDICy8Udk2EyMzJSc+bmp88PGDbNh1Uq1gxcxB0yhTL300mXT42Vg+LHzkybZGcNm2gXbv8re+9MG0anDuXfaA1o1degbAw66u+9+6dFtBbtOjO6vTIIzpT9NChOzt+zBjdLukzwu9UbOzdn0MIIYQQQoj7nQQohRBCiALk7Jz71e2NxrSgaZs2MHiwXvU9MlJnTt4PX0IXKaIXI5ozJ+eFbhwd9fyNw4bpDMkff9TzSYLOkuzY8d4MuTYY9Py4jz2W++HndnZ6rtm339ave1iYnpvy0iW9P6uFeVav1kPIFy7U00L06gVRUSZiY28QHOxHkSL3QSMKIYQQQgiRzyRAKYQQQtwn7Ox0oPJ+k7oQT48eOsC3bx907qznXty1Sw/pbthQZ5X6+eljAgJ0huOaNVC5ctrq2PdK+/bw4Yd6/tHseHnpxZxatLDcHhCg6/7bb3rF8jp19Krnf/yhr/fbb+HZZ/XPoBfPSWUyQViYCT+/+yPILIQQQgghRH6TAKUQQgghCoSHh17JO72nn866vK+v9dW475XXXoP69XVWanQ0PP64nid0yBCduVmlis58rFLF+vFFi+rVyFP99pueX7RIkYd74SwhhBBCCCHySgKUQgghhHhoNWumH+mdOaNX8i5XLm8ZjkajzqQUQgghhBBC5I0EKIUQQggh0nF0hAoVCrsWQgghhBBCPDxk5iMhhBBCCCGEEEIIIUShkQClEEIIIYQQQgghhBCi0EiAUgghhBBCCCGEEEIIUWgkQCmEEEIIIUQuTJ48mVKlSuHs7EyDBg3YuXNntuUXLlxIpUqVcHZ2pnr16qxcubKAaiqEEEIIcX+RAKUQQgghhBA5WLBgAaNHj2bcuHHs3buXmjVr0q5dO8LCwqyW37p1K08//TSDBg1i3759dOnShS5dunD48OECrrkQQgghhO2TAKUQQgghhBA5+Oyzzxg8eDADBgygSpUqfPvtt7i6ujJjxgyr5b/88kvat2/Pa6+9RuXKlXn//fepU6cO33zzTQHXXAghhBDC9kmAUgghhBBCiGwkJiayZ88eWrdubd5mNBpp3bo127Zts3rMtm3bLMoDtGvXLsvyQgghhBAPM/vCroAQQgghhBC27MaNG6SkpBAYGGixPTAwkGPHjlk95urVq1bLX7161Wr5hIQEEhISzL9HRkYCYDKZMJlMd1P9LJlMJpRS+XZ+ceekbWyTtIvtkraxXdI2tqmg2iUv55cApRBCCCGEEIVs4sSJjB8/PtP269evEx8fny/PaTKZiIiIQCmF0SgDq2yJtI1tknaxXdI2tkvaxjYVVLtERUXluqwEKIUQQgghhMiGn58fdnZ2XLt2zWL7tWvXCAoKsnpMUFBQnsq/8cYbjB492vx7ZGQkJUuWxN/fH09Pz7u8AutMJhMGgwF/f3/50GhjpG1sk7SL7ZK2sV3SNrapoNrF2dk512UlQCmEEEIIIUQ2HB0dqVu3LmvXrqVLly6A7tivXbuW4cOHWz0mNDSUtWvXMmrUKPO2NWvWEBoaarW8k5MTTk5OmbYbjcZ8/eBgMBjy/TnEnZG2sU3SLrZL2sZ2SdvYpoJol7ycWwKUQgghhBBC5GD06NH069ePevXqUb9+fb744gtiYmIYMGAAAH379qV48eJMnDgRgJEjR9KsWTM+/fRTOnbsyPz589m9ezfTp08vzMsQQgghhLBJEqAUQgghhBAiBz179uT69euMHTuWq1evUqtWLf744w/zQjjnz5+3yBJo1KgR8+bN4+233+bNN9+kfPnyLFmyhGrVqhXWJQghhBBC2CwJUAohhBBCCJELw4cPz3JI94YNGzJt69GjBz169MjnWgkhhBBC3P9kAgAhhBBCCCGEEEIIIUShkQClEEIIIYQQQgghhBCi0EiAUgghhBBCCCGEEEIIUWgkQCmEEEIIIYQQQgghhCg0EqAUQgghhBBCCCGEEEIUGlnF2wqlFACRkZH59hwmk4moqCicnZ0xGiVObCukXWyXtI3tkraxTdIutqug2ia1H5ParxH3F+mPPtykbWyTtIvtkraxXdI2tskW+6MSoLQiKioKgJIlSxZyTYQQQggh7k5UVBReXl6FXQ2RR9IfFUIIIcSDIjf9UYOSr9UzMZlMXL58GQ8PDwwGQ748R2RkJCVLluTChQt4enrmy3OIvJN2sV3SNrZL2sY2SbvYroJqG6UUUVFRFCtWTDIW7kPSH324SdvYJmkX2yVtY7ukbWyTLfZHJYPSCqPRSIkSJQrkuTw9PeWP1AZJu9guaRvbJW1jm6RdbFdBtI1kTt6/pD8qQNrGVkm72C5pG9slbWObbKk/Kl+nCyGEEEIIIYQQQgghCo0EKIUQQgghhBBCCCGEEIVGApSFxMnJiXHjxuHk5FTYVRHpSLvYLmkb2yVtY5ukXWyXtI2wFXIv2i5pG9sk7WK7pG1sl7SNbbLFdpFFcoQQQgghhBBCCCGEEIVGMiiFEEIIIYQQQgghhBCFRgKUQgghhBBCCCGEEEKIQiMBSiGEEEIIIYQQQgghRKGRAKUQQgghhBBCCCGEEKLQSICyEEyePJlSpUrh7OxMgwYN2LlzZ2FX6YG3adMmOnfuTLFixTAYDCxZssRiv1KKsWPHUrRoUVxcXGjdujUnTpywKBMeHk6fPn3w9PTE29ubQYMGER0dXYBX8eCZOHEijzzyCB4eHgQEBNClSxeOHz9uUSY+Pp5hw4ZRpEgR3N3d6datG9euXbMoc/78eTp27IirqysBAQG89tprJCcnF+SlPHCmTp1KjRo18PT0xNPTk9DQUH7//XfzfmkX2zBp0iQMBgOjRo0yb5O2KRzvvvsuBoPB4lGpUiXzfmkXYWukP1rwpD9qm6Q/arukP3p/kP6o7bjf+6MSoCxgCxYsYPTo0YwbN469e/dSs2ZN2rVrR1hYWGFX7YEWExNDzZo1mTx5stX9H330EV999RXffvstO3bswM3NjXbt2hEfH28u06dPH44cOcKaNWtYsWIFmzZt4vnnny+oS3ggbdy4kWHDhrF9+3bWrFlDUlISbdu2JSYmxlzm5ZdfZvny5SxcuJCNGzdy+fJlunbtat6fkpJCx44dSUxMZOvWrcyePZtZs2YxduzYwrikB0aJEiWYNGkSe/bsYffu3bRs2ZInnniCI0eOANIutmDXrl1MmzaNGjVqWGyXtik8VatW5cqVK+bH5s2bzfukXYQtkf5o4ZD+qG2S/qjtkv6o7ZP+qO25r/ujShSo+vXrq2HDhpl/T0lJUcWKFVMTJ04sxFo9XAC1ePFi8+8mk0kFBQWpjz/+2Lzt9u3bysnJSf3vf/9TSil19OhRBahdu3aZy/z+++/KYDCoS5cuFVjdH3RhYWEKUBs3blRK6XZwcHBQCxcuNJf5+++/FaC2bdumlFJq5cqVymg0qqtXr5rLTJ06VXl6eqqEhISCvYAHnI+Pj/r++++lXWxAVFSUKl++vFqzZo1q1qyZGjlypFJK/mYK07hx41TNmjWt7pN2EbZG+qOFT/qjtkv6o7ZN+qO2Q/qjtud+749KBmUBSkxMZM+ePbRu3dq8zWg00rp1a7Zt21aINXu4nTlzhqtXr1q0i5eXFw0aNDC3y7Zt2/D29qZevXrmMq1bt8ZoNLJjx44Cr/ODKiIiAgBfX18A9uzZQ1JSkkXbVKpUieDgYIu2qV69OoGBgeYy7dq1IzIy0vztqrg7KSkpzJ8/n5iYGEJDQ6VdbMCwYcPo2LGjRRuA/M0UthMnTlCsWDHKlClDnz59OH/+PCDtImyL9Edtk/RHbYf0R22T9Edtj/RHbdP93B+1z/dnEGY3btwgJSXForEBAgMDOXbsWCHVSly9ehXAaruk7rt69SoBAQEW++3t7fH19TWXEXfHZDIxatQoGjduTLVq1QD9ujs6OuLt7W1RNmPbWGu71H3izh06dIjQ0FDi4+Nxd3dn8eLFVKlShf3790u7FKL58+ezd+9edu3alWmf/M0UngYNGjBr1iwqVqzIlStXGD9+PE2bNuXw4cPSLsKmSH/UNkl/1DZIf9T2SH/UNkl/1Dbd7/1RCVAKIWzCsGHDOHz4sMUcGaJwVaxYkf379xMREcEvv/xCv3792LhxY2FX66F24cIFRo4cyZo1a3B2di7s6oh0OnToYP65Ro0aNGjQgJCQEH7++WdcXFwKsWZCCCFyS/qjtkf6o7ZH+qO2637vj8oQ7wLk5+eHnZ1dplWSrl27RlBQUCHVSqS+9tm1S1BQUKaJ45OTkwkPD5e2uweGDx/OihUrWL9+PSVKlDBvDwoKIjExkdu3b1uUz9g21toudZ+4c46OjpQrV466desyceJEatasyZdffintUoj27NlDWFgYderUwd7eHnt7ezZu3MhXX32Fvb09gYGB0jY2wtvbmwoVKnDy5En5mxE2Rfqjtkn6o4VP+qO2Sfqjtkf6o/eP+60/KgHKAuTo6EjdunVZu3ateZvJZGLt2rWEhoYWYs0ebqVLlyYoKMiiXSIjI9mxY4e5XUJDQ7l9+zZ79uwxl1m3bh0mk4kGDRoUeJ0fFEophg8fzuLFi1m3bh2lS5e22F+3bl0cHBws2ub48eOcP3/eom0OHTpk0WFfs2YNnp6eVKlSpWAu5CFhMplISEiQdilErVq14tChQ+zfv9/8qFevHn369DH/LG1jG6Kjozl16hRFixaVvxlhU6Q/apukP1p4pD96f5H+aOGT/uj9477rj+b7MjzCwvz585WTk5OaNWuWOnr0qHr++eeVt7e3xSpJ4t6LiopS+/btU/v27VOA+uyzz9S+ffvUuXPnlFJKTZo0SXl7e6ulS5eqgwcPqieeeEKVLl1axcXFmc/Rvn17Vbt2bbVjxw61efNmVb58efX0008X1iU9EF588UXl5eWlNmzYoK5cuWJ+xMbGmsu88MILKjg4WK1bt07t3r1bhYaGqtDQUPP+5ORkVa1aNdW2bVu1f/9+9ccffyh/f3/1xhtvFMYlPTDGjBmjNm7cqM6cOaMOHjyoxowZowwGg1q9erVSStrFlqRfNVEpaZvC8sorr6gNGzaoM2fOqC1btqjWrVsrPz8/FRYWppSSdhG2RfqjhUP6o7ZJ+qO2S/qj9w/pj9qG+70/KgHKQvD111+r4OBg5ejoqOrXr6+2b99e2FV64K1fv14BmR79+vVTSillMpnUO++8owIDA5WTk5Nq1aqVOn78uMU5bt68qZ5++mnl7u6uPD091YABA1RUVFQhXM2Dw1qbAGrmzJnmMnFxcWro0KHKx8dHubq6qieffFJduXLF4jxnz55VHTp0UC4uLsrPz0+98sorKikpqYCv5sEycOBAFRISohwdHZW/v79q1aqVuTOolLSLLcnYIZS2KRw9e/ZURYsWVY6Ojqp48eKqZ8+e6uTJk+b90i7C1kh/tOBJf9Q2SX/Udkl/9P4h/VHbcL/3Rw1KKZX/eZpCCCGEEEIIIYQQQgiRmcxBKYQQQgghhBBCCCGEKDQSoBRCCCGEEEIIIYQQQhQaCVAKIYQQQgghhBBCCCEKjQQohRBCCCGEEEIIIYQQhUYClEIIIYQQQgghhBBCiEIjAUohhBBCCCGEEEIIIUShkQClEEIIIYQQQgghhBCi0EiAUgghhBBCCCGEEEIIUWgkQCmEEEIIIYQQQgghhCg0EqAUQoi7dP36dV588UWCg4NxcnIiKCiIdu3asWXLFgAMBgNLliwp3EoKIYQQQogHlvRHhRD3O/vCroAQQtzvunXrRmJiIrNnz6ZMmTJcu3aNtWvXcvPmzcKumhBCCCGEeAhIf1QIcb8zKKVUYVdCCCHuV7dv38bHx4cNGzbQrFmzTPtLlSrFuXPnzL+HhIRw9uxZAJYuXcr48eM5evQoxYoVo1+/frz11lvY2+vvjgwGA1OmTGHZsmVs2LCBokWL8tFHH9G9e/cCuTYhhBBCCGH7pD8qhHgQyBBvIUSBmjVrFgaDwdwpAmjevDnNmzcvtDrdDXd3d9zd3VmyZAkJCQmZ9u/atQuAmTNncuXKFfPvf/31F3379uX8+fN07NiRadOmMWvWLD744AOL49955x26devGgQMH6NOnD7169eLvv/8GdGezU6dO+XyF987Zs2cxGAzMmjUr12U/+eST/K9YHlm7h4UQQgghCsvd9kdHjhzJ0aNH76g/KoQQ94oEKIXIpSlTpmAwGGjQoEFhV8UmpaSkMHPmTJo3b46vry9OTk6UKlWKAQMGsHv37sKuHlu3buXdd9/l9u3bOZYdOnQoRqOR8PBwi+3h4eEYjUacnJyIj48HwN7enlmzZjFjxgycnZ0pUaIEb775JgcPHgTA398fAG9vb4KCgsy/jx8/njFjxuDu7o6Hhwdt2rTh/fffZ9q0aRbP2aNHD5577jkqVKjA+++/T7169fj666/zdO1Hjx7l3XfftcmA2sqVK3n33Xfz5dxnz55lwIABlC1bFmdnZ4KCgnj00UcZN26cRbkpU6bkKmiaV++++y4Gg8H8cHV1JTg4mM6dOzNz5kyrHyCEEEIIIfIqtT86e/ZsvL29ady4cZ76o/369aNMmTL52h8VQoicSIBSiFyaO3cupUqVYufOnZw8ebKwq2NT4uLi6NSpEwMHDkQpxZtvvsnUqVPp27cv27Zto379+ly8eDHL41evXs3q1avztY5bt25l/PjxuQpQNmnSBKWUeVLx9OcwGo0kJSVZBF27devG559/DkCLFi3YsGEDderUyTbodeDAAd577z2uX7/OwoULcXd3Z/DgwVy5coXY2FhzudDQUIvjQkND8/yN9dGjRxk/fnyhByhDQkKIi4vj2WefNW9buXIl48ePv+fPdfLkSWrXrs2qVat4+umn+eabbxg2bBhFihThww8/tCibXwHKVFOnTmXOnDl8/fXXPPfcc4SHhzNw4EDq16/PhQsX8u15hRBCCPHw6NatG5cvX2bZsmW0b98+T/3R1AzM/OyPCiFETmSRHCFy4cyZM2zdupVFixYxZMgQ5s6dmykLK7+ZTCYSExNxdnYu0OfNjddee40//viDzz//nFGjRlnsGzdunDl4lxVHR8d8rF3eNWnSBIDNmzfTuXNn8/YtW7ZQo0YN4uLi2Lx5s7kcwI4dOzAajXz99dd4e3vz3HPPMW7cOPr372/1OaKjoxk/fjxdu3bNtM8W2/heMBgMBXZtn3/+OdHR0ezfv5+QkBCLfWFhYQVSh1Tdu3fHz8/P/PvYsWOZO3cuffv2pUePHmzfvr1A6yOEEEKIB5OzszNt2rShTZs2vPPOO9IfFULcVySDUohcmDt3Lj4+PnTs2JHu3bszd+5c876kpCR8fX0ZMGBApuMiIyNxdnbm1VdfNW9LSEhg3LhxlCtXDicnJ0qWLMnrr7+eabinwWBg+PDhzJ07l6pVq+Lk5MQff/wBwCeffEKjRo0oUqQILi4u1K1bl19++SXT88fFxTFixAj8/Pzw8PDg8ccf59KlSxgMhkzDai9dusTAgQMJDAzEycmJqlWrMmPGjBxfm4sXLzJt2jTatGmTKTgJYGdnx6uvvkqJEiWyPIe1OSjz+jotWbKEatWqmeue+lqBHmr72muvAVC6dGnzkNusMgqDg4MpWbJkpgzKLVu20LhxYxo1amR1X9WqVfH29gagQoUKXL9+nXLlygHw3HPPWdS/Tp06HD9+nNatWzNhwgTKlStnfhw+fNg8wfnQoUOZMGECM2fOxGAwsGHDBipXrmzx3Js3b6Z+/fo4OztTpkwZfvzxR/O+WbNm0aNHD0Bnd6Ze+4YNGwDYvXs37dq1w8/PDxcXF0qXLs3AgQOtvi6pRo8eTZEiRUi/xtpLL72EwWDgq6++Mm+7du0aBoOBqVOnApnnoOzfvz+TJ08GsBgKndH06dMpW7YsTk5OPPLII+Z5k7Jz6tQpSpQokSk4CRAQEGD+uVSpUhw5coSNGzeanz/9vXjkyBFatmyJi4sLJUqUYMKECZhMphyfPyd9+vThueeeY8eOHaxZs8Zi344dO2jfvj1eXl64urrSrFkzi/vtl19+wWAwsHHjxkznnTZtGgaDgcOHD991HYUQQghxf6tSpQoxMTEAODg4kJKSYrE/tT+avh+a+jAa00IFGb9M3b59e6b+qBBC3C3JoBQiF+bOnUvXrl1xdHTk6aefZurUqezatYtHHnkEBwcHnnzySRYtWsS0adMssgFTJ6ru1asXoLMgH3/8cTZv3szzzz9P5cqVOXToEJ9//jn//PMPS5YssXjedevW8fPPPzN8+HD8/PwoVaoUAF9++SWPP/44ffr0ITExkfnz59OjRw9WrFhBx44dzcf379+fn3/+mWeffZaGDRuyceNGi/2prl27RsOGDc3BPn9/f37//XcGDRpEZGSk1cBjqt9//53k5GSLYbt3K6+v0+bNm1m0aBFDhw7Fw8ODr776im7dunH+/HmKFClC165d+eeff/jf//7H559/bs5mS51/x5omTZqwaNEiEhIScHJyIjExkV27dvHiiy8SGxvL66+/jlKK8PBwunTpwtGjR+nRowdnzpxh586dvPPOO5hMJjp37sy8efMICAjg888/58iRI/z222+MHTuWTp064ebmxu3bt/n77785cOAAW7duZe7cueZAnVKKKVOm4OXlBeihOOkD5CdPnqR79+4MGjSIfv36MWPGDPr370/dunWpWrUqjz76KCNGjOCrr77izTffNHcmK1euTFhYGG3btsXf358xY8bg7e3N2bNnWbRoUbbt07RpU/O1VKtWDdCTrBuNRv766y9GjBhh3gbw6KOPWj3PkCFDuHz5MmvWrGHOnDlWy8ybN4+oqCiGDBmCwWDgo48+omvXrpw+fRoHB4cs6xgSEsKff/7JunXraNmyZZblvvjiC1566SXc3d156623AAgMDATg6tWrtGjRguTkZMaMGYObmxvTp0/HxcUl29cnt5599lmmT5/O6tWradOmDaD/5jt06EDdunUZN24cRqORmTNn0rJlS/766y/q169Px44dcXd35+eff860UueCBQuoWrWquV2EEEII8eC7efMmPXr0YODAgdSoUQMPDw92797NRx99xBNPPAHoL2XXrl1L48aNcXJywsfHx9wfDQ4Opnv37hiNRg4cOMDhw4eZMGGC+fwLFy6kXr16NGnShLlz57Jz505++OGHwrpcIcSDSgkhsrV7924FqDVr1iillDKZTKpEiRJq5MiR5jKrVq1SgFq+fLnFsY899pgqU6aM+fc5c+Yoo9Go/vrrL4ty3377rQLUli1bzNsAZTQa1ZEjRzLVKTY21uL3xMREVa1aNdWyZUvztj179ihAjRo1yqJs//79FaDGjRtn3jZo0CBVtGhRdePGDYuyvXr1Ul5eXpmeL72XX35ZAWrfvn1Zlklv5syZClBnzpwxb2vWrJlq1qyZ+fe8vk6Ojo7q5MmT5m0HDhxQgPr666/N2z7++ONMz5udyZMnK8Bch23btilAnTt3Th09elQB6siRIyo+Pl51795dAcrFxUW5urqqoKAgZTAY1J9//qmUUmrZsmWqXLlyymg0WtT/jz/+UE5OTsrOzk55enqq+vXrq5YtWyqDwaD27dunADV58mTVvHlzBWS6ppCQEAWoTZs2mbeFhYUpJycn9corr5i3LVy4UAFq/fr1Fte4ePFiBahdu3bl6jVJ/xyAmjJlilJKqdu3byuj0ah69OihAgMDzeVGjBihfH19lclkUkopdebMGQWomTNnmssMGzZMWfuvKLVskSJFVHh4uHn70qVLrf6tZXT48GHl4uKiAFWrVi01cuRItWTJEhUTE5OpbNWqVS3uv1SjRo1SgNqxY4fFtXt5eeXqXho3bpwC1PXr163uv3XrlgLUk08+qZTS7y3ly5dX7dq1M79mSum/99KlS6s2bdqYtz399NMqICBAJScnm7dduXJFGY1G9d5772VbLyGEEEI8WOLj49WYMWNUnTp1lJeXl3J1dVUVK1ZUb7/9trkfn9oftbe3VyEhIeZj//jjD9WoUSPl4uJi7o9Onz7dvD+1P9qmTRvl5OSkSpUqpRYsWFDQlyiEeAjIEG8hcjB37lwCAwNp0aIFoIei9uzZk/nz55uHSbRs2RI/Pz8WLFhgPu7WrVusWbOGnj17mrctXLiQypUrU6lSJW7cuGF+pGZ4rV+/3uK5mzVrRpUqVTLVKX0G161bt4iIiKBp06bs3bvXvD11iPPQoUMtjn3ppZcsfldK8euvv9K5c2eUUhb1ateuHRERERbnzSgyMhIADw+PLMvkVV5fp9atW1O2bFnz7zVq1MDT05PTp0/fcR3Sz0MJegh38eLFCQ4OplKlSvj6+rJlyxacnJwoX748AMeOHSMmJob69etTpUoVatasyY0bNwgNDWXbtm0cO3bMov7t2rUjKCiIZ555hoiICHbs2MGFCxcIDQ2lVq1aABQrVoz169eb261Tp04W9axSpQpNmzY1/+7v70/FihVzde2pw9FXrFhBUlJSrl8bf39/KlWqxKZNm8yvjZ2dHa+99hrXrl3jxIkTgM6gbNKkidVh27nVs2dPfHx8zL+nXmtO11e1alX279/PM888w9mzZ/nyyy/p0qULgYGBfPfdd7l67pUrV9KwYUPq169v3ubv70+fPn3u4Eoyc3d3ByAqKgqA/fv3c+LECXr37s3NmzfN931MTAytWrVi06ZN5uHlPXv2JCwszDxUH/TQb5PJZPGeI4QQQogHn5OTExMnTmTPnj3cvn2bmJgYjh07xvvvv2/+3NC5c2dOnDhBUlKSxTRH7dq1Y8uWLcTGxpr7o4MHD7Y4f7FixVi9ejXx8fGcOXOGp556qiAvTwjxkJAh3kJkIyUlhfnz59OiRQvOnDlj3t6gQQM+/fRT1q5dS9u2bbG3t6dbt27MmzfPPCR40aJFJCUlWQQLTpw4wd9//53l0OKMi3eULl3aarkVK1YwYcIE9u/fbzEnY/pA0Llz5zAajZnOkTonYqrr169z+/Ztpk+fzvTp03NVr/Q8PT2BtCDLvZDX1yk4ODhTGR8fH27dunXHdahWrRre3t7muf9S558E/TqHhoayZcsWBg8ezJYtWyhZsqS5Hnmtf3rnzp3LtFIiZG63VHdz7c2aNaNbt26MHz+ezz//nObNm9OlSxd69+6Nk5NTtsc2bdqUlStXAjoQWa9ePerVq4evry9//fUXgYGBHDhwgN69e+dYj+xkvL7UYGVurq9ChQrMmTOHlJQUjh49yooVK/joo494/vnnKV26NK1bt872+HPnztGgQYNM2ytWrJiHK8hadHQ0kBbcTw3s9uvXL8tjIiIi8PHxMc9RuWDBAlq1agXo4d21atWiQoUK96R+QgghhBBCCFFQJEApRDbWrVvHlStXmD9/PvPnz8+0f+7cubRt2xaAXr16MW3aNH7//Xe6dOnCzz//TKVKlahZs6a5vMlkonr16nz22WdWn69kyZIWv1ub6+6vv/7i8ccf59FHH2XKlCkULVoUBwcHZs6cybx58/J8jakZWc8880yWgZEaNWpkeXylSpUAOHTokDnr727l9XWys7OzWk6lW8Qlr4xGI6GhoWzduhWlFFu2bOHNN98072/UqBEzZswwz03ZpUuXO67/3bibazcYDPzyyy9s376d5cuXs2rVKgYOHMinn37K9u3bzRl+1jRp0oTvvvuO06dP89dff9G0aVMMBgNNmjThr7/+olixYphMJovszjtxL9rWzs6O6tWrU716dUJDQ2nRogVz587NMUCZ31IXskkNPqf+LX788cdZ/i2ltomTkxNdunRh8eLFTJkyhWvXrrFlyxb++9//5n/FhRBCCCGEEOIekwClENmYO3cuAQEB5pWG01u0aBGLFy/m22+/xcXFhUcffZSiRYuyYMECmjRpwrp168yLbqQqW7YsBw4coFWrVnc87PXXX3/F2dmZVatWWWS5zZw506JcSEgIJpOJM2fOmIcgg15UJT1/f388PDxISUm5o4BNhw4dsLOz46effrpnC+Xci9cpozs5T5MmTfj9999ZtmwZYWFh5gxK0AHKt956i5UrVxIXF2ceEn639Q8JCTG3UfogXMZ2y4uc6tCwYUMaNmzIBx98wLx58+jTpw/z58/nueeey/KY1MDjmjVr2LVrF2PGjAH0gjhTp06lWLFiuLm5Ubdu3buq271Wr149AK5cuZJjHUJCQsxZjekdP378ntQldWGgdu3aAZinKfD09MzV32LPnj2ZPXs2a9eu5e+//0YpJcO7hRBCCHFP3c0X/kIIkRcyB6UQWYiLi2PRokV06tSJ7t27Z3oMHz6cqKgoli1bBuiMu+7du7N8+XLmzJlDcnJypmDBU089xaVLl6zOgRcXF0dMTEyO9bKzs8NgMJjnvwQ4e/ZsppWtU4MeU6ZMsdj+9ddfZzpft27d+PXXX80ZXeldv3492/qULFmSwYMHs3r16kznBp0V9umnn3Lx4sVsz5PevXidMnJzcwPg9u3buT4mNej44Ycf4urqapHVVr9+fezt7fnoo48syt5t/du1a8e2bdvYv3+/eVt4eLjFyt15ldW137p1K1OnM/Ua008dYE3p0qUpXrw4n3/+OUlJSebgbdOmTTl16hS//PILDRs2xN4+++/B7qRdcuOvv/6yOq9m6rD09MO0U1dSz+ixxx5j+/bt7Ny507zt+vXrd9UWqebNm8f3339PaGioeYh23bp1KVu2LJ988ol5+Hd6Gf8WW7duja+vLwsWLGDBggXUr18/y2khhBBCCCGEEMKWSQalEFlYtmwZUVFRPP7441b3N2zYEH9/f+bOnWsORPbs2ZOvv/6acePGUb16dSpXrmxxzLPPPsvPP//MCy+8wPr162ncuDEpKSkcO3aMn3/+mVWrVpkzvLLSsWNHPvvsM9q3b0/v3r0JCwtj8uTJlCtXjoMHD5rL1a1bl27duvHFF19w8+ZNGjZsyMaNG/nnn38Ay6yxSZMmsX79eho0aMDgwYOpUqUK4eHh7N27lz///JPw8PBs6/Tpp59y6tQpRowYYQ7q+vj4cP78eRYuXMixY8fo1atXtue4169TRqmZfG+99Ra9evXCwcGBzp07mwNk1tSvXx9HR0e2bdtG8+bNLYJtrq6u1KxZk23btuHt7U21atXuSf1ff/11fvrpJ9q0acNLL72Em5sb33//PcHBwYSHh99RxmGtWrWws7Pjww8/JCIiAicnJ1q2bMm8efOYMmUKTz75JGXLliUqKorvvvsOT09PHnvssRzP27RpU+bPn0/16tXNc0PWqVMHNzc3/vnnn1zNP5naLiNGjKBdu3bY2dnl6V7JyocffsiePXvo2rWreYqCvXv38uOPP+Lr68uoUaMs6jB16lQmTJhAuXLlCAgIoGXLlrz++uvMmTOH9u3bM3LkSNzc3Jg+fTohISEWf2s5+eWXX3B3dycxMZFLlyZVyqAAAE6hSURBVC6xatUqtmzZQs2aNVm4cKG5nNFo5Pvvv6dDhw5UrVqVAQMGULx4cS5dusT69evx9PRk+fLl5vIODg507dqV+fPnExMTwyeffHLXr5sQQgghhBBCFIrCWj5cCFvXuXNn5ezsrGJiYrIs079/f+Xg4KBu3LihlFLKZDKpkiVLKkBNmDDB6jGJiYnqww8/VFWrVlVOTk7Kx8dH1a1bV40fP15FRESYywFq2LBhVs/xww8/qPLlyysnJydVqVIlNXPmTDVu3DiV8U86JiZGDRs2TPn6+ip3d3fVpUsXdfz4cQWoSZMmWZS9du2aGjZsmCpZsqRycHBQQUFBqlWrVmr69Om5er2Sk5PV999/r5o2baq8vLyUg4ODCgkJUQMGDFD79u0zl5s5c6YC1JkzZ8zbmjVrppo1a3ZPX6eQkBDVr18/i23vv/++Kl68uDIajZnqkJXQ0FAFqDfffDPTvhEjRihAdejQIdO+3NbfWj337dunmjZtqpycnFSJEiXUxIkT1VdffaUAdfXqVYtjO3bsmOm5rb2e3333nSpTpoyys7NTgFq/fr3au3evevrpp1VwcLBycnJSAQEBqlOnTmr37t05vi5KKTV58mQFqBdffNFie+vWrRWg1q5da7H9zJkzClAzZ840b0tOTlYvvfSS8vf3VwaDwXwPp5b9+OOPMz0voMaNG5dt3bZs2aKGDRumqlWrZr4fg4ODVf/+/dWpU6csyl69elV17NhReXh4KMDitTt48KBq1qyZcnZ2VsWLF1fvv/+++uGHH3J1/6T+TaY+nJ2dVYkSJVSnTp3UjBkzVHx8vNXj9u3bp7p27aqKFCminJycVEhIiHrqqacyvZ5KKbVmzRoFKIPBoC5cuJBtfYQQQgghhBDCVhmUkkklhHiY7N+/n9q1a/PTTz/Rp0+fwq6OyKVRo0Yxbdo0oqOjs1w4RgghhBBCCCGEuB/JHJRCPMDi4uIybfviiy8wGo08+uijhVAjkRsZ2+3mzZvMmTOHJk2aSHBSCCGEEEIIIcQDR+agFOIB9tFHH7Fnzx5atGiBvb09v//+O7///jvPP/88JUuWLOzqiSyEhobSvHlzKleuzLVr1/jhhx+IjIzknXfeKeyqCSGEEEIIIYQQ95wM8RbiAbZmzRrGjx/P0aNHiY6OJjg4mGeffZa33norx9WVReF58803+eWXX7h48SIGg4E6deowbtw4WrduXdhVE0IIIYQQQggh7jkJUAohhBBCCCGEEEIIIQqNzEEphBBCCCGEEEIIIYQoNBKgFEIIIYQQQgghhBBCFBqZhM4Kk8nE5cuX8fDwwGAwFHZ1hBBCCCHyTClFVFQUxYoVw2iU76SFEEIIIYTtkgClFZcvX5YVjoUQQgjxQLhw4QIlSpQo7GoIIYQQQgiRJQlQWuHh4QHoDr2np2ch10YIIYQQIu8iIyMpWbKkuV8jhBBCCCGErZIApRWpw7o9PT0lQCmEEEKI+5pMVyOEEEIIIWydTEgkhBBCCCGEEEIIIYQoNBKgFEIIIYQQQgghhBBCFBoJUAohhBBCCCGEEEIIIQqNBCiFEEIIIYQQQgghhBCFRgKUQgghhBBCCCGEEEKIQiMBSiGEEEIIIYQQQgghRKGRAKUQQgghhBBCCCGEEKLQSIBSCCGEEEIIIYQQQghRaCRAKYQQQgghhBBCCCGEKDQSoHwIJSUlMXz4cHx8fPD19eWll14iOTk5y/IvvfQSJUuWxNPTk+LFizNq1CgSExPN+7t3707RokXx9PSkdOnSTJgwweL4UqVK4eLigru7O+7u7nh7e+fXpbFkyRLKly+Pq6srTZo04dixY3dcfvbs2dSvXx8vLy+KFi3KoEGDuH37tnn/zJkzqVixIl5eXvj5+dG1a1fOnz+fX5cmhBBCCCGEEEII8UCSAOVDaMKECWzevJmjR49y5MgR/vrrL/773/9mWX7o0KEcO3aMyMhIDhw4wIEDB/joo4/M+8eNG8fZs2eJjIxk48aNzJs3j59++sniHP/73/+Ijo4mOjraIsiX6sqVK+zduzfTY8iQIQwZMsTqvr1793LlyhXzOY4fP06fPn34/PPPCQ8Pp2XLljzxxBNZBl9zKh8bG8tHH33EtWvXOHLkCFeuXGHo0KHm41u2bMmWLVuIiIjg4sWLlC1bloEDB+aqDUT+KMjg+z///MOTTz5JUFAQ3t7eNG7cmC1btuTbtU2bNo3g4GDc3Nzo2LGjxb2f1/Lr16+nRYsWeHl5Wf3CIDIykn79+hEQEICvry/t27fn1KlT9/qSRB4U5L2dkJBA8+bNCQgIwNPTk0qVKjF9+vR8u7aCvLfDwsLo1asX/v7++Pv78+qrr5KSknKvL0kIIYQQQgiRV0pkEhERoQAVERFR2FXJFyVKlFALFy40//7zzz+r4ODgXB0bFhamWrZsqfr27Wt1//nz51XVqlXV2LFjzdtCQkLU4sWLsz3vuHHjFJDnx7hx48znePvtt1XHjh3NvycmJipvb2+1bt06q8+Z1/JLly5VJUuWtLovNjZWvf7667l+HUX+GDt2rKpZs6a6fPmyunz5sqpZs6YaP358luWPHj2qoqOjlVJKXb9+XTVv3ly9//775v0HDx5U8fHxSimlzp07pypXrqzmzJmjlFJqx44datq0aSosLEwlJyer6dOnK09PT3X9+vVc1bVfv35q5syZuSq7du1a5eXlpbZv366io6PVgAEDVIsWLe64/I4dO9SPP/6ovv/+e+Xl5ZXp+JEjR6oGDRqo69evq/j4eDVkyBDVsGHDXNVV5I+CvLeTk5PVwYMHVVJSklJKqSNHjqiAgAC1adOmXNXVlu/ttm3bqr59+6qYmBh16dIlVatWLfXBBx/kqq73owe9PyOEEEIIIR4cNpFBOXnyZEqVKoWzszMNGjRg586dWZZdtGgR9erVw9vbGzc3N2rVqsWcOXMsyvTv3x+DwWDxaN++fX5fxn3h1q1bXLx4kVq1apm31apVi/PnzxMREZHlcZMmTcLd3Z2AgAAOHDjASy+9ZLF/6NChuLq6EhwcTHR0NP3797fYP2TIEPz8/AgNDWXlypWZzj9kyBD27NmT6eHq6gqAv7+/1f1Dhgwxn+PgwYMW1+Xg4ECVKlU4ePCg1WvKa/mNGzdSo0YNi22bN2/G29sbV1dXPvvsM9566y2rx4qCMWPGDN5++22KFi1K0aJFeeutt/jhhx+yLF+5cmXc3NwAUEphNBo5ceKEeX/16tVxcnICwGAwWOyvX78+zz//PP7+/tjZ2TF48GDs7OyyvH/uxsyZM3nmmWdo0KABbm5uTJw4kY0bN3L69Ok7Kl+/fn2effZZypYta/X406dP8/jjj+Pn54eTkxPPPvsshw4duufXJXKvIO9tOzs7qlevjr29vXm/wWDg5MmT9/y6CvLejomJYc2aNYwbNw5XV1eKFSvGqFGj8jU7VAghhBBCCJE7hR6gXLBgAaNHj2bcuHHs3buXmjVr0q5dO8LCwqyW9/X15a233mLbtm0cPHiQAQMGMGDAAFatWmVRrn379ly5csX8+N///lcQl2PzoqOjASyGvqX+HBUVleVxY8aMITo6mqNHj/LCCy8QFBRksX/KlClER0eza9cu+vbti4+Pj3nfnDlzOHPmDJcuXeKll16iW7du7Nq1y+L4okWLUqdOHerUqUP//v1p2bIlLVu2JDY2FoAbN26Yt6WWq1OnDkWLFrW4toxD+ry9vbO8rryU//333/n++++ZOHGixfYmTZpw+/Ztrl+/zvvvv0+VKlWsv4Ai3xVW8D3VoUOHiIqKypd7IGMwPTAwkKCgoCyDhnktn9Hw4cNZtWoVV69eJS4ujlmzZtG5c+e7uQRxFwrr3u7UqRPOzs5UqVKFwMBAnnzyyXt5WUDB3ttKKfMjlclk4ty5c0RGRt7xNQghhBBCCCHuXqEHKD/77DMGDx7MgAEDqFKlCt9++y2urq7MmDHDavnmzZvz5JNPUrlyZcqWLcvIkSOpUaMGmzdvtijn5OREUFCQ+ZE+YPYwc3d3B7D4UJv6s4eHR47HV65cmZo1a1oN0hiNRurVq4eHhwevvvqqeXvTpk1xdXXFycmJ3r1707lzZ3799dcsn+PgwYPcvn2b27dvm+tUrFgx87bsri3jh/WIiIgsryu35detW8czzzzDokWLqF69utVz+fn5MWjQIDp16kRMTEyWdRT5pzCC76lu375Nr169ePPNNzMdn563t7f5MW/ePIYOHWr+PWN2bsZry6/guzU1a9Y0Lw7l4eHB5s2b+fjjj3N1rLj3CuveXrFiBTExMWzYsIFu3brh4uJisT/93MEeHh7mx9y5c3nhhRfMv5cvXz7LuYML8t52d3fn0UcfZdy4cURHR3P+/Hm+/PJLAAlQCiGEEEIIUcgKNUCZmJjInj17aN26tXmb0WikdevWbNu2LcfjlVKsXbuW48eP8+ijj1rs27BhAwEBAVSsWJEXX3yRmzdv3vP63498fHwoUaIE+/fvN2/bv38/JUuWxMvLK1fnSEpKshgqmNf9RmP+3HY1atSwuK6kpCSOHj2aZVAxN+XXrVtH9+7dmTdvHq1atcr2+ZOSkoiIiMgy+1fkr8IIvqc+R7t27WjSpAnvvvtupmPTB3HWrVtnfrRr147XX3/d/PusWbOyDOLkV/A9K927d8fT05Pw8HBiY2N54YUXaNq0qTmjWRSswrq3QQ/3btasGdeuXcsUpJ42bRp169albt265kXQoqOjSU5OJiEhwfz7yZMnzeXq1q3LtGnTLK6tIO/tuXPnEhcXR7ly5WjdujW9e/fGYDDIl5hCCCGEEEIUMvvCfPIbN26QkpJCYGCgxfbAwECOHTuW5XEREREUL16chIQE7OzsmDJlCm3atDHvb9++PV27dqV06dKcOnWKN998kw4dOrBt2zbs7OwynS8hIYGEhATz7w96JsWAAQP44IMPaNy4MQD//e9/ee6556yWjY6OZuHChTz55JN4eXlx+PBhJkyYQLt27QA4d+4cu3fvpl27dri6urJ9+3a++uorRowYAcD58+c5e/YsDRo0wGg0snjxYpYuXcr69etzVVdPT89cZ30988wzfPbZZ6xcuZJWrVoxceJE/Pz8MgWvc1s+NWvop59+Ml9vejNnzqRNmzYUL16ca9euMWLECCpUqECpUqVyVV9xb6UPvqfOP5ffwffU4GTVqlX59ttvMRgMmY6ZNm0a48ePt3q+FStWZLlv3Lhx5oBnxmB6WFgYV65cyXXwPafyGe3bt4+JEyeagzYjRozg1Vdf5ejRo9SrVy9X5xD3TmHc27nZP2TIEB5//PFMZRs1akRCQgL+/v788ccfmfann5qjoO/tEiVKWGTwT506lXr16pnn6xRCCCGEEEIUksJcoefSpUsKUFu3brXY/tprr6n69etneVxKSoo6ceKE2rdvn/rkk0+Ul5eXWr9+fZblT506pQD1559/Wt2f1QrSD+qql4mJiWro0KHK29tbeXt7q+HDh5tXa1VKqSFDhqghQ4YopZSKjo5WrVu3Vr6+vsrNzU2VLl1avfrqqyomJkYppdTZs2dVkyZNlJeXl/Lw8FAVK1ZUEyZMUCkpKUopvfprzZo1lZubm/Ly8lKPPPKIWrZsWbb1q1KlinJzc1Nubm7KYDAoQBkMBvO27CxatEiVK1dOOTs7q0aNGqm///7bvG/Tpk2Zjs+ufPPmzZXRaDQ/b8bnHzFihCpWrJhydXVVRYsWVb169VKnTp3Ktn4if73zzjuqdu3a6sqVK+rKlSuqdu3aWa50HBUVpWbMmKFu3bqlTCaTOnjwoKpcubIaPHiwUkrf27/88ouKiopSKSkpasuWLSowMNC84m9ERIRq2LChevbZZ833uzWXL19We/bsyfTw9/dXgPL397e6//Lly+ZzrF27Vnl7e6sdO3aomJgYNWjQoBxXOs6ufEpKioqLi1OrVq1SXl5eKi4uTsXFxZn3t27dWvXt21dFRkaqpKQk9c033yh3d3d169atXLWDuPcK8t7et2+fWr16tYqNjVVJSUlqxYoVytXVVc2dOzdXdS1evLgCVPHixXMsW9D39t9//61u3bqlkpOT1fr161WxYsXU77//nqvruh/JKt5CCCGEEOJ+UagByoSEBGVnZ6cWL15ssb1v377q8ccfz/V5Bg0apNq2bZttGT8/P/Xtt99a3RcfH68iIiLMjwsXLkiH3kbk5YOuEAUZfJ81a5YClKurq0UA+6effsqyfunLZQy+V6lSJdtrmzp1qipevLhydXVVHTp0sAhg/vTTT5mOz678+vXrrX4pk+rChQuqW7duys/PT3l5ean69etn+QWPKBgFeW/v2rVL1atXT3l4eChPT09Vo0aNLP//THW/3NtTpkxRAQEBysXFRdWoUUMtWbIk27rd7yRAKYQQQggh7hcGpdItZ1kIGjRoQP369fn6668BvaJmcHAww4cPZ8yYMbk6x8CBAzl9+jQbNmywuv/ixYsEBwezZMkSq8PRMoqMjMTLy4uIiAg8PT1zfS3i3itRogSXLl2iePHiXLx4sbCrI8Q9I/e2eFDJvW07pD8jhBBCCCHuF4U6ByXA6NGj6devH/Xq1aN+/fp88cUXxMTEMGDAAAD69u1L8eLFmThxIgATJ06kXr16lC1bloSEBFauXMmcOXOYOnUqoOdMHD9+PN26dSMoKIhTp07x+uuvU65cOavzCAohhBBCCCGEEEIIIQpPoQcoe/bsyfXr1xk7dixXr17l/+3de1iUdf7/8dcwygCiHFIZYEnEY64aHhJPpRaJtWtZVurWVyO/1mWHrei0doBcLdTUyHSjk2Undffq8Nv229K6bLSaeMjDaqV5jlAG8AAjmKDD/P5wHZ0AY2DGG8fn47rmau7P/Z7PvG+7ucNX9yExMVE5OTmuB+cUFBS4PfW5srJS9957rwoLCxUcHKzu3bvrvffe07hx4ySdeuLoli1btGTJEpWVlSkmJkYjR47UjBkzZLFYDNlGAAAAAAAAAHUz/BLv5ohLopoPLhWEv2Lfhr9i324++H0GAAAAF4qAXy4BAAAAAAAAAN8goAQAAAAAAABgGAJKAAAAAAAAAIYhoAQAAAAAAABgGMOf4t2cORwOORyOWuMmk8ntyeJ11ZzNbDZfFLU1NTU61zOXPKkNCAiQyWRyvQ8ICKi3l7NrPZnXn2udTqdqamrqrT17H6bW81rp3D8bDakNCAiQ2Wx2/TtryLxS8/u5bw7HCGqb1zHi9L5d13G7uf0sN+djxNl89fMJAAAANBcElOewevVqtWrVqtZ4ZGSkevfu7Vr+6quv6v1LS3h4uBITE13La9as0YkTJ1zLS5eeqa2ubq2DB/u5ltu3X68WLY7XOe+JEyEqLR3gWm7XboNatjxWZ+3Jk0EqKRnoWm7bdrMCA4/WWVtT01I22xDX8iWXbJXFUlZnrdMZoKKiq1zLkZHfKCjocJ21knTgwHDX+4iIbQoOLq23tqjoSjmdZh06JHXt2lW/+lUX3XvvyjprbbbBqqkJlCSFhe1Sq1YH6p23uHigHI4gSVKbNnsVGvpjvbUlJVfo5MlT//5bty5Q69b76q0tLe2rEydOPSE1NLRQbdrsqbf24MFEVVeHS5JatSpSWNjOemsPHeqlqqpLJEnBwSWKiNheZ92ECVKPHj3Uvn37//ZTqu+++67eebt37y6r1SpJOnz4sLZu3VpvbZcuXRQbGytJKi8v1+bNm+utTUhI0KWXXipJOnr0qDZu3FhvbXx8vOLj4yVJx44d0/r16+utjYuLU6dOnSRJVVVVWrNmTb21MTEx6tq1qyTpxIkTWr16db21VqtV3bt3l3Qq3Fm5su59TJLatWunX//6167lc9U25BiRmJjo2qaz/fwYcbbWrVurX78zx4j169fr+PG6jxEhISEaMODMMWLDhg06dqzuY0RQUJAGDjxzjNi8ebOOHq37GNGyZUsNGXLmGLF161aVlZXVWRsQEKCrrjpzjPjmm290+HD9x4jhw4e73m/btk2lpfUfI6688kpXELNjxw7ZbLZ6awcPHqzAwFPHiF27dunAgfqPEQMHDlRQ0KljxN69e/Xjj/UfI6644grXfyMKCgq0b9++emv79u3reopyYWGh9uyp/xiRmJio8PBwSVJRUZF27qz/GNGrVy9dcsmpY0RJSYm2b6/7GCGdv2PE6X27devWtX5OOEac4s3fI87282PEuf58AQAAgOaEgBIAAE+c/j9LL70knTx56n3r1qde9VmwQDodKoWGSv8NK+v08stSdfWp961aSWFh9dcuXChVVZ16HxwsRUTUX3v4sHQ60A4KkiIj6689ckT66adT7y0W6b8haJ3Ky6XKylPvAwPPfO6nn9z/L5wk2e1SRcWp9y1bSu3a1T/v0aOnXpLUooX033C1ThUVp+aWJLNZioqqv7ay8lTPkhQQIP03iK3TsWPS6QDeZJKio+uv/emnU39up8XE1F97/Li0ZEn96wEAAICLjMl5ruvBLlJ2u11hYWE6fPiw64yXs3nz0qybbnKvdTrP1JpM5563udVKNTKZ6t+dPKsNkGTSP//5K1VXFykoKFbXXLP3nLWezOtvtR9/3Pwu8R79wWiZZKq31innmU1z6sKqlWRyNq0298FcVR2pkiXCoqsXXN2geSXJedY+cKHV/uKfcR21H4/7uM5aQy+Z/u+BO8DpPOunU3Ka6t+25lbrlFRzjlqT0+m6SbWntR3+9S8dqKpSjMWivddc47V5L6RaSXL8Uu2nn56p9dEl3keOHFFkZKTKy8vr/H0GAAAAaC44g/IczGaz2y/656rzZM6znSsedg/0zq051EoB59yextbW1NT8NyhoSC++6aG51/58FzSZTA3eL31VK9N/gzp/rNXPwrRG1NbU1MjhcNQKfJs6b3Oubcy/j4bsb2f/D6PzUlvHD2JAPeN1ztsMak2SzD6qPXvfPtfnfNmD0bXytLYJv0d4qxYAAAAwEk/xBgAAAAAAAGAYAkoAAAAAAAAAhiGgBAAAAAAAAGAYAkoAAAAAAAAAhiGgBAAAAAAAAGAYAkoAAAAAAAAAhiGgBAAAAAAAAGAYAkoAAAAAAAAAhiGgBAAAAAAAAGAYAkoAAAAAAAAAhmlhdAMA4M+OHzmuqrKqWuM1J2tc/yzfW15rvSXcoqCIIJ/3BwAAAACA0QgoAcCHfsj9QTs/2lnv+mp7tVY+tbLWeJebu6jbLd182RrQJEXHj6uoqnb4Xl1T4/rnxvLa4Xu0xaLoIMJ3AAAAAGcQUKJZOH68SFVVRbXGa2qqXf8sL99Ya73FEq2goGif9wc0VodrOsjaz+rx5yzhFh90A3jPqz/8oOk76w/fS6ur1W9l7fA9o0sXPduN8B0AAADAGQSUaBZ++OFV7dw5vd711dWlWrmyX63xLl0y1K3bsz7sDGiaoIggLtWGX7qnQwfdYPU8fI+2EL4DAAAAcEdAiWahQ4d7ZLXe4PHnLBbOngQAI0QHBXGpNgAAAACvIKBEsxAUxKXaAAAAAAAAF6MAoxsAAAAAAAAAcPFqFgHlokWLFB8fr6CgICUlJWndunX11n700Ufq37+/wsPD1apVKyUmJurdd991q3E6nUpPT1d0dLSCg4OVnJysnee4kT8AAAAAAAAAYxgeUC5fvlxpaWnKyMjQxo0bdfnllyslJUUlJSV11kdGRuqpp55Sfn6+tmzZotTUVKWmpurzzz931cyZM0cLFixQdna21q5dq1atWiklJUXHjx8/X5sFAAAAAAAAoAEMDyjnz5+vKVOmKDU1VT169FB2drZCQkK0ePHiOuuHDx+um266SZdddpk6deqkBx98UL1799aqVasknTp7MisrS08//bRuvPFG9e7dW++8844OHDigTz755DxuGQAAAAAAAIBfYmhAWV1drQ0bNig5Odk1FhAQoOTkZOXn5//i551Op3Jzc/X999/rqquukiTt3btXNpvNbc6wsDAlJSU1aE4AAAAAAAAA54+hT/E+ePCgHA6HoqKi3MajoqK0ffv2ej9XXl6u2NhYVVVVyWw2609/+pOuvfZaSZLNZnPN8fM5T6/7uaqqKlVVVbmW7XZ7o7YHAAAAAAAAgGcMDSgbq3Xr1tq8ebMqKiqUm5urtLQ0JSQkaPjw4Y2aLzMzU9OnT/dukwAAAAAAAAB+kaGXeLdt21Zms1nFxcVu48XFxbJarfV+LiAgQJ07d1ZiYqIeeeQR3XLLLcrMzJQk1+c8mXPatGkqLy93vX788cembBYAAAAAAACABjI0oAwMDFS/fv2Um5vrGqupqVFubq4GDRrU4Hlqampcl2h37NhRVqvVbU673a61a9fWO6fFYlGbNm3cXgAAAAAAAAB8z/BLvNPS0jRp0iT1799fAwYMUFZWliorK5WamipJmjhxomJjY11nSGZmZqp///7q1KmTqqqq9Nlnn+ndd9/VK6+8IkkymUx66KGHNHPmTHXp0kUdO3bUM888o5iYGI0ZM8aozQQAAAAAAABQB8MDynHjxqm0tFTp6emy2WxKTExUTk6O6yE3BQUFCgg4c6JnZWWl7r33XhUWFio4OFjdu3fXe++9p3HjxrlqHn/8cVVWVuruu+9WWVmZhg4dqpycHAUFBZ337QMAAAAAAABQP5PT6XQa3URzY7fbFRYWpvLycp9f7j16tE+nx0Xi00+N7qC20UvZudF0n05ojjs3+za84DwcuM/n7zMAAABAUxh6D0oAAAAAAAAAFzcCSgAAAAAAAACGIaAEAAAAAAAAYBgCSgAAAAAAAACGIaAEAAAAAAAAYBgCSgAAAAAAAACGIaAEAAAAAAAAYBgCSgAAAAAAAACGIaAEAAAAAAAAYBgCSgAAAAAAAACGIaAEAAAAAAAAYBgCSgAAAAAAAACGIaAEAAAAAAAAYBgCSgAAAAAAAACGIaAEAAAAAAAAYBgCSgAAAAAAAACGIaAEAAAAAAAAYBgCSgAAAAAAAACGIaAEAAAAAAAAYBgCSgAAAAAAAACGIaAEAAAAAAAAYBgCSgAAAAAAAACGIaAEAAAAAAAAYBiPA8qrr75aZWVltcbtdruuvvpqb/QEAAAAAAAA4CLhcUCZl5en6urqWuPHjx/XypUrvdIUAAAAAAAAgItDi4YWbtmyxfX+u+++k81mcy07HA7l5OQoNjbWu90BAAAAAAAA8GsNDigTExNlMplkMpnqvJQ7ODhYL7/8slebAwAAAAAAAODfGnyJ9969e7V79245nU6tW7dOe/fudb32798vu92uu+66q1FNLFq0SPHx8QoKClJSUpLWrVtXb+3rr7+uK6+8UhEREYqIiFBycnKt+jvvvNMVpp5+jRo1qlG9AQAAAAAAAPCdBp9B2aFDB0lSTU2NVxtYvny50tLSlJ2draSkJGVlZSklJUXff/+92rdvX6s+Ly9PEyZM0ODBgxUUFKTZs2dr5MiR+vbbb90uMR81apTeeust17LFYvFq3wAAAAAAAACarsEB5dl27typL774QiUlJbUCy/T0dI/mmj9/vqZMmaLU1FRJUnZ2tv7v//5Pixcv1h/+8Ida9e+//77b8htvvKEPP/xQubm5mjhxomvcYrHIarV61AsAAAAAAACA88vjgPL111/X1KlT1bZtW1mtVplMJtc6k8nkUUBZXV2tDRs2aNq0aa6xgIAAJScnKz8/v0FzHDt2TCdOnFBkZKTbeF5entq3b6+IiAhdffXVmjlzpi655JI656iqqlJVVZVr2W63N3gbAAAAAAAAADSexwHlzJkz9dxzz+mJJ55o8pcfPHhQDodDUVFRbuNRUVHavn17g+Z44oknFBMTo+TkZNfYqFGjdPPNN6tjx47avXu3nnzySV133XXKz8+X2WyuNUdmZqamT5/etI0BAAAAAAAA4DGPA8ojR47o1ltv9UUvHps1a5aWLVumvLw8BQUFucbHjx/vet+rVy/17t1bnTp1Ul5enq655ppa80ybNk1paWmuZbvdrri4ON82DwAAAAAAAKDhT/E+7dZbb9U//vEPr3x527ZtZTabVVxc7DZeXFz8i/ePnDt3rmbNmqV//OMf6t279zlrExIS1LZtW+3atavO9RaLRW3atHF7AQAAAAAAAPC9Bp1BuWDBAtf7zp0765lnntGaNWvUq1cvtWzZ0q3297//fYO/PDAwUP369VNubq7GjBkj6dRTwnNzc3X//ffX+7k5c+boueee0+eff67+/fv/4vcUFhbq0KFDio6ObnBvAAAAAAAAAHyvQQHliy++6LYcGhqqL7/8Ul9++aXbuMlk8iiglKS0tDRNmjRJ/fv314ABA5SVlaXKykrXU70nTpyo2NhYZWZmSpJmz56t9PR0ffDBB4qPj5fNZnP1FBoaqoqKCk2fPl1jx46V1WrV7t279fjjj6tz585KSUnxqDcAAAAAAAAAvtWggHLv3r0+a2DcuHEqLS1Venq6bDabEhMTlZOT43pwTkFBgQICzlyJ/sorr6i6ulq33HKL2zwZGRl69tlnZTabtWXLFi1ZskRlZWWKiYnRyJEjNWPGDFksFp9tBwAAAAAAAADPefyQHF+4//77672kOy8vz215375955wrODhYn3/+uZc6AwAAAAAAAOBLHgeUZz/t+mwmk0lBQUHq3LmzbrzxRkVGRja5OQAAAAAAAAD+zeOActOmTdq4caMcDoe6desmSdqxY4fMZrO6d++uP/3pT3rkkUe0atUq9ejRw+sNAwAAAAAAAPAfAb9c4u7GG29UcnKyDhw4oA0bNmjDhg0qLCzUtddeqwkTJmj//v266qqr9PDDD/uiXwAAAAAAAAB+xOOA8oUXXtCMGTPUpk0b11hYWJieffZZzZkzRyEhIUpPT9eGDRu82igAAAAAAAAA/+NxQFleXq6SkpJa46WlpbLb7ZKk8PBwVVdXN707AAAAAAAAAH6tUZd433XXXfr4449VWFiowsJCffzxx5o8ebLGjBkjSVq3bp26du3q7V4BAAAAAAAA+BmPH5Lz6quv6uGHH9b48eN18uTJU5O0aKFJkybpxRdflCR1795db7zxhnc7BQAAAAAAAOB3PA4oQ0ND9frrr+vFF1/Unj17JEkJCQkKDQ111SQmJnqtQQAAAAAAAAD+y+OA8rTQ0FD17t3bm70AAAAAAAAAuMg0KKC8+eab9fbbb6tNmza6+eabz1n70UcfeaUxAAAAAAAAAP6vQQFlWFiYTCaT6z0AAAAAAAAAeEODAsq33nqrzvcAAAAAAAAA0BQBjfnQyZMn9c9//lOvvvqqjh49Kkk6cOCAKioqvNocAAAAAAAAAP/m8UNyfvjhB40aNUoFBQWqqqrStddeq9atW2v27NmqqqpSdna2L/oEAAAAAAAA4Ic8PoPywQcfVP/+/XXkyBEFBwe7xm+66Sbl5uZ6tTkAAAAAAAAA/s3jMyhXrlyp1atXKzAw0G08Pj5e+/fv91pjAAAAAAAAAPyfx2dQ1tTUyOFw1BovLCxU69atvdIUAAAAAAAAgIuDxwHlyJEjlZWV5Vo2mUyqqKhQRkaGrr/+em/2BgAAAAAAAMDPeXyJ97x585SSkqIePXro+PHj+t3vfqedO3eqbdu2Wrp0qS96BAAAAAAAAOCnPA4of/WrX+k///mPli1bpi1btqiiokKTJ0/W7bff7vbQHAAAAAAAAAD4JQ0OKIcNG6ZrrrlGw4cP16BBg3THHXf4si8AAAAAAAAAF4EG34OyY8eOeuuttzR8+HCFh4crOTlZzz33nNasWVPnQ3MAAAAAAAAA4Jc0OKB8++23tXfvXu3Zs0cvv/yyYmNj9dprr2nw4MGKiIjQddddpxdeeMGXvQIAAAAAAADwMx4/xTs+Pl533XWXlixZoh9++EG7du3S73//e61evVp/+MMffNEjAAAAAAAAAD/l8UNyJOmHH35QXl6e61VSUqKBAwdq2LBh3u4PAAAAAAAAgB9rcED5zjvvuALJgwcPavDgwRo2bJimTJmiK664Qi1btvRlnwAAAAAAAAD8UIMv8b7zzjv1r3/9S48//rgOHTqknJwcTZs2TYMHD25yOLlo0SLFx8crKChISUlJWrduXb21r7/+uq688kpFREQoIiJCycnJteqdTqfS09MVHR2t4OBgJScna+fOnU3qEQAAAAAAAID3NTig/NOf/qSBAwdq+vTpat++vUaPHq158+bp66+/ltPpbHQDy5cvV1pamjIyMrRx40ZdfvnlSklJUUlJSZ31eXl5mjBhgr744gvl5+crLi5OI0eO1P79+101c+bM0YIFC5Sdna21a9eqVatWSklJ0fHjxxvdJwAAAAAAAADvMzkbkS5+9913+vLLL12XfFdVVWnIkCEaMWKEHn30UY/mSkpK0hVXXKGFCxdKkmpqahQXF6cHHnigQQ/dcTgcioiI0MKFCzVx4kQ5nU7FxMTokUcecfVSXl6uqKgovf322xo/fvwvzmm32xUWFqby8nK1adPGo+3x1OjRPp0eF4lPPzW6g9pGL2XnRtN9OqE57tzs2/CC83DgPp+/zwAAAABN4fFTvCWpR48emjp1qpYvX65Nmzbp/vvv16pVq/TEE094NE91dbU2bNig5OTkMw0FBCg5OVn5+fkNmuPYsWM6ceKEIiMjJUl79+6VzWZzmzMsLExJSUn1zllVVSW73e72AgAAAAAAAOB7Hj/Fu6SkRF988YXr7MkdO3aoZcuWGjhwoEaMGOHRXAcPHpTD4VBUVJTbeFRUlLZv396gOZ544gnFxMS4Akmbzeaa4+dznl73c5mZmZo+fbpHvQMAAAAAAABougYHlPfee6/y8vL0/fffq0WLFhowYIBuueUWjRgxQoMHD1ZQUJAv+6zTrFmztGzZMuXl5TXp+6dNm6a0tDTXst1uV1xcnDdaBAAAAAAAAHAODQ4oN23apDFjxmjEiBEaMmSIQkJCmvzlbdu2ldlsVnFxsdt4cXGxrFbrOT87d+5czZo1S//85z/Vu3dv1/jpzxUXFys6OtptzsTExDrnslgsslgsjdwKAAAAAAAAAI3V4HtQ5ufn6/nnn9e1117rlXBSkgIDA9WvXz/l5ua6xmpqapSbm6tBgwbV+7k5c+ZoxowZysnJUf/+/d3WdezYUVar1W1Ou92utWvXnnNOAAAAAAAAAOefx/eg9La0tDRNmjRJ/fv314ABA5SVlaXKykqlpqZKkiZOnKjY2FhlZmZKkmbPnq309HR98MEHio+Pd91XMjQ0VKGhoTKZTHrooYc0c+ZMdenSRR07dtQzzzyjmJgYjRkzxqjNBAAAAAAAAFAHwwPKcePGqbS0VOnp6bLZbEpMTFROTo7rITcFBQUKCDhzoucrr7yi6upq3XLLLW7zZGRk6Nlnn5UkPf7446qsrNTdd9+tsrIyDR06VDk5OYbcJxMAAAAAAABA/UxOp9NpdBPNjd1uV1hYmMrLy9WmTRufftfo0T6dHheJTz81uoPaRi9l50bTfTqhOe7c7NvwgvNw4D6fv88AAAAATdHge1ACAAAAAAAAgLc1KqAsKyvTG2+8oWnTpunw4cOSpI0bN2r//v1ebQ4AAAAAAACAf/P4HpRbtmxRcnKywsLCtG/fPk2ZMkWRkZH66KOPVFBQoHfeeccXfQIAAAAAAADwQx6fQZmWlqY777xTO3fudHvozPXXX69///vfXm0OAAAAAAAAgH/zOKBcv3697rnnnlrjsbGxstlsXmkKAAAAAAAAwMXB44DSYrHIbrfXGt+xY4fatWvnlaYAAAAAAAAAXBw8DihvuOEG/fGPf9SJEyckSSaTSQUFBXriiSc0duxYrzcIAAAAAAAAwH95HFDOmzdPFRUVat++vX766ScNGzZMnTt3VuvWrfXcc8/5okcAAAAAAAAAfsrjp3iHhYVpxYoVWrVqlbZs2aKKigr17dtXycnJvugPAAAAAAAAgB/zOKA8bejQoRo6dKg3ewEAAAAAAABwkfE4oFywYEGd4yaTSUFBQercubOuuuoqmc3mJjcHAAAAAAAAwL95HFC++OKLKi0t1bFjxxQRESFJOnLkiEJCQhQaGqqSkhIlJCToiy++UFxcnNcbBgAAAAAAAOA/PH5IzvPPP68rrrhCO3fu1KFDh3To0CHt2LFDSUlJeumll1RQUCCr1aqHH37YF/0CAAAAAAAA8CMen0H59NNP68MPP1SnTp1cY507d9bcuXM1duxY7dmzR3PmzNHYsWO92igAAAAAAAAA/+PxGZRFRUU6efJkrfGTJ0/KZrNJkmJiYnT06NGmdwcAAAAAAADAr3kcUI4YMUL33HOPNm3a5BrbtGmTpk6dqquvvlqStHXrVnXs2NF7XQIAAAAAAADwSx4HlG+++aYiIyPVr18/WSwWWSwW9e/fX5GRkXrzzTclSaGhoZo3b57XmwUAAAAAAADgXzy+B6XVatWKFSu0fft27dixQ5LUrVs3devWzVUzYsQI73UIAAAAAAAAwG95HFCe1r17d3Xv3t2bvQAAAAAAAAC4yDQqoCwsLNRf//pXFRQUqLq62m3d/PnzvdIYAAAAAAAAAP/ncUCZm5urG264QQkJCdq+fbt69uypffv2yel0qm/fvr7oEQAAAAAAAICf8vghOdOmTdOjjz6qrVu3KigoSB9++KF+/PFHDRs2TLfeeqsvegQAAAAAAADgpzwOKLdt26aJEydKklq0aKGffvpJoaGh+uMf/6jZs2d7vUEAAAAAAAAA/svjgLJVq1au+05GR0dr9+7drnUHDx70XmcAAAAAAAAA/J7H96AcOHCgVq1apcsuu0zXX3+9HnnkEW3dulUfffSRBg4c6IseAQAAAAAAAPgpjwPK+fPnq6KiQpI0ffp0VVRUaPny5erSpQtP8AYAAAAAAADgEY8u8XY4HCosLNSll14q6dTl3tnZ2dqyZYs+/PBDdejQweMGFi1apPj4eAUFBSkpKUnr1q2rt/bbb7/V2LFjFR8fL5PJpKysrFo1zz77rEwmk9ure/fuHvcFAAAAAAAAwPc8CijNZrNGjhypI0eOeOXLly9frrS0NGVkZGjjxo26/PLLlZKSopKSkjrrjx07poSEBM2aNUtWq7XeeX/961+rqKjI9Vq1apVX+gUAAAAAAADgXR4/JKdnz57as2ePV758/vz5mjJlilJTU9WjRw9lZ2crJCREixcvrrP+iiuu0AsvvKDx48fLYrHUO2+LFi1ktVpdr7Zt23qlXwAAAAAAAADe5XFAOXPmTD366KP629/+pqKiItntdrdXQ1VXV2vDhg1KTk4+00xAgJKTk5Wfn+9pW2527typmJgYJSQk6Pbbb1dBQUGT5gMAAAAAAADgGx4/JOf666+XJN1www0ymUyucafTKZPJJIfD0aB5Dh48KIfDoaioKLfxqKgobd++3dO2XJKSkvT222+rW7duKioq0vTp03XllVfqm2++UevWrev8TFVVlaqqqlzLngStAAAAAAAAABrP44Dyiy++8EUfXnPddde53vfu3VtJSUnq0KGD/vznP2vy5Ml1fiYzM1PTp08/Xy0CAAAAAAAA+C+PA8phw4Z55Yvbtm0rs9ms4uJit/Hi4uJzPgDHU+Hh4eratat27dpVb820adOUlpbmWrbb7YqLi/NaDwAAAAAAAADq5vE9KCVp5cqVuuOOOzR48GDt379fkvTuu+969LTswMBA9evXT7m5ua6xmpoa5ebmatCgQY1pq04VFRXavXu3oqOj662xWCxq06aN2wsAAAAAAACA73kcUH744YdKSUlRcHCwNm7c6Lp3Y3l5uZ5//nmP5kpLS9Prr7+uJUuWaNu2bZo6daoqKyuVmpoqSZo4caKmTZvmqq+urtbmzZu1efNmVVdXa//+/dq8ebPb2ZGPPvqovvzyS+3bt0+rV6/WTTfdJLPZrAkTJni6qQAAAAAAAAB8zONLvGfOnKns7GxNnDhRy5Ytc40PGTJEM2fO9GiucePGqbS0VOnp6bLZbEpMTFROTo7rwTkFBQUKCDiToR44cEB9+vRxLc+dO1dz587VsGHDlJeXJ0kqLCzUhAkTdOjQIbVr105Dhw7VmjVr1K5dO083FQAAAAAAAICPeRxQfv/997rqqqtqjYeFhamsrMzjBu6//37df//9da47HTqeFh8fL6fTec75zg5NAQAAAAAAADRvHl/ibbVa63zgzKpVq5SQkOCVpgAAAAAAAABcHDwOKKdMmaIHH3xQa9eulclk0oEDB/T+++/r0Ucf1dSpU33RIwAAAAAAAAA/5fEl3n/4wx9UU1Oja665RseOHdNVV10li8WiRx99VA888IAvegQAAAAAAADgpzwOKE0mk5566ik99thj2rVrlyoqKtSjRw+Fhob6oj8AAAAAAAAAfszjS7zfe+89HTt2TIGBgerRo4cGDBhAOAkAAAAAAACgUTwOKB9++GG1b99ev/vd7/TZZ5/J4XD4oi8AAAAAAAAAFwGPA8qioiItW7ZMJpNJt912m6Kjo3Xfffdp9erVvugPAAAAAAAAgB/zOKBs0aKFfvvb3+r9999XSUmJXnzxRe3bt08jRoxQp06dfNEjAAAAAAAAAD/l8UNyzhYSEqKUlBQdOXJEP/zwg7Zt2+atvgAAAAAAAABcBDw+g1KSjh07pvfff1/XX3+9YmNjlZWVpZtuuknffvutt/sDAAAAAAAA4Mc8PoNy/Pjx+tvf/qaQkBDddttteuaZZzRo0CBf9AYAAAAAAADAz3kcUJrNZv35z39WSkqKzGaz27pvvvlGPXv29FpzAAAAAAAAAPybxwHl+++/77Z89OhRLV26VG+88YY2bNggh8PhteYAAAAAAAAA+LdG3YNSkv79739r0qRJio6O1ty5c3X11VdrzZo13uwNAAAAAAAAgJ/z6AxKm82mt99+W2+++absdrtuu+02VVVV6ZNPPlGPHj181SMAAAAAAAAAP9XgMyhHjx6tbt26acuWLcrKytKBAwf08ssv+7I3AAAAAAAAAH6uwWdQ/v3vf9fvf/97TZ06VV26dPFlTwAAAAAAAAAuEg0+g3LVqlU6evSo+vXrp6SkJC1cuFAHDx70ZW8AAAAAAAAA/FyDA8qBAwfq9ddfV1FRke655x4tW7ZMMTExqqmp0YoVK3T06FFf9gkAAAAAAADAD3n8FO9WrVrprrvu0qpVq7R161Y98sgjmjVrltq3b68bbrjBFz0CAAAAAAAA8FMeB5Rn69atm+bMmaPCwkItXbrUWz0BAAAAAAAAuEg0KaA8zWw2a8yYMfrrX//qjekAAAAAAAAAXCS8ElACAAAAAAAAQGMQUAIAAAAAAAAwDAElAAAAAAAAAMMQUAIAAAAAAAAwjOEB5aJFixQfH6+goCAlJSVp3bp19dZ+++23Gjt2rOLj42UymZSVldXkOQEAAAAAAAAYx9CAcvny5UpLS1NGRoY2btyoyy+/XCkpKSopKamz/tixY0pISNCsWbNktVq9MicAAAAAAAAA4xgaUM6fP19TpkxRamqqevTooezsbIWEhGjx4sV11l9xxRV64YUXNH78eFksFq/MCQAAAAAAAMA4hgWU1dXV2rBhg5KTk880ExCg5ORk5efnN5s5AQAAAAAAAPhOC6O++ODBg3I4HIqKinIbj4qK0vbt28/rnFVVVaqqqnIt2+32Rn0/AAAAAAAAAM8Y/pCc5iAzM1NhYWGuV1xcnNEtAQAAAAAAABcFwwLKtm3bymw2q7i42G28uLi43gfg+GrOadOmqby83PX68ccfG/X9AAAAAAAAADxjWEAZGBiofv36KTc31zVWU1Oj3NxcDRo06LzOabFY1KZNG7cXAAAAAAAAAN8z7B6UkpSWlqZJkyapf//+GjBggLKyslRZWanU1FRJ0sSJExUbG6vMzExJpx6C891337ne79+/X5s3b1ZoaKg6d+7coDkBAAAAAAAANB+GBpTjxo1TaWmp0tPTZbPZlJiYqJycHNdDbgoKChQQcOYkzwMHDqhPnz6u5blz52ru3LkaNmyY8vLyGjQnAAAAAAAAgObD5HQ6nUY30dzY7XaFhYWpvLzc55d7jx7t0+lxkfj0U6M7qG30UnZuNN2nE5rjzs2+DS84Dwfu8/n7DAAAANAUPMUbAAAAAAAAgGEIKAEAAAAAAAAYhoASAAAAAAAAgGEIKAEAAAAAAAAYhoASAAAAAAAAgGEIKAEAAAAAAAAYhoASAAAAAAAAgGEIKAEAAAAAAAAYhoASAAAAAAAAgGEIKAEAAAAAAAAYhoASAAAAAAAAgGEIKAEAAAAAAAAYhoASAAAAAAAAgGEIKAEAAAAAAAAYhoASAAAAAAAAgGEIKAEAAAAAAAAYhoASAAAAAAAAgGEIKAEAAAAAAAAYhoASAAAAAAAAgGEIKAEAAAAAAAAYhoASAAAAAAAAgGEIKAEAAAAAAAAYhoASAAAAAAAAgGEIKAEAAAAAAAAYhoASAAAAAAAAgGEIKAEAAAAAAAAYhoASAAAAAAAAgGGaRUC5aNEixcfHKygoSElJSVq3bt056//yl7+oe/fuCgoKUq9evfTZZ5+5rb/zzjtlMpncXqNGjfLlJgAAAAAAAABoBMMDyuXLlystLU0ZGRnauHGjLr/8cqWkpKikpKTO+tWrV2vChAmaPHmyNm3apDFjxmjMmDH65ptv3OpGjRqloqIi12vp0qXnY3MAAAAAAAAAeMDwgHL+/PmaMmWKUlNT1aNHD2VnZyskJESLFy+us/6ll17SqFGj9Nhjj+myyy7TjBkz1LdvXy1cuNCtzmKxyGq1ul4RERHnY3MAAAAAAAAAeMDQgLK6ulobNmxQcnKyaywgIEDJycnKz8+v8zP5+flu9ZKUkpJSqz4vL0/t27dXt27dNHXqVB06dKjePqqqqmS3291eAAAAAAAAAHzP0IDy4MGDcjgcioqKchuPioqSzWar8zM2m+0X60eNGqV33nlHubm5mj17tr788ktdd911cjgcdc6ZmZmpsLAw1ysuLq6JWwYAAAAAAACgIVoY3YAvjB8/3vW+V69e6t27tzp16qS8vDxdc801teqnTZumtLQ017LdbiekBAAAAAAAAM4DQ8+gbNu2rcxms4qLi93Gi4uLZbVa6/yM1Wr1qF6SEhIS1LZtW+3atavO9RaLRW3atHF7AQAAAAAAAPA9QwPKwMBA9evXT7m5ua6xmpoa5ebmatCgQXV+ZtCgQW71krRixYp66yWpsLBQhw4dUnR0tHcaBwAAAAAAAOAVhl/inZaWpkmTJql///4aMGCAsrKyVFlZqdTUVEnSxIkTFRsbq8zMTEnSgw8+qGHDhmnevHn6zW9+o2XLlunrr7/Wa6+9JkmqqKjQ9OnTNXbsWFmtVu3evVuPP/64OnfurJSUFK/27nA4dOLEiSbN0batl5rxA06ndPRoS1VXm41uBQAAAAAAAOeJ4QHluHHjVFpaqvT0dNlsNiUmJionJ8f1IJyCggIFBJw50XPw4MH64IMP9PTTT+vJJ59Uly5d9Mknn6hnz56SJLPZrC1btmjJkiUqKytTTEyMRo4cqRkzZshisXilZ6fTKZvNprKysibPdeedTZ7CbzidksMhbdwYrpUrrXI6TUa3BAAAAAAAAB8zOZ1Op9FNNDd2u11hYWEqLy+v836URUVFKisrU/v27RUSEiKTqfFB2r59TWjU7zjlcBzTkSMlys8P17//zSX5DfXpp0Z3UNvopaONbgF+4NMJzXHnZt+GF5yHA/cv/T4DAAAANBeGn0F5oXE4HK5w8pJLLmnyfGauZnZjNgcrIkLq27dEa9a053JvAAAAAAAAP2foQ3IuRKfvORkSEmJwJ/7LbA6R2Sy1bt20+3sCAAAAAACg+SOgbKSmXNaNX2KSySTxRwwAAAAAAOD/CCgBAAAAAAAAGIaAEufdHXcM13PPPWR0GwAAAAAAAGgGCCgvIk88cae6djXVek2ePMro1gAAAAAAAHCR4ineF5krrxylWbPechsLDLQY1A0AAAAAAAAudpxBeZEJDLSoXTur2yssLEKStG/fTv3ud1epZ88gXXddD3311Qp17WrSihWfSJLWrs1T164m2e1lrvm++26zunY1qbBwnyTpyJFDevjhCRo6NFa9e4fot7/tpb/9bel53koAAAAAAABcKDiD0oscDke960wmkwICAmrV1vURk0kKCDCfc16z2VxrrClqamp0//0365JLovSXv6xVRUV5o+4TWVV1XD179tOUKU8oNLSN8vL+T4899j+Ki+ukyy8f4NWeAQAAAAAAcOEjoPSilStX1rsuMjJSvXv3di1/9dVXqqmp0aFDtWtDQ8OVkJDoWv7++zU6efKEW03v3sMb1WNe3t+UmBjqNnbPPU+qV6/+2rNnu95883NFRcVIktLSntf//u91Hs1vtcZq8uRHXcsTJz6gVas+19///mcCSgAAAAAAANRCQHmRSUoaoenTX3EbCwuL1P/7f+/Kao1zhZOS1KfPII/ndzgcys5+Xn//+59VXLxfJ05Uq7q6SsHBIU3uHQAAAAAAAP6HgNKLrrzyynrXmUwmt+UhQ4ZIknburKvWfblbt4FN7u204OBW6tChc6M+e/oSdafT6Rr7+Zmdb7zxgpYseUlPPZWlrl17KSSklZ577iFVV1c3vmkAAAAAAAD4LQJKL/LkvpCnaxvyEW/fb7IunTpdJpvtR5WUFKl9+2hJ0ubNa9xqIiLaSZJKS4tcD9bZtm2zW83GjV8pOflG3XjjHZJO3dty374d6tSph4+3AAAAAAAAABcinuJ9kamurlJpqc3tdfjwQQ0enKz4+K564olJ2rbtP1q/fqVefPEpt8926NBZ0dFxevnlZ7Vv30598cX/afHieT+r6aKvvlqhjRtXa9eubXrmmXt08GDx+dxEAAAAAAAAXEAIKC8yK1fmaMiQaLfXhAlDFRAQoEWLPtbx4z/pllsG6Omn/1cPP/yc22dbtmyp+fOXas+e7Ro9urdef322Hn54plvNvfc+rR49+mry5BT9z/8MV7t2ViUnjzmPWwgAAAAAAIALCZd4X0Rmz35bs2e/Xe/6jh27aunS+p9ELkn9+g3Rp59ucRvbsePMPSnDwyP1yiufnHOO997L+6VWAQAAAAAAcJHgDEoAAAAAAAAAhiGgBAAAAAAAAGAYLvHGOZ19+TYAAAAAAADgbZxBCQAAAAAAAMAwBJQAAAAAAAAADENA2Ug1NTVGt+DHauR0SvwRAwAAAAAA+D/uQemhwMBABQQE6MCBA2rXrp0CAwNlMpkaPZ/D4cXmLnhOOZ3Vqqgo1dGjASovDzS6IQAAAAAAAPgYAaWHAgIC1LFjRxUVFenAgQNNnq+kxAtN+RGHQ9qzJ0T/+telcjg4wRcAAAAAAMDfEVA2QmBgoC699FKdPHlSjiaeAjlnjpea8gNOp/TTT2b99FMLOZ2NPysVAAAAAAAAFw4CykYymUxq2bKlWrZs2aR5Dh70UkMAAAAAAADABahZXEO7aNEixcfHKygoSElJSVq3bt056//yl7+oe/fuCgoKUq9evfTZZ5+5rXc6nUpPT1d0dLSCg4OVnJysnTt3+nITAAAAAAAAADSC4QHl8uXLlZaWpoyMDG3cuFGXX365UlJSVFLPzRlXr16tCRMmaPLkydq0aZPGjBmjMWPG6JtvvnHVzJkzRwsWLFB2drbWrl2rVq1aKSUlRcePHz9fmwUAAAAAAACgAQwPKOfPn68pU6YoNTVVPXr0UHZ2tkJCQrR48eI661966SWNGjVKjz32mC677DLNmDFDffv21cKFCyWdOnsyKytLTz/9tG688Ub17t1b77zzjg4cOKBPPvnkPG4ZAAAAAAAAgF9i6D0oq6urtWHDBk2bNs01FhAQoOTkZOXn59f5mfz8fKWlpbmNpaSkuMLHvXv3ymazKTk52bU+LCxMSUlJys/P1/jx42vNWVVVpaqqKtdyeXm5JMlutzd62xrqxAmffwUuAudhV/XYiWPs3Gi683Ec9hgHbnjDedi3T//8OJ1On38XAAAA0BSGBpQHDx6Uw+FQVFSU23hUVJS2b99e52dsNlud9TabzbX+9Fh9NT+XmZmp6dOn1xqPi4tr2IYABgsLM7oDwDfC/pedG37qPB64jx49qjD+QwEAAIBmjKd4S5o2bZrbWZk1NTU6fPiwLrnkEplMJgM7g91uV1xcnH788Ue1adPG6HYAr2Hfhr9i324+nE6njh49qpiYGKNbAQAAAM7J0ICybdu2MpvNKi4udhsvLi6W1Wqt8zNWq/Wc9af/WVxcrOjoaLeaxMTEOue0WCyyWCxuY+Hh4Z5sCnysTZs2/EUXfol9G/6Kfbt54MxJAAAAXAgMfUhOYGCg+vXrp9zcXNdYTU2NcnNzNWjQoDo/M2jQILd6SVqxYoWrvmPHjrJarW41drtda9eurXdOAAAAAAAAAMYw/BLvtLQ0TZo0Sf3799eAAQOUlZWlyspKpaamSpImTpyo2NhYZWZmSpIefPBBDRs2TPPmzdNvfvMbLVu2TF9//bVee+01SZLJZNJDDz2kmTNnqkuXLurYsaOeeeYZxcTEaMyYMUZtJgAAAAAAAIA6GB5Qjhs3TqWlpUpPT5fNZlNiYqJycnJcD7kpKChQQMCZEz0HDx6sDz74QE8//bSefPJJdenSRZ988ol69uzpqnn88cdVWVmpu+++W2VlZRo6dKhycnIUFBR03rcPTWOxWJSRkVHrEnzgQse+DX/Fvg0AAADAUyan0+k0ugkAAAAAAAAAFydD70EJAAAAAAAA4OJGQAkAAAAAAADAMASUAAAAAAAAAAxDQAmfM5lM+uSTTyRJ+/btk8lk0ubNmw3tCfAG9m34K/ZtAAAAAOcTASWaxGaz6YEHHlBCQoIsFovi4uI0evRo5ebm1lkfFxenoqIit6eue8PZf5k+l8OHD+v2229XmzZtFB4ersmTJ6uiosKrvcA/XGj79nPPPafBgwcrJCRE4eHhXu0B/uVC2rf37dunyZMnq2PHjgoODlanTp2UkZGh6upqr/YCAAAAwFgtjG4AF659+/ZpyJAhCg8P1wsvvKBevXrpxIkT+vzzz3Xfffdp+/bttT5jNptltVoN6PaU22+/XUVFRVqxYoVOnDih1NRU3X333frggw8M6wnNz4W4b1dXV+vWW2/VoEGD9OabbxrWB5q3C23f3r59u2pqavTqq6+qc+fO+uabbzRlyhRVVlZq7ty5hvQEAAAAwAecQCNdd911ztjYWGdFRUWtdUeOHHG9l+T8+OOPnU6n07l3716nJOemTZtc67du3eocNWqUs1WrVs727ds777jjDmdpaalr/bBhw5wPPPCA87HHHnNGREQ4o6KinBkZGa71HTp0cEpyvTp06FBnv999951TknP9+vWusb///e9Ok8nk3L9/f6P+DOCfLrR9+2xvvfWWMywszMMtxsXiQt63T5szZ46zY8eODa4HAAAA0PxxiTca5fDhw8rJydF9992nVq1a1Vrf0EtMy8rKdPXVV6tPnz76+uuvlZOTo+LiYt12221udUuWLFGrVq20du1azZkzR3/84x+1YsUKSdL69eslSW+99ZaKiopcyz+Xn5+v8PBw9e/f3zWWnJysgIAArV27tkH9wv9diPs20BD+sm+Xl5crMjKywfUAAAAAmj8u8Uaj7Nq1S06nU927d2/SPAsXLlSfPn30/PPPu8YWL16suLg47dixQ127dpUk9e7dWxkZGZKkLl26aOHChcrNzdW1116rdu3aSTr1l+tzXYZos9nUvn17t7EWLVooMjJSNputSdsB/3Eh7ttAQ/jDvr1r1y69/PLLXN4NAAAA+BkCSjSK0+n0yjz/+c9/9MUXXyg0NLTWut27d7v9Rfds0dHRKikp8UoPwNnYt+GvLvR9e//+/Ro1apRuvfVWTZkypdHzAAAAAGh+CCjRKF26dJHJZKrzgQqeqKio0OjRozV79uxa66Kjo13vW7Zs6bbOZDKppqbGo++yWq21/nJ88uRJHT58mLPT4HIh7ttAQ1zI+/aBAwc0YsQIDR48WK+99lqj5gAAAADQfHEPSjRKZGSkUlJStGjRIlVWVtZaX1ZW1qB5+vbtq2+//Vbx8fHq3Lmz26uue6TVp2XLlnI4HOesGTRokMrKyrRhwwbX2L/+9S/V1NQoKSmpwd8F/3Yh7ttAQ1yo+/b+/fs1fPhw9evXT2+99ZYCAvjVBQAAAPA3/JaPRlu0aJEcDocGDBigDz/8UDt37tS2bdu0YMECDRo0qEFz3HfffTp8+LAmTJig9evXa/fu3fr888+VmprqUSgTHx+v3Nxc2Ww2HTlypM6ayy67TKNGjdKUKVO0bt06ffXVV7r//vs1fvx4xcTENPi74P8utH1bkgoKCrR582YVFBTI4XBo8+bN2rx5syoqKhr8XfB/F9q+fTqcvPTSSzV37lyVlpbKZrNx32AAAADAzxBQotESEhK0ceNGjRgxQo888oh69uypa6+9Vrm5uXrllVcaNEdMTIy++uorORwOjRw5Ur169dJDDz2k8PBwj86SmTdvnlasWKG4uDj16dOn3rr3339f3bt31zXXXKPrr79eQ4cO5XJB1HIh7tvp6enq06ePMjIyVFFRoT59+riesgycdqHt2ytWrNCuXbuUm5urX/3qV4qOjna9AAAAAPgPk9Nbd80HAAAAAAAAAA9xBiUAAAAAAAAAwxBQAgAAAAAAADAMASUAAAAAAAAAwxBQAgAAAAAAADAMASUAAAAAAAAAwxBQAgAAAAAAADAMASUAAAAAAAAAwxBQAgAAAAAAADAMASUAAAAAAAAAwxBQAgAAAAAAADAMASUAAAAAAAAAwxBQAgAAAAAAADDM/wfClpr/eXPVzQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1600x800 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "try:\n",
        "    model, tokenizer, logger = main_simple()\n",
        "except Exception as e:\n",
        "    print(f\"–û—à–∏–±–∫–∞: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C26Kz4ObJQA"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OLv5y-MK82H"
      },
      "source": [
        "# –ù–µ—Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "BgTiKgqXQDXE"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    # –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "    seed: int = 42\n",
        "    epochs: int = 3\n",
        "    max_steps: int = 500\n",
        "    batch_size: int = 2\n",
        "    lr: float = 3e-5\n",
        "\n",
        "    # –º–æ–¥–µ–ª—å (–Ω–µ —Ç–∞, —á—Ç–æ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–µ -- –Ω–æ –∑–∞—Ç–æ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –ö–æ–ª–∞–±)\n",
        "    model_type: str = \"MarianMT\"\n",
        "    model_checkpoint: str = \"Helsinki-NLP/opus-mt-en-ru\"\n",
        "\n",
        "    # –¥–∞–Ω–Ω—ã–µ\n",
        "    max_seq_len: int = 32\n",
        "\n",
        "    # federated learning\n",
        "    fl: bool = True\n",
        "    npeers: int = 4\n",
        "    mdlr_: float = 0.1\n",
        "    mdniters_: int = 2\n",
        "    fl_beta_1: float = 0.9\n",
        "\n",
        "    # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "    save_every: int = 100\n",
        "    save_strategy: str = \"steps\"\n",
        "    saving_path: str = \"/content/drive/MyDrive/meritopt_checkpoints\"\n",
        "\n",
        "    device: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def __post_init__(self):\n",
        "        import os\n",
        "        os.makedirs(self.saving_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "vos78glZQsRV"
      },
      "outputs": [],
      "source": [
        "def load_my_dataset():\n",
        "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è 4 –∫–ª–∏–µ–Ω—Ç–æ–≤\"\"\"\n",
        "    all_data = []\n",
        "\n",
        "    # —Ü–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞\n",
        "    TARGET_SIZE = 10000\n",
        "\n",
        "    try:\n",
        "        # —É—Ä–∞–ª—å—Å–∫–∏–µ —è–∑—ã–∫–∏\n",
        "        print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ katykool/samoyed.ic...\")\n",
        "        try:\n",
        "            dataset1 = load_dataset(\"katykool/samoyed.ic\")\n",
        "        except Exception as e:\n",
        "            print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ katykool/samoyed.ic: {e}\")\n",
        "            # –ª–æ–∫–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –µ—Å—Ç—å\n",
        "            try:\n",
        "                dataset1 = load_dataset(\"samoyedic/samoyed.ic\")\n",
        "            except:\n",
        "                print(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç\")\n",
        "                dataset1 = None\n",
        "\n",
        "        if dataset1:\n",
        "            if 'train' in dataset1:\n",
        "                data1 = dataset1['train']\n",
        "            else:\n",
        "                data1 = dataset1\n",
        "\n",
        "            # –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —è–∑—ã–∫–∞–º\n",
        "            data_by_lang = defaultdict(list)\n",
        "            skipped = 0\n",
        "\n",
        "            for item in data1:\n",
        "                try:\n",
        "                    original = item.get('original', '')\n",
        "                    russian = item.get('russian_translation', '')\n",
        "                    lang = item.get('lang', '')\n",
        "\n",
        "                    if (original and russian and lang and\n",
        "                        isinstance(original, str) and\n",
        "                        isinstance(russian, str) and\n",
        "                        isinstance(lang, str) and\n",
        "                        len(original.strip()) > 0 and\n",
        "                        len(russian.strip()) > 0):\n",
        "\n",
        "                        data_by_lang[lang.strip().lower()].append({\n",
        "                            'original': original.strip(),\n",
        "                            'russian_translation': russian.strip(),\n",
        "                            'lang': lang.strip().lower()\n",
        "                        })\n",
        "                    else:\n",
        "                        skipped += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    skipped += 1\n",
        "\n",
        "            # –æ—Ç–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–µ —è–∑—ã–∫–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä\n",
        "            target_languages = ['enf', 'nga', 'nen']\n",
        "            for lang in target_languages:\n",
        "                if lang in data_by_lang:\n",
        "                    lang_data = data_by_lang[lang]\n",
        "                    random.shuffle(lang_data)\n",
        "                    # —Ç–æ–ª—å–∫–æ TARGET_SIZE –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "                    lang_data = lang_data[:min(TARGET_SIZE, len(lang_data))]\n",
        "                    all_data.extend(lang_data)\n",
        "                    print(f\"  {lang.upper()}: –≤–∑—è—Ç–æ {len(lang_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "                else:\n",
        "                    print(f\"  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: —è–∑—ã–∫ {lang} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
        "\n",
        "        else:\n",
        "            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å, —Å–æ–∑–¥–∞–¥–∏–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É—Ä–∞–ª—å—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤\n",
        "            print(\"–æ—à–∏–±–∫–∞\")\n",
        "            # target_languages = ['enf', 'nga', 'nen']\n",
        "            # for lang in target_languages:\n",
        "            #     lang_data = []\n",
        "            #     for i in range(TARGET_SIZE):\n",
        "            #         lang_data.append({\n",
        "            #             'original': f\"–ü—Ä–∏–º–µ—Ä –Ω–∞ —è–∑—ã–∫–µ {lang.upper()} {i}\",\n",
        "            #             'russian_translation': f\"–ü—Ä–∏–º–µ—Ä –Ω–∞ —Ä—É—Å—Å–∫–æ–º {i}\",\n",
        "            #             'lang': lang\n",
        "            #         })\n",
        "            #     all_data.extend(lang_data)\n",
        "            #     print(f\"  {lang.upper()}: —Å–æ–∑–¥–∞–Ω–æ {TARGET_SIZE} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "\n",
        "        # –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π\n",
        "        try:\n",
        "            dataset2 = load_dataset(\"open_subtitles\", lang1=\"pt\", lang2=\"ru\")\n",
        "\n",
        "            if 'train' in dataset2:\n",
        "                data2 = dataset2['train']\n",
        "                if len(data2) > TARGET_SIZE:\n",
        "                    indices = list(range(len(data2)))\n",
        "                    random.shuffle(indices)\n",
        "                    data2 = data2.select(indices[:TARGET_SIZE])\n",
        "            else:\n",
        "                data2 = dataset2\n",
        "\n",
        "            # –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "            pt_data = []\n",
        "            skipped_pt = 0\n",
        "\n",
        "            for item in data2:\n",
        "                try:\n",
        "                    if 'translation' in item:\n",
        "                        original = item['translation'].get('pt', '')\n",
        "                        russian = item['translation'].get('ru', '')\n",
        "                    else:\n",
        "                        original = item.get('pt', item.get('source', ''))\n",
        "                        russian = item.get('ru', item.get('target', ''))\n",
        "\n",
        "                    if (original and russian and\n",
        "                        isinstance(original, str) and\n",
        "                        isinstance(russian, str) and\n",
        "                        len(original.strip()) > 0 and\n",
        "                        len(russian.strip()) > 0):\n",
        "\n",
        "                        pt_data.append({\n",
        "                            'original': original.strip(),\n",
        "                            'russian_translation': russian.strip(),\n",
        "                            'lang': 'pt'\n",
        "                        })\n",
        "                    else:\n",
        "                        skipped_pt += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    skipped_pt += 1\n",
        "\n",
        "            pt_data = pt_data[:min(TARGET_SIZE, len(pt_data))]\n",
        "            all_data.extend(pt_data)\n",
        "            print(f\"  PT: –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(pt_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ open_subtitles: {e}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {e}\")\n",
        "\n",
        "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —è–∑—ã–∫–∞–º\n",
        "    lang_counts = {}\n",
        "    for item in all_data:\n",
        "        lang = item['lang']\n",
        "        lang_counts[lang] = lang_counts.get(lang, 0) + 1\n",
        "\n",
        "    for lang, count in sorted(lang_counts.items()):\n",
        "        print(f\"  - {lang.upper()}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "\n",
        "    print(f\"\\n–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "\n",
        "    print(\"\\n–ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "    for lang in ['enf', 'nga', 'nen', 'pt']:\n",
        "        lang_examples = [d for d in all_data if d['lang'] == lang]\n",
        "        if lang_examples:\n",
        "            example = lang_examples[0]\n",
        "            print(f\"  {lang.upper()}: '{example['original'][:50]}...' ‚Üí '{example['russian_translation'][:50]}...'\")\n",
        "\n",
        "    return all_data\n",
        "\n",
        "\n",
        "def create_language_clients(config, tokenizer):\n",
        "    \"\"\"\n",
        "    –°–æ–∑–¥–∞–µ—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ —è–∑—ã–∫–∞–º\n",
        "    \"\"\"\n",
        "    all_data = load_my_dataset()\n",
        "\n",
        "    if not all_data:\n",
        "        print(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö! –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—ã–µ –∫–ª–∏–µ–Ω—Ç—ã...\")\n",
        "        return {}, None, {}\n",
        "\n",
        "    # –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —è–∑—ã–∫–∞–º\n",
        "    data_by_language = defaultdict(list)\n",
        "    for item in all_data:\n",
        "        lang = item.get('lang', 'unknown')\n",
        "        data_by_language[lang].append(item)\n",
        "\n",
        "    # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫–∏ –¥–ª—è 4 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "    target_languages = ['enf', 'nga', 'nen', 'pt']\n",
        "\n",
        "    available_languages = []\n",
        "    for lang in target_languages:\n",
        "        if lang in data_by_language and len(data_by_language[lang]) > 0:\n",
        "            available_languages.append(lang)\n",
        "        else:\n",
        "            print(f\"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: —è–∑—ã–∫ {lang} –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç!\")\n",
        "            if lang in data_by_language:\n",
        "                print(f\"  –î–∞–Ω–Ω—ã—Ö –¥–ª—è {lang}: {len(data_by_language[lang])}\")\n",
        "\n",
        "    if not available_languages:\n",
        "        print(\"–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤!\")\n",
        "        return {}, None, {}\n",
        "\n",
        "    print(f\"\\n–°–æ–∑–¥–∞–Ω–∏–µ {len(available_languages)} —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤:\")\n",
        "\n",
        "    # —Å–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞\n",
        "    train_loaders = {}\n",
        "    val_loaders = {}\n",
        "    test_loaders = {}\n",
        "\n",
        "    for i, lang in enumerate(available_languages):\n",
        "        lang_data = data_by_language[lang]\n",
        "\n",
        "        # if len(lang_data) == 0:\n",
        "        #     print(f\"  –ö–ª–∏–µ–Ω—Ç {i} ({lang.upper()}): –ù–ï–¢ –î–ê–ù–ù–´–•!\")\n",
        "        #     continue\n",
        "\n",
        "        random.shuffle(lang_data)\n",
        "\n",
        "        # —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val/test (70/20/10)\n",
        "        n_total = len(lang_data)\n",
        "        n_train = int(n_total * 0.7)\n",
        "        n_val = int(n_total * 0.2)\n",
        "\n",
        "        train_data = lang_data[:n_train]\n",
        "        val_data = lang_data[n_train:n_train + n_val]\n",
        "        test_data = lang_data[n_train + n_val:]\n",
        "\n",
        "        print(f\"  –ö–ª–∏–µ–Ω—Ç {i} ({lang.upper()}):\")\n",
        "        print(f\"    –í—Å–µ–≥–æ: {n_total} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "        print(f\"    Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "        print(f\"    Val: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "        print(f\"    Test: {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "\n",
        "        # if len(train_data) > 0:\n",
        "        #     print(f\"    –ü—Ä–∏–º–µ—Ä: '{train_data[0]['original'][:50]}...' ‚Üí '{train_data[0]['russian_translation'][:50]}...'\")\n",
        "\n",
        "        # —Å–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
        "        train_dataset = TranslationDataset(train_data, split=\"train\")\n",
        "        val_dataset = TranslationDataset(val_data, split=\"val\")\n",
        "        test_dataset = TranslationDataset(test_data, split=\"test\")\n",
        "\n",
        "        # —Å–æ–∑–¥–∞–µ–º DataLoader—ã\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=config.batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=lambda x: x,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=config.batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=lambda x: x,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=config.batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=lambda x: x,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        train_loaders[i] = train_loader\n",
        "        val_loaders[i] = val_loader\n",
        "        test_loaders[i] = test_loader\n",
        "\n",
        "    # –æ–±—â–∏–π –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –∏–∑ –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤\n",
        "    all_val_data = []\n",
        "    for lang in available_languages:\n",
        "        if lang in data_by_language and data_by_language[lang]:\n",
        "            lang_data = data_by_language[lang]\n",
        "            # –ø–æ 2 –ø—Ä–∏–º–µ—Ä–∞ –∏–∑ –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞\n",
        "            sample_size = min(2, len(lang_data))\n",
        "            if sample_size > 0:\n",
        "                all_val_data.extend(random.sample(lang_data, sample_size))\n",
        "\n",
        "    if all_val_data:\n",
        "        combined_val_dataset = TranslationDataset(all_val_data, split=\"val\")\n",
        "        combined_val_loader = DataLoader(\n",
        "            combined_val_dataset,\n",
        "            batch_size=config.batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=lambda x: x,\n",
        "            num_workers=0\n",
        "        )\n",
        "    else:\n",
        "        combined_val_loader = None\n",
        "\n",
        "    # –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∫–ª–∏–µ–Ω—Ç\n",
        "    if len(train_loaders) == 0:\n",
        "        print(\"–ù–µ —Å–æ–∑–¥–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞!\")\n",
        "        return {}, None, {}\n",
        "\n",
        "    return train_loaders, combined_val_loader, test_loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "DroCiaP7RA9D"
      },
      "outputs": [],
      "source": [
        "def visualize_results(logger, smooth_window=10):\n",
        "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è 4 –∫–ª–∏–µ–Ω—Ç–æ–≤\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import numpy as np\n",
        "\n",
        "        def smooth_curve_centered(data, window_size=5):\n",
        "            if len(data) < window_size or window_size < 2:\n",
        "                return data.copy()\n",
        "\n",
        "            smoothed = np.zeros_like(data, dtype=float)\n",
        "            half = window_size // 2\n",
        "\n",
        "            for i in range(len(data)):\n",
        "                start = max(0, i - half)\n",
        "                end = min(len(data), i + half + 1)\n",
        "                smoothed[i] = np.mean(data[start:end])\n",
        "\n",
        "            return smoothed\n",
        "\n",
        "        # –¥–∞–Ω–Ω—ã–µ\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        client_weights = {i: [] for i in range(4)}  # 4 –∫–ª–∏–µ–Ω—Ç–∞\n",
        "\n",
        "        train_steps = []\n",
        "        val_steps = []\n",
        "        weight_steps = {i: [] for i in range(4)}\n",
        "\n",
        "        for metric in logger.metrics_history:\n",
        "            if metric['name'] == 'loss' and metric['mode'] == 'train':\n",
        "                train_losses.append(metric['value'])\n",
        "                train_steps.append(metric['step'])\n",
        "            elif metric['name'] == 'val_loss':\n",
        "                val_losses.append(metric['value'])\n",
        "                val_steps.append(metric['step'])\n",
        "            elif 'weight_client' in metric['name']:\n",
        "                try:\n",
        "                    client_id = int(metric['name'].split('_')[-1])\n",
        "                    if client_id < 4:\n",
        "                        client_weights[client_id].append(metric['value'])\n",
        "                        weight_steps[client_id].append(metric['step'])\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        # loss\n",
        "        plt.figure(figsize=(16, 10))\n",
        "\n",
        "        if train_losses:\n",
        "            plt.subplot(2, 3, 1)\n",
        "\n",
        "            # plt.scatter(train_steps, train_losses, alpha=0.3, s=15, color='blue',\n",
        "            #            label=f'Raw Train Loss ({len(train_losses)} points)')\n",
        "\n",
        "            if len(train_losses) >= smooth_window:\n",
        "                smoothed_train = smooth_curve_centered(train_losses, smooth_window)\n",
        "                plt.plot(train_steps, smoothed_train, 'b-', linewidth=3,\n",
        "                        label=f'Train Loss (smoothed, w={smooth_window})')\n",
        "            else:\n",
        "                plt.plot(train_steps, train_losses, 'b-', linewidth=2, label='Train Loss')\n",
        "\n",
        "            if val_losses and len(val_losses) > 0:\n",
        "                # plt.scatter(val_steps, val_losses, alpha=0.7, s=80, color='red',\n",
        "                          #  marker='s', label=f'Raw Val Loss ({len(val_losses)} points)')\n",
        "\n",
        "                if len(val_losses) >= max(3, smooth_window//2):\n",
        "                    smoothed_val = smooth_curve_centered(val_losses, min(smooth_window, len(val_losses)))\n",
        "                    plt.plot(val_steps, smoothed_val, 'r--', linewidth=3,\n",
        "                            label=f'Val Loss (smoothed)', markersize=8)\n",
        "                else:\n",
        "                    plt.plot(val_steps, val_losses, 'r--', linewidth=2,\n",
        "                            label='Val Loss', markersize=8)\n",
        "\n",
        "            plt.xlabel('Step')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Training Progress (Train vs Validation)')\n",
        "            plt.legend(loc='best')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            if train_losses:\n",
        "                y_max = max(max(train_losses), max(val_losses) if val_losses else 0) * 1.1\n",
        "                y_min = min(min(train_losses), min(val_losses) if val_losses else 0) * 0.9\n",
        "                plt.ylim(y_min, y_max)\n",
        "\n",
        "        # –≥—Ä–∞—Ñ–∏–∫ –≤–µ—Å–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "        plt.subplot(2, 3, 2)\n",
        "        has_weights = False\n",
        "\n",
        "        colors = ['blue', 'green', 'red', 'purple']\n",
        "        labels = ['Client 0 (ENF)', 'Client 1 (NGA)', 'Client 2 (NEN)', 'Client 3 (PT)']\n",
        "\n",
        "        for client_id in range(4):\n",
        "            weights = client_weights.get(client_id, [])\n",
        "            steps = weight_steps.get(client_id, [])\n",
        "\n",
        "            if weights and len(weights) > 0:\n",
        "                has_weights = True\n",
        "\n",
        "                # plt.scatter(steps, weights, alpha=0.3, s=20, color=colors[client_id])\n",
        "\n",
        "                if len(weights) >= smooth_window:\n",
        "                    smoothed_weights = smooth_curve_centered(weights, smooth_window)\n",
        "                    plt.plot(steps, smoothed_weights, color=colors[client_id],\n",
        "                            linewidth=3, label=labels[client_id])\n",
        "                else:\n",
        "                    plt.plot(steps, weights, color=colors[client_id],\n",
        "                            linewidth=2, label=labels[client_id])\n",
        "\n",
        "        if has_weights:\n",
        "            plt.xlabel('Step')\n",
        "            plt.ylabel('Weight')\n",
        "            plt.title('Client Weights Evolution (4 clients)')\n",
        "            plt.legend(loc='best')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # –ª–∏–Ω–∏—è —Ä–∞–≤–Ω—ã—Ö –≤–µ—Å–æ–≤ (1/4 –¥–ª—è 4 –∫–ª–∏–µ–Ω—Ç–æ–≤)\n",
        "            plt.axhline(y=0.25, color='gray', linestyle=':', alpha=0.7,\n",
        "                       linewidth=2, label='Equal (0.25)')\n",
        "            plt.legend()\n",
        "            plt.ylim(-0.05, 1.05)\n",
        "\n",
        "        # –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Å—Ä–µ–¥–Ω–∏—Ö –≤–µ—Å–æ–≤\n",
        "        plt.subplot(2, 3, 3)\n",
        "\n",
        "        avg_weights = []\n",
        "        std_weights = []\n",
        "        client_labels = []\n",
        "\n",
        "        for client_id in range(4):\n",
        "            weights = client_weights.get(client_id, [])\n",
        "            if weights:\n",
        "                avg_weights.append(np.mean(weights))\n",
        "                std_weights.append(np.std(weights) if len(weights) > 1 else 0)\n",
        "                client_labels.append(f'Client {client_id}')\n",
        "\n",
        "        if avg_weights:\n",
        "            x_pos = np.arange(len(avg_weights))\n",
        "            bars = plt.bar(x_pos, avg_weights, yerr=std_weights,\n",
        "                          color=colors[:len(avg_weights)], alpha=0.7,\n",
        "                          capsize=5, error_kw={'elinewidth': 2})\n",
        "\n",
        "            plt.axhline(y=0.25, color='gray', linestyle='--', alpha=0.5, label='Equal')\n",
        "\n",
        "            for i, (bar, avg, std) in enumerate(zip(bars, avg_weights, std_weights)):\n",
        "                height = bar.get_height()\n",
        "                plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                        f'{avg:.3f} ¬± {std:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "            plt.xticks(x_pos, client_labels)\n",
        "            plt.ylabel('Average Weight')\n",
        "            plt.title('Average Client Weights (4 clients)')\n",
        "            plt.legend()\n",
        "\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "tdA5QOXZRLdE"
      },
      "outputs": [],
      "source": [
        "def main_simple():\n",
        "    config = Config()\n",
        "    config.npeers = 4\n",
        "    config.epochs = 1\n",
        "    config.max_steps = 500\n",
        "    config.batch_size = 2\n",
        "    config.val_every = 50\n",
        "\n",
        "    logger = SimpleLogger()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.model_checkpoint)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(config.model_checkpoint)\n",
        "    model = model.to(config.device)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    train_loaders, val_loader, test_loaders = create_language_clients(config, tokenizer)\n",
        "\n",
        "    optimizer = MeritFedAdamFixed(model.parameters(), config)\n",
        "\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{config.epochs}\")\n",
        "\n",
        "        train_loss = train_epoch_fl_simple_fixed(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            optimizer=optimizer,\n",
        "            train_loaders=train_loaders,\n",
        "            val_loader=val_loader,\n",
        "            logger=logger,\n",
        "            config=config,\n",
        "            epoch=epoch,\n",
        "            val_every=config.val_every\n",
        "        )\n",
        "\n",
        "        # —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
        "        model.eval()\n",
        "        val_loss, val_batches = validate_simple(model, tokenizer, val_loader, logger, config)\n",
        "        model.train()\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} completed:\")\n",
        "        print(f\"   Train Loss = {train_loss:.4f}\")\n",
        "        print(f\"   Val Loss = {val_loss:.4f} ({val_batches} batches)\")\n",
        "\n",
        "        save_model_simple(model, tokenizer, config, epoch=epoch+1)\n",
        "\n",
        "        if logger.get_step() >= config.max_steps:\n",
        "            break\n",
        "\n",
        "\n",
        "    visualize_results(logger)\n",
        "    return model, tokenizer, logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "069e12b9d36d463aaa34e01038a21290",
            "9a79fb54490145d2ab2a75181cce4237",
            "8d79cdf07e3c4c84b2de1ca2e0e24b8e",
            "985ff672866d4b99aba8dc964d4e3b9d",
            "4b124bfcbee44b52981b35122991ed3f",
            "f36e2cb34f87421e9deaba7e1aeec022",
            "85620ffa02304c79b40af9600895dd3e",
            "3cbadf8369964980b269abb48d3297d6",
            "8c14d1a9e1af4d30b8b652f294cbc3f5",
            "5a5402a85088406d88314602527aa2dd",
            "e98343bb6a4d400e9ea0c7c2626882ed"
          ]
        },
        "id": "qMVGW51jROmp",
        "outputId": "2c1af83f-57b7-47df-9dd0-c1a88d529af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ katykool/samoyed.ic...\n",
            "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 97434 –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ katykool/samoyed.ic\n",
            "  ENF: –≤–∑—è—Ç–æ 10000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "  NGA: –≤–∑—è—Ç–æ 10000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "  NEN: –≤–∑—è—Ç–æ 10000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "–í—Å–µ–≥–æ —É—Ä–∞–ª—å—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: 30000\n",
            "\n",
            "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ open_subtitles (pt-ru)...\n",
            "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ open_subtitles: Dataset scripts are no longer supported, but found open_subtitles.py\n",
            "–°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–æ–≥–æ...\n",
            "  PT: —Å–æ–∑–¥–∞–Ω–æ 10000 —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "\n",
            "–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —è–∑—ã–∫–∞–º (—Ü–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä: 10000):\n",
            "  - ENF: 10000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "  - NEN: 10000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "  - NGA: 10000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "  - PT: 10000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "\n",
            "–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ 40000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "\n",
            "–ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:\n",
            "  ENF: 'An ºi ƒçikoxone a î...' ‚Üí '–û–ø—è—Ç—å —Ç–∞–º –æ–Ω–∏....'\n",
            "  NGA: 'Mii…ÅiÕ°a t…ôtu≈ãuru…Å...' ‚Üí '–°—é–¥–∞ –ø—Ä–∏–≤–µ–¥–∏—Ç–µ....'\n",
            "  NEN: 'Ee katatu tat ºa ≈ãamƒço î w ºiw ºinto…¨ ºa ma…¨ama ≈äaj…¨ ºa ...' ‚Üí '–ë–∞–±—É—à–∫–∞ —Å–∏–¥–∏—Ç, —Ä–∞–∑–¥—É–º—ã–≤–∞—è, –≥–æ–≤–æ—Ä–∏—Ç: ¬´–ß—Ç–æ —Ç—É—Ç –ø–ª–æ—Ö–æ...'\n",
            "  PT: 'Exemplo em portugu√™s 0...' ‚Üí '–ü—Ä–∏–º–µ—Ä –Ω–∞ —Ä—É—Å—Å–∫–æ–º 0...'\n",
            "\n",
            "–°–æ–∑–¥–∞–Ω–∏–µ 4 —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤:\n",
            "  –ö–ª–∏–µ–Ω—Ç 0 (ENF):\n",
            "    –í—Å–µ–≥–æ: 10000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    Train: 7000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    Val: 2000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    Test: 1000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    –ü—Ä–∏–º–µ—Ä: 'D ºazazo î d d ºazazo î d ºazaz...' ‚Üí '–ò–¥—É, –∏–¥—É, –∏–¥—É, –∏–¥—É....'\n",
            "  –ö–ª–∏–µ–Ω—Ç 1 (NGA):\n",
            "    –í—Å–µ–≥–æ: 10000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    Train: 7000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    Val: 2000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    Test: 1000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    –ü—Ä–∏–º–µ—Ä: 'K…ônd√ºt º√º≈ã huÕ°anu…Å…ô…Å...' ‚Üí '–°–∞–Ω–∫–∏ –ø–æ—Å—Ç–∞–≤–∏–ª–∏....'\n",
            "  –ö–ª–∏–µ–Ω—Ç 2 (NEN):\n",
            "    –í—Å–µ–≥–æ: 10000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    Train: 7000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    Val: 2000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    Test: 1000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    –ü—Ä–∏–º–µ—Ä: '≈äob≈ãguna wesakor ≈ãanonta m ºu ƒçij î ƒçon ºa≈ãi ≈ãisin p º...' ‚Üí '–û–¥–Ω–∞–∂–¥—ã —Å—Ç–∞—Ä–∏–∫ —Å–µ–ª –≤ —Å–≤–æ—é –ª–æ–¥–∫—É –∏ –æ—Ç–ø—Ä–∞–≤–∏–ª—Å—è –≤ —Å–æ—Å...'\n",
            "  –ö–ª–∏–µ–Ω—Ç 3 (PT):\n",
            "    –í—Å–µ–≥–æ: 10000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    Train: 7000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    Val: 2000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    Test: 1000 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "    –ü—Ä–∏–º–µ—Ä: 'Exemplo em portugu√™s 5531...' ‚Üí '–ü—Ä–∏–º–µ—Ä –Ω–∞ —Ä—É—Å—Å–∫–æ–º 5531...'\n",
            "\n",
            "–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—â–µ–≥–æ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞: 8 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
            "\n",
            "–ò—Ç–æ–≥: —Å–æ–∑–¥–∞–Ω–æ 4 —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "069e12b9d36d463aaa34e01038a21290",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FL Epoch 1:   0%|          | 0/3500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Processing] Client 0, batch 0\n",
            "  Loss for client 0: 6.3562\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 0\n",
            "  Loss for client 1: 6.1468\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 0\n",
            "  Loss for client 2: 7.1694\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 0\n",
            "  Loss for client 3: 6.7337\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 1\n",
            "  Client 0: total grad norm = 298.394402 (253 params)\n",
            "  Client 1: total grad norm = 294.938001 (253 params)\n",
            "  Client 2: total grad norm = 369.340984 (253 params)\n",
            "  Client 3: total grad norm = 381.729126 (253 params)\n",
            "\n",
            "=== Optimizer Step 1 ===\n",
            "\n",
            "=== Updating Client Weights (Step 1) ===\n",
            "Gradient norms: [20.862607955932617, 20.82631492614746, 28.84539031982422, 26.908348083496094]\n",
            "Target weights: [0.285688191652298, 0.2861860394477844, 0.20662575960159302, 0.22150003910064697]\n",
            "Updated weights: [0.26070648431777954, 0.2608558237552643, 0.23698773980140686, 0.24145002663135529]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 1\n",
            "  Loss for client 0: 5.7042\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 1\n",
            "  Loss for client 1: 5.5094\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 1\n",
            "  Loss for client 2: 5.1295\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 1\n",
            "  Loss for client 3: 4.8784\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 2\n",
            "  Client 0: total grad norm = 263.957208 (253 params)\n",
            "  Client 1: total grad norm = 184.436989 (253 params)\n",
            "  Client 2: total grad norm = 194.539784 (253 params)\n",
            "  Client 3: total grad norm = 247.587827 (253 params)\n",
            "\n",
            "=== Optimizer Step 2 ===\n",
            "\n",
            "=== Updating Client Weights (Step 2) ===\n",
            "Gradient norms: [21.346698760986328, 17.435640335083008, 17.753931045532227, 20.108558654785156]\n",
            "Target weights: [0.22280392050743103, 0.27278196811676025, 0.2678915560245514, 0.2365225851535797]\n",
            "Updated weights: [0.24933570623397827, 0.2644336521625519, 0.24625888466835022, 0.2399718016386032]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 2\n",
            "  Loss for client 0: 5.2729\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 2\n",
            "  Loss for client 1: 4.2461\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 2\n",
            "  Loss for client 2: 4.6685\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 2\n",
            "  Loss for client 3: 3.8801\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 3\n",
            "  Client 0: total grad norm = 253.798814 (253 params)\n",
            "  Client 1: total grad norm = 135.885296 (253 params)\n",
            "  Client 2: total grad norm = 146.087381 (253 params)\n",
            "  Client 3: total grad norm = 178.331960 (253 params)\n",
            "\n",
            "=== Optimizer Step 3 ===\n",
            "\n",
            "=== Updating Client Weights (Step 3) ===\n",
            "Gradient norms: [24.329242706298828, 14.129250526428223, 13.25288200378418, 17.430051803588867]\n",
            "Target weights: [0.1679684966802597, 0.2892259359359741, 0.3083515167236328, 0.23445403575897217]\n",
            "Updated weights: [0.22492554783821106, 0.27187132835388184, 0.26488667726516724, 0.23831647634506226]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 3\n",
            "  Loss for client 0: 4.5390\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 3\n",
            "  Loss for client 1: 4.1080\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 3\n",
            "  Loss for client 2: 4.4624\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 3\n",
            "  Loss for client 3: 3.4022\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 4\n",
            "  Client 0: total grad norm = 316.735443 (253 params)\n",
            "  Client 1: total grad norm = 113.470223 (253 params)\n",
            "  Client 2: total grad norm = 107.443382 (253 params)\n",
            "  Client 3: total grad norm = 161.132106 (253 params)\n",
            "\n",
            "=== Optimizer Step 4 ===\n",
            "\n",
            "=== Updating Client Weights (Step 4) ===\n",
            "Gradient norms: [29.207929611206055, 12.839800834655762, 11.240067481994629, 16.73991584777832]\n",
            "Target weights: [0.131265327334404, 0.2986018657684326, 0.34110012650489807, 0.2290327250957489]\n",
            "Updated weights: [0.1968274712562561, 0.2798904776573181, 0.2877507209777832, 0.23553134500980377]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 4\n",
            "  Loss for client 0: 4.0458\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 4\n",
            "  Loss for client 1: 3.9423\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 4\n",
            "  Loss for client 2: 4.1932\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 4\n",
            "  Loss for client 3: 2.9967\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 5\n",
            "  Client 0: total grad norm = 118.909248 (253 params)\n",
            "  Client 1: total grad norm = 153.179951 (253 params)\n",
            "  Client 2: total grad norm = 95.557006 (253 params)\n",
            "  Client 3: total grad norm = 152.338378 (253 params)\n",
            "\n",
            "=== Optimizer Step 5 ===\n",
            "\n",
            "=== Updating Client Weights (Step 5) ===\n",
            "Gradient norms: [12.321429252624512, 14.18835735321045, 11.042614936828613, 14.242656707763672]\n",
            "Target weights: [0.2597852647304535, 0.22560229897499084, 0.2898702919483185, 0.22474218904972076]\n",
            "Updated weights: [0.21571481227874756, 0.2636040151119232, 0.28838658332824707, 0.23229460418224335]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 5\n",
            "  Loss for client 0: 3.6984\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 5\n",
            "  Loss for client 1: 3.7141\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 5\n",
            "  Loss for client 2: 4.1582\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 5\n",
            "  Loss for client 3: 2.7684\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 6\n",
            "  Client 0: total grad norm = 170.727658 (253 params)\n",
            "  Client 1: total grad norm = 96.090253 (253 params)\n",
            "  Client 2: total grad norm = 214.497621 (253 params)\n",
            "  Client 3: total grad norm = 160.188869 (253 params)\n",
            "\n",
            "=== Optimizer Step 6 ===\n",
            "\n",
            "=== Updating Client Weights (Step 6) ===\n",
            "Gradient norms: [18.563798904418945, 10.67218017578125, 21.450927734375, 13.486699104309082]\n",
            "Target weights: [0.20075011253356934, 0.349196195602417, 0.17373070120811462, 0.27632296085357666]\n",
            "Updated weights: [0.21122540533542633, 0.2892816662788391, 0.2539898157119751, 0.24550311267375946]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 6\n",
            "  Loss for client 0: 3.7090\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 6\n",
            "  Loss for client 1: 3.8375\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 6\n",
            "  Loss for client 2: 3.8633\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 6\n",
            "  Loss for client 3: 2.4746\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 7\n",
            "  Client 0: total grad norm = 85.740124 (253 params)\n",
            "  Client 1: total grad norm = 88.082074 (253 params)\n",
            "  Client 2: total grad norm = 81.674304 (253 params)\n",
            "  Client 3: total grad norm = 152.526990 (253 params)\n",
            "\n",
            "=== Optimizer Step 7 ===\n",
            "\n",
            "=== Updating Client Weights (Step 7) ===\n",
            "Gradient norms: [9.8997163772583, 12.264497756958008, 9.922609329223633, 13.223992347717285]\n",
            "Target weights: [0.2814130485057831, 0.22715234756469727, 0.28076380491256714, 0.2106708288192749]\n",
            "Updated weights: [0.23228169977664948, 0.27064287662506104, 0.2620220184326172, 0.2350534200668335]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 7\n",
            "  Loss for client 0: 3.3076\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 7\n",
            "  Loss for client 1: 3.7755\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 7\n",
            "  Loss for client 2: 3.4454\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 7\n",
            "  Loss for client 3: 2.2299\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 8\n",
            "  Client 0: total grad norm = 159.323606 (253 params)\n",
            "  Client 1: total grad norm = 199.214303 (253 params)\n",
            "  Client 2: total grad norm = 84.649708 (253 params)\n",
            "  Client 3: total grad norm = 127.828782 (253 params)\n",
            "\n",
            "=== Optimizer Step 8 ===\n",
            "\n",
            "=== Updating Client Weights (Step 8) ===\n",
            "Gradient norms: [16.678091049194336, 20.42191505432129, 12.4773530960083, 11.911388397216797]\n",
            "Target weights: [0.2196100652217865, 0.17935030162334442, 0.2935459613800049, 0.307493656873703]\n",
            "Updated weights: [0.22848021984100342, 0.2432551085948944, 0.27147918939590454, 0.25678548216819763]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 8\n",
            "  Loss for client 0: 3.5068\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 8\n",
            "  Loss for client 1: 3.5770\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 8\n",
            "  Loss for client 2: 3.3739\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 8\n",
            "  Loss for client 3: 2.1676\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 9\n",
            "  Client 0: total grad norm = 84.280942 (253 params)\n",
            "  Client 1: total grad norm = 65.635132 (253 params)\n",
            "  Client 2: total grad norm = 83.415751 (253 params)\n",
            "  Client 3: total grad norm = 115.023944 (253 params)\n",
            "\n",
            "=== Optimizer Step 9 ===\n",
            "\n",
            "=== Updating Client Weights (Step 9) ===\n",
            "Gradient norms: [10.294642448425293, 8.528209686279297, 10.638432502746582, 10.177330017089844]\n",
            "Target weights: [0.23887217044830322, 0.28834936022758484, 0.23115281760692596, 0.2416256219148636]\n",
            "Updated weights: [0.23159779608249664, 0.2567833960056305, 0.2593812942504883, 0.2522375285625458]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 9\n",
            "  Loss for client 0: 3.1956\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 9\n",
            "  Loss for client 1: 3.6820\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 9\n",
            "  Loss for client 2: 3.4020\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 9\n",
            "  Loss for client 3: 2.2003\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 10\n",
            "  Client 0: total grad norm = 78.997026 (253 params)\n",
            "  Client 1: total grad norm = 68.881174 (253 params)\n",
            "  Client 2: total grad norm = 84.814398 (253 params)\n",
            "  Client 3: total grad norm = 120.855967 (253 params)\n",
            "\n",
            "=== Optimizer Step 10 ===\n",
            "\n",
            "=== Updating Client Weights (Step 10) ===\n",
            "Gradient norms: [10.552433013916016, 8.208634376525879, 10.273077011108398, 11.750844955444336]\n",
            "Target weights: [0.23748815059661865, 0.3052977919578552, 0.24394617974758148, 0.21326787769794464]\n",
            "Updated weights: [0.23336489498615265, 0.27133771777153015, 0.2547507584095001, 0.24054664373397827]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 10\n",
            "  Loss for client 0: 3.1039\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 10\n",
            "  Loss for client 1: 3.3210\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 10\n",
            "  Loss for client 2: 3.1900\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 10\n",
            "  Loss for client 3: 1.9168\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 11\n",
            "  Client 0: total grad norm = 97.854590 (253 params)\n",
            "  Client 1: total grad norm = 82.946885 (253 params)\n",
            "  Client 2: total grad norm = 62.163774 (253 params)\n",
            "  Client 3: total grad norm = 114.606326 (253 params)\n",
            "\n",
            "=== Optimizer Step 11 ===\n",
            "\n",
            "=== Updating Client Weights (Step 11) ===\n",
            "Gradient norms: [12.266274452209473, 9.773956298828125, 8.621478080749512, 11.525063514709473]\n",
            "Target weights: [0.21087850630283356, 0.26465165615081787, 0.30002903938293457, 0.22444075345993042]\n",
            "Updated weights: [0.22661897540092468, 0.2693319022655487, 0.2683342397212982, 0.2357148826122284]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 11\n",
            "  Loss for client 0: 2.8963\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 11\n",
            "  Loss for client 1: 2.2350\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 11\n",
            "  Loss for client 2: 3.6881\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 11\n",
            "  Loss for client 3: 2.1420\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 12\n",
            "  Client 0: total grad norm = 85.346456 (253 params)\n",
            "  Client 1: total grad norm = 63.145677 (253 params)\n",
            "  Client 2: total grad norm = 76.924826 (253 params)\n",
            "  Client 3: total grad norm = 118.702557 (253 params)\n",
            "\n",
            "=== Optimizer Step 12 ===\n",
            "\n",
            "=== Updating Client Weights (Step 12) ===\n",
            "Gradient norms: [11.405734062194824, 7.057112693786621, 10.433473587036133, 12.658426284790039]\n",
            "Target weights: [0.21689951419830322, 0.3505539000034332, 0.2371116578578949, 0.19543489813804626]\n",
            "Updated weights: [0.22370313107967377, 0.2936984896659851, 0.25896745920181274, 0.2236308753490448]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 12\n",
            "  Loss for client 0: 2.6922\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 12\n",
            "  Loss for client 1: 3.5212\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 12\n",
            "  Loss for client 2: 3.2805\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 12\n",
            "  Loss for client 3: 1.7690\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 13\n",
            "  Client 0: total grad norm = 136.558542 (253 params)\n",
            "  Client 1: total grad norm = 72.101953 (253 params)\n",
            "  Client 2: total grad norm = 70.668969 (253 params)\n",
            "  Client 3: total grad norm = 112.664639 (253 params)\n",
            "\n",
            "=== Optimizer Step 13 ===\n",
            "\n",
            "=== Updating Client Weights (Step 13) ===\n",
            "Gradient norms: [15.384475708007812, 11.395899772644043, 10.374581336975098, 10.95551586151123]\n",
            "Target weights: [0.19094283878803253, 0.2577730119228363, 0.2831493020057678, 0.26813483238220215]\n",
            "Updated weights: [0.21387504041194916, 0.28292083740234375, 0.2662220001220703, 0.2369820773601532]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 13\n",
            "  Loss for client 0: 3.2504\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 13\n",
            "  Loss for client 1: 3.4050\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 13\n",
            "  Loss for client 2: 2.4345\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 13\n",
            "  Loss for client 3: 1.5204\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 14\n",
            "  Client 0: total grad norm = 75.796993 (253 params)\n",
            "  Client 1: total grad norm = 77.228670 (253 params)\n",
            "  Client 2: total grad norm = 59.220243 (253 params)\n",
            "  Client 3: total grad norm = 104.058218 (253 params)\n",
            "\n",
            "=== Optimizer Step 14 ===\n",
            "\n",
            "=== Updating Client Weights (Step 14) ===\n",
            "Gradient norms: [10.258345603942871, 10.38514232635498, 8.047764778137207, 10.179515838623047]\n",
            "Target weights: [0.23418012261390686, 0.23132091760635376, 0.29850533604621887, 0.23599360883235931]\n",
            "Updated weights: [0.2199665755033493, 0.2674408555030823, 0.2759070098400116, 0.23668552935123444]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 14\n",
            "  Loss for client 0: 2.8426\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 14\n",
            "  Loss for client 1: 3.2627\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 14\n",
            "  Loss for client 2: 3.1697\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 14\n",
            "  Loss for client 3: 1.3745\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 15\n",
            "  Client 0: total grad norm = 72.372116 (253 params)\n",
            "  Client 1: total grad norm = 72.617144 (253 params)\n",
            "  Client 2: total grad norm = 69.185096 (253 params)\n",
            "  Client 3: total grad norm = 101.793214 (253 params)\n",
            "\n",
            "=== Optimizer Step 15 ===\n",
            "\n",
            "=== Updating Client Weights (Step 15) ===\n",
            "Gradient norms: [9.89900016784668, 10.542805671691895, 10.039974212646484, 9.294842720031738]\n",
            "Target weights: [0.2506333589553833, 0.23532821238040924, 0.2471141368150711, 0.26692432165145874]\n",
            "Updated weights: [0.22916661202907562, 0.25780707597732544, 0.2672691345214844, 0.24575716257095337]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 15\n",
            "  Loss for client 0: 3.0342\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 15\n",
            "  Loss for client 1: 3.5038\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 15\n",
            "  Loss for client 2: 3.2709\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 15\n",
            "  Loss for client 3: 1.2788\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 16\n",
            "  Client 0: total grad norm = 86.459437 (253 params)\n",
            "  Client 1: total grad norm = 72.450157 (253 params)\n",
            "  Client 2: total grad norm = 72.440670 (253 params)\n",
            "  Client 3: total grad norm = 97.073707 (253 params)\n",
            "\n",
            "=== Optimizer Step 16 ===\n",
            "\n",
            "=== Updating Client Weights (Step 16) ===\n",
            "Gradient norms: [12.531578063964844, 11.7066011428833, 10.49091625213623, 8.92900276184082]\n",
            "Target weights: [0.21420353651046753, 0.2292986959218979, 0.2558697760105133, 0.30062800645828247]\n",
            "Updated weights: [0.2246776968240738, 0.24925455451011658, 0.26384931802749634, 0.2622184157371521]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 16\n",
            "  Loss for client 0: 2.9274\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 16\n",
            "  Loss for client 1: 2.6535\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 16\n",
            "  Loss for client 2: 3.3058\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 16\n",
            "  Loss for client 3: 1.4508\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 17\n",
            "  Client 0: total grad norm = 60.914280 (253 params)\n",
            "  Client 1: total grad norm = 73.384290 (253 params)\n",
            "  Client 2: total grad norm = 72.032233 (253 params)\n",
            "  Client 3: total grad norm = 115.059370 (253 params)\n",
            "\n",
            "=== Optimizer Step 17 ===\n",
            "\n",
            "=== Updating Client Weights (Step 17) ===\n",
            "Gradient norms: [8.135307312011719, 10.115591049194336, 9.637507438659668, 11.984343528747559]\n",
            "Target weights: [0.30055373907089233, 0.2417156845331192, 0.2537063658237457, 0.2040242701768875]\n",
            "Updated weights: [0.24744050204753876, 0.24699288606643677, 0.26080644130706787, 0.2447601705789566]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 17\n",
            "  Loss for client 0: 2.3704\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 17\n",
            "  Loss for client 1: 2.4684\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 17\n",
            "  Loss for client 2: 3.3759\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 17\n",
            "  Loss for client 3: 1.0737\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 18\n",
            "  Client 0: total grad norm = 65.021277 (253 params)\n",
            "  Client 1: total grad norm = 82.992110 (253 params)\n",
            "  Client 2: total grad norm = 61.362723 (253 params)\n",
            "  Client 3: total grad norm = 97.258583 (253 params)\n",
            "\n",
            "=== Optimizer Step 18 ===\n",
            "\n",
            "=== Updating Client Weights (Step 18) ===\n",
            "Gradient norms: [8.532493591308594, 10.352417945861816, 8.73579216003418, 9.498555183410645]\n",
            "Target weights: [0.2703268826007843, 0.2228042334318161, 0.2640358507633209, 0.2428329885005951]\n",
            "Updated weights: [0.254306435585022, 0.23973628878593445, 0.26177525520324707, 0.2441820204257965]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 18\n",
            "  Loss for client 0: 2.1464\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 18\n",
            "  Loss for client 1: 2.2830\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 18\n",
            "  Loss for client 2: 2.5611\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 18\n",
            "  Loss for client 3: 1.0953\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 19\n",
            "  Client 0: total grad norm = 69.769048 (253 params)\n",
            "  Client 1: total grad norm = 57.357385 (253 params)\n",
            "  Client 2: total grad norm = 70.001172 (253 params)\n",
            "  Client 3: total grad norm = 118.674987 (253 params)\n",
            "\n",
            "=== Optimizer Step 19 ===\n",
            "\n",
            "=== Updating Client Weights (Step 19) ===\n",
            "Gradient norms: [9.045026779174805, 7.832361698150635, 8.363801956176758, 10.301679611206055]\n",
            "Target weights: [0.24305522441864014, 0.2806868553161621, 0.26285186409950256, 0.21340607106685638]\n",
            "Updated weights: [0.2509310841560364, 0.252021461725235, 0.2620982527732849, 0.2349492311477661]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 19\n",
            "  Loss for client 0: 2.4343\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 19\n",
            "  Loss for client 1: 2.7790\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 19\n",
            "  Loss for client 2: 2.3668\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 19\n",
            "  Loss for client 3: 0.8661\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 20\n",
            "  Client 0: total grad norm = 87.088934 (253 params)\n",
            "  Client 1: total grad norm = 54.240126 (253 params)\n",
            "  Client 2: total grad norm = 68.786706 (253 params)\n",
            "  Client 3: total grad norm = 86.115702 (253 params)\n",
            "\n",
            "=== Optimizer Step 20 ===\n",
            "\n",
            "=== Updating Client Weights (Step 20) ===\n",
            "Gradient norms: [9.487215042114258, 7.384375095367432, 8.739760398864746, 7.749916076660156]\n",
            "Target weights: [0.2176533341407776, 0.2796342074871063, 0.2362677901983261, 0.26644468307495117]\n",
            "Updated weights: [0.24094775319099426, 0.2603052854537964, 0.25434911251068115, 0.2443978637456894]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 20\n",
            "  Loss for client 0: 2.6134\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 20\n",
            "  Loss for client 1: 2.5818\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 20\n",
            "  Loss for client 2: 2.7334\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 20\n",
            "  Loss for client 3: 0.7165\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 21\n",
            "  Client 0: total grad norm = 63.608155 (253 params)\n",
            "  Client 1: total grad norm = 59.013947 (253 params)\n",
            "  Client 2: total grad norm = 74.453883 (253 params)\n",
            "  Client 3: total grad norm = 94.343212 (253 params)\n",
            "\n",
            "=== Optimizer Step 21 ===\n",
            "\n",
            "=== Updating Client Weights (Step 21) ===\n",
            "Gradient norms: [8.476091384887695, 8.544809341430664, 10.979509353637695, 8.466854095458984]\n",
            "Target weights: [0.26560136675834656, 0.26346540451049805, 0.20504209399223328, 0.2658911645412445]\n",
            "Updated weights: [0.2483438402414322, 0.26125332713127136, 0.23955701291561127, 0.25084584951400757]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 21\n",
            "  Loss for client 0: 2.7125\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 21\n",
            "  Loss for client 1: 2.9840\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 21\n",
            "  Loss for client 2: 2.8286\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 21\n",
            "  Loss for client 3: 0.7287\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 22\n",
            "  Client 0: total grad norm = 59.123160 (253 params)\n",
            "  Client 1: total grad norm = 85.343972 (253 params)\n",
            "  Client 2: total grad norm = 67.233104 (253 params)\n",
            "  Client 3: total grad norm = 93.758809 (253 params)\n",
            "\n",
            "=== Optimizer Step 22 ===\n",
            "\n",
            "=== Updating Client Weights (Step 22) ===\n",
            "Gradient norms: [8.439784049987793, 13.106603622436523, 8.803549766540527, 8.404838562011719]\n",
            "Target weights: [0.2772562801837921, 0.17853467166423798, 0.26579996943473816, 0.27840903401374817]\n",
            "Updated weights: [0.257017582654953, 0.23643773794174194, 0.24742990732192993, 0.2591148018836975]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 22\n",
            "  Loss for client 0: 2.4111\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 22\n",
            "  Loss for client 1: 2.9509\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 22\n",
            "  Loss for client 2: 2.8360\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 22\n",
            "  Loss for client 3: 0.6638\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 23\n",
            "  Client 0: total grad norm = 66.822598 (253 params)\n",
            "  Client 1: total grad norm = 61.231699 (253 params)\n",
            "  Client 2: total grad norm = 81.718267 (253 params)\n",
            "  Client 3: total grad norm = 68.048197 (253 params)\n",
            "\n",
            "=== Optimizer Step 23 ===\n",
            "\n",
            "=== Updating Client Weights (Step 23) ===\n",
            "Gradient norms: [9.366063117980957, 8.966856002807617, 10.955799102783203, 6.629142761230469]\n",
            "Target weights: [0.23189601302146912, 0.24222008883953094, 0.19824685156345367, 0.32763704657554626]\n",
            "Updated weights: [0.24948111176490784, 0.23817244172096252, 0.2326749861240387, 0.27967149019241333]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 23\n",
            "  Loss for client 0: 3.2061\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 23\n",
            "  Loss for client 1: 2.4163\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 23\n",
            "  Loss for client 2: 2.8501\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 23\n",
            "  Loss for client 3: 0.4979\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 24\n",
            "  Client 0: total grad norm = 77.140529 (253 params)\n",
            "  Client 1: total grad norm = 55.404856 (253 params)\n",
            "  Client 2: total grad norm = 60.115585 (253 params)\n",
            "  Client 3: total grad norm = 64.725931 (253 params)\n",
            "\n",
            "=== Optimizer Step 24 ===\n",
            "\n",
            "=== Updating Client Weights (Step 24) ===\n",
            "Gradient norms: [9.618562698364258, 6.627699375152588, 8.650391578674316, 6.900791168212891]\n",
            "Target weights: [0.20173394680023193, 0.2927698493003845, 0.2243124544620514, 0.28118377923965454]\n",
            "Updated weights: [0.23515696823596954, 0.25455164909362793, 0.2301662266254425, 0.2801251709461212]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 24\n",
            "  Loss for client 0: 2.1778\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 24\n",
            "  Loss for client 1: 2.5771\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 24\n",
            "  Loss for client 2: 2.6284\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 24\n",
            "  Loss for client 3: 0.4312\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 25\n",
            "  Client 0: total grad norm = 72.233930 (253 params)\n",
            "  Client 1: total grad norm = 67.841661 (253 params)\n",
            "  Client 2: total grad norm = 60.251442 (253 params)\n",
            "  Client 3: total grad norm = 60.225639 (253 params)\n",
            "\n",
            "=== Optimizer Step 25 ===\n",
            "\n",
            "=== Updating Client Weights (Step 25) ===\n",
            "Gradient norms: [8.25599193572998, 9.566611289978027, 7.256638526916504, 5.629655838012695]\n",
            "Target weights: [0.22385214269161224, 0.19318455457687378, 0.2546801269054413, 0.3282832205295563]\n",
            "Updated weights: [0.2317655235528946, 0.23614151775836945, 0.23752039670944214, 0.2945725917816162]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 25\n",
            "  Loss for client 0: 2.2938\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 25\n",
            "  Loss for client 1: 3.2480\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 25\n",
            "  Loss for client 2: 2.9334\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 25\n",
            "  Loss for client 3: 0.3796\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 26\n",
            "  Client 0: total grad norm = 56.140823 (253 params)\n",
            "  Client 1: total grad norm = 76.935047 (253 params)\n",
            "  Client 2: total grad norm = 63.872956 (253 params)\n",
            "  Client 3: total grad norm = 67.205222 (253 params)\n",
            "\n",
            "=== Optimizer Step 26 ===\n",
            "\n",
            "=== Updating Client Weights (Step 26) ===\n",
            "Gradient norms: [6.750829219818115, 11.268299102783203, 8.405728340148926, 6.017006874084473]\n",
            "Target weights: [0.28375399112701416, 0.16999678313732147, 0.22788919508457184, 0.3183600604534149]\n",
            "Updated weights: [0.24736207723617554, 0.21629810333251953, 0.2346310317516327, 0.30170881748199463]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 26\n",
            "  Loss for client 0: 1.8189\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 26\n",
            "  Loss for client 1: 2.5240\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 26\n",
            "  Loss for client 2: 3.0073\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 26\n",
            "  Loss for client 3: 0.3715\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 27\n",
            "  Client 0: total grad norm = 57.949355 (253 params)\n",
            "  Client 1: total grad norm = 56.397828 (253 params)\n",
            "  Client 2: total grad norm = 72.282542 (253 params)\n",
            "  Client 3: total grad norm = 63.143048 (253 params)\n",
            "\n",
            "=== Optimizer Step 27 ===\n",
            "\n",
            "=== Updating Client Weights (Step 27) ===\n",
            "Gradient norms: [6.766054153442383, 7.5197858810424805, 10.904970169067383, 6.268371105194092]\n",
            "Target weights: [0.27780720591545105, 0.249961718916893, 0.1723671406507492, 0.29986393451690674]\n",
            "Updated weights: [0.2564955949783325, 0.22639718651771545, 0.2159518599510193, 0.30115535855293274]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 27\n",
            "  Loss for client 0: 2.6435\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 27\n",
            "  Loss for client 1: 2.0058\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 27\n",
            "  Loss for client 2: 2.1635\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 27\n",
            "  Loss for client 3: 0.3935\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 28\n",
            "  Client 0: total grad norm = 61.318148 (253 params)\n",
            "  Client 1: total grad norm = 64.343735 (253 params)\n",
            "  Client 2: total grad norm = 62.455514 (253 params)\n",
            "  Client 3: total grad norm = 64.846559 (253 params)\n",
            "\n",
            "=== Optimizer Step 28 ===\n",
            "\n",
            "=== Updating Client Weights (Step 28) ===\n",
            "Gradient norms: [8.189417839050293, 9.007040977478027, 8.134839057922363, 6.092288970947266]\n",
            "Target weights: [0.234732985496521, 0.21342486143112183, 0.23630787432193756, 0.315534383058548]\n",
            "Updated weights: [0.2499668002128601, 0.22250548005104065, 0.22205866873264313, 0.3054690659046173]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 28\n",
            "  Loss for client 0: 2.1642\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 28\n",
            "  Loss for client 1: 2.8615\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 28\n",
            "  Loss for client 2: 2.2015\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 28\n",
            "  Loss for client 3: 0.3362\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 29\n",
            "  Client 0: total grad norm = 64.103673 (253 params)\n",
            "  Client 1: total grad norm = 86.315220 (253 params)\n",
            "  Client 2: total grad norm = 57.618838 (253 params)\n",
            "  Client 3: total grad norm = 59.675127 (253 params)\n",
            "\n",
            "=== Optimizer Step 29 ===\n",
            "\n",
            "=== Updating Client Weights (Step 29) ===\n",
            "Gradient norms: [8.731067657470703, 9.791520118713379, 6.949746608734131, 5.743712425231934]\n",
            "Target weights: [0.21421898901462555, 0.19101840257644653, 0.26912644505500793, 0.3256361782550812]\n",
            "Updated weights: [0.23924244940280914, 0.21305936574935913, 0.23617899417877197, 0.31151920557022095]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 29\n",
            "  Loss for client 0: 2.1916\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 29\n",
            "  Loss for client 1: 2.1413\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 29\n",
            "  Loss for client 2: 2.8274\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 29\n",
            "  Loss for client 3: 0.1790\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 30\n",
            "  Client 0: total grad norm = 71.678286 (253 params)\n",
            "  Client 1: total grad norm = 66.636287 (253 params)\n",
            "  Client 2: total grad norm = 84.330515 (253 params)\n",
            "  Client 3: total grad norm = 45.704089 (253 params)\n",
            "\n",
            "=== Optimizer Step 30 ===\n",
            "\n",
            "=== Updating Client Weights (Step 30) ===\n",
            "Gradient norms: [8.557846069335938, 8.730561256408691, 11.231729507446289, 4.208390712738037]\n",
            "Target weights: [0.20939461886882782, 0.20525220036506653, 0.15954504907131195, 0.4258080720901489]\n",
            "Updated weights: [0.2302880883216858, 0.21071721613407135, 0.21318881213665009, 0.3458058834075928]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 30\n",
            "  Loss for client 0: 2.5969\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 30\n",
            "  Loss for client 1: 2.5905\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 30\n",
            "  Loss for client 2: 2.2347\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 30\n",
            "  Loss for client 3: 0.1374\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 31\n",
            "  Client 0: total grad norm = 84.279084 (253 params)\n",
            "  Client 1: total grad norm = 78.270829 (253 params)\n",
            "  Client 2: total grad norm = 75.316030 (253 params)\n",
            "  Client 3: total grad norm = 38.562431 (253 params)\n",
            "\n",
            "=== Optimizer Step 31 ===\n",
            "\n",
            "=== Updating Client Weights (Step 31) ===\n",
            "Gradient norms: [9.831446647644043, 9.568167686462402, 9.018719673156738, 3.429901599884033]\n",
            "Target weights: [0.1671116054058075, 0.17170986533164978, 0.18217095732688904, 0.4790075421333313]\n",
            "Updated weights: [0.21133513748645782, 0.19901500642299652, 0.20388345420360565, 0.3857663869857788]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 31\n",
            "  Loss for client 0: 2.8283\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 31\n",
            "  Loss for client 1: 3.3046\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 31\n",
            "  Loss for client 2: 2.1704\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 31\n",
            "  Loss for client 3: 0.1496\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 32\n",
            "  Client 0: total grad norm = 75.300874 (253 params)\n",
            "  Client 1: total grad norm = 111.209295 (253 params)\n",
            "  Client 2: total grad norm = 72.370564 (253 params)\n",
            "  Client 3: total grad norm = 42.718733 (253 params)\n",
            "\n",
            "=== Optimizer Step 32 ===\n",
            "\n",
            "=== Updating Client Weights (Step 32) ===\n",
            "Gradient norms: [9.235750198364258, 12.038622856140137, 9.50467300415039, 3.6315066814422607]\n",
            "Target weights: [0.1893182396888733, 0.14524053037166595, 0.18396171927452087, 0.4814794957637787]\n",
            "Updated weights: [0.2047300785779953, 0.1828826665878296, 0.19790694117546082, 0.4144803285598755]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 32\n",
            "  Loss for client 0: 1.7587\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 32\n",
            "  Loss for client 1: 2.8458\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 32\n",
            "  Loss for client 2: 2.4112\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 32\n",
            "  Loss for client 3: 0.1475\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 33\n",
            "  Client 0: total grad norm = 246.426937 (253 params)\n",
            "  Client 1: total grad norm = 78.664769 (253 params)\n",
            "  Client 2: total grad norm = 119.503930 (253 params)\n",
            "  Client 3: total grad norm = 52.486599 (253 params)\n",
            "\n",
            "=== Optimizer Step 33 ===\n",
            "\n",
            "=== Updating Client Weights (Step 33) ===\n",
            "Gradient norms: [25.92737579345703, 9.277303695678711, 11.867731094360352, 4.767215728759766]\n",
            "Target weights: [0.08758033066987991, 0.244761660695076, 0.19133633375167847, 0.47632166743278503]\n",
            "Updated weights: [0.16958515346050262, 0.20144635438919067, 0.19593575596809387, 0.43303272128105164]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 33\n",
            "  Loss for client 0: 2.1493\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 33\n",
            "  Loss for client 1: 2.2652\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 33\n",
            "  Loss for client 2: 2.6455\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 33\n",
            "  Loss for client 3: 0.1187\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 34\n",
            "  Client 0: total grad norm = 88.946785 (253 params)\n",
            "  Client 1: total grad norm = 83.096564 (253 params)\n",
            "  Client 2: total grad norm = 82.457424 (253 params)\n",
            "  Client 3: total grad norm = 50.135038 (253 params)\n",
            "\n",
            "=== Optimizer Step 34 ===\n",
            "\n",
            "=== Updating Client Weights (Step 34) ===\n",
            "Gradient norms: [9.155590057373047, 8.235774993896484, 9.542790412902832, 4.550454616546631]\n",
            "Target weights: [0.19672930240631104, 0.2187010794878006, 0.18874697387218475, 0.3958226144313812]\n",
            "Updated weights: [0.17772839963436127, 0.20662277936935425, 0.1937791258096695, 0.421869695186615]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 34\n",
            "  Loss for client 0: 1.6775\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 34\n",
            "  Loss for client 1: 3.1996\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 34\n",
            "  Loss for client 2: 2.7327\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 34\n",
            "  Loss for client 3: 0.0841\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 35\n",
            "  Client 0: total grad norm = 73.552064 (253 params)\n",
            "  Client 1: total grad norm = 110.531185 (253 params)\n",
            "  Client 2: total grad norm = 90.979439 (253 params)\n",
            "  Client 3: total grad norm = 38.948064 (253 params)\n",
            "\n",
            "=== Optimizer Step 35 ===\n",
            "\n",
            "=== Updating Client Weights (Step 35) ===\n",
            "Gradient norms: [8.895048141479492, 11.814980506896973, 10.349100112915039, 3.711475133895874]\n",
            "Target weights: [0.19964082539081573, 0.1503019630908966, 0.17159122228622437, 0.47846606373786926]\n",
            "Updated weights: [0.18430212140083313, 0.18972653150558472, 0.18712276220321655, 0.438848614692688]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 35\n",
            "  Loss for client 0: 2.3820\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 35\n",
            "  Loss for client 1: 2.7211\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 35\n",
            "  Loss for client 2: 2.7320\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 35\n",
            "  Loss for client 3: 0.1216\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 36\n",
            "  Client 0: total grad norm = 92.036866 (253 params)\n",
            "  Client 1: total grad norm = 78.616998 (253 params)\n",
            "  Client 2: total grad norm = 80.118236 (253 params)\n",
            "  Client 3: total grad norm = 36.573330 (253 params)\n",
            "\n",
            "=== Optimizer Step 36 ===\n",
            "\n",
            "=== Updating Client Weights (Step 36) ===\n",
            "Gradient norms: [8.915627479553223, 8.78294849395752, 9.832799911499023, 3.2272441387176514]\n",
            "Target weights: [0.17591872811317444, 0.17857623100280762, 0.1595095843076706, 0.48599541187286377]\n",
            "Updated weights: [0.18178710341453552, 0.18638142943382263, 0.1788388043642044, 0.45299264788627625]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 36\n",
            "  Loss for client 0: 2.3677\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 36\n",
            "  Loss for client 1: 2.9906\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 36\n",
            "  Loss for client 2: 2.6239\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 36\n",
            "  Loss for client 3: 0.0919\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 37\n",
            "  Client 0: total grad norm = 86.168610 (253 params)\n",
            "  Client 1: total grad norm = 86.038604 (253 params)\n",
            "  Client 2: total grad norm = 87.407380 (253 params)\n",
            "  Client 3: total grad norm = 38.664044 (253 params)\n",
            "\n",
            "=== Optimizer Step 37 ===\n",
            "\n",
            "=== Updating Client Weights (Step 37) ===\n",
            "Gradient norms: [9.623197555541992, 9.65480899810791, 8.961784362792969, 3.147857904434204]\n",
            "Target weights: [0.16319625079631805, 0.16266192495822906, 0.17524075508117676, 0.4989010989665985]\n",
            "Updated weights: [0.17620985209941864, 0.1792655736207962, 0.17775937914848328, 0.4667651951313019]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 37\n",
            "  Loss for client 0: 2.4237\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 37\n",
            "  Loss for client 1: 2.8773\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 37\n",
            "  Loss for client 2: 3.0542\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 37\n",
            "  Loss for client 3: 0.0660\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 38\n",
            "  Client 0: total grad norm = 110.236276 (253 params)\n",
            "  Client 1: total grad norm = 93.040408 (253 params)\n",
            "  Client 2: total grad norm = 86.655650 (253 params)\n",
            "  Client 3: total grad norm = 32.011945 (253 params)\n",
            "\n",
            "=== Optimizer Step 38 ===\n",
            "\n",
            "=== Updating Client Weights (Step 38) ===\n",
            "Gradient norms: [9.841416358947754, 10.312874794006348, 9.194479942321777, 2.7019240856170654]\n",
            "Target weights: [0.14999207854270935, 0.14313510060310364, 0.1605457216501236, 0.5463271737098694]\n",
            "Updated weights: [0.16834452748298645, 0.16842642426490784, 0.1725952923297882, 0.4906337857246399]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 38\n",
            "  Loss for client 0: 2.5740\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 38\n",
            "  Loss for client 1: 2.6304\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 38\n",
            "  Loss for client 2: 1.6993\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 38\n",
            "  Loss for client 3: 0.0625\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 39\n",
            "  Client 0: total grad norm = 128.070678 (253 params)\n",
            "  Client 1: total grad norm = 86.980616 (253 params)\n",
            "  Client 2: total grad norm = 72.466759 (253 params)\n",
            "  Client 3: total grad norm = 24.397075 (253 params)\n",
            "\n",
            "=== Optimizer Step 39 ===\n",
            "\n",
            "=== Updating Client Weights (Step 39) ===\n",
            "Gradient norms: [14.092617988586426, 8.89136028289795, 8.01346206665039, 1.9004048109054565]\n",
            "Target weights: [0.08503991365432739, 0.134786456823349, 0.14955270290374756, 0.6306208968162537]\n",
            "Updated weights: [0.1433531492948532, 0.15833443403244019, 0.16568250954151154, 0.5326299071311951]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 39\n",
            "  Loss for client 0: 1.6021\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 39\n",
            "  Loss for client 1: 1.9754\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 39\n",
            "  Loss for client 2: 2.4500\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 39\n",
            "  Loss for client 3: 0.0671\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 40\n",
            "  Client 0: total grad norm = 66.574042 (253 params)\n",
            "  Client 1: total grad norm = 75.955026 (253 params)\n",
            "  Client 2: total grad norm = 76.817302 (253 params)\n",
            "  Client 3: total grad norm = 26.019881 (253 params)\n",
            "\n",
            "=== Optimizer Step 40 ===\n",
            "\n",
            "=== Updating Client Weights (Step 40) ===\n",
            "Gradient norms: [6.860391139984131, 8.636920928955078, 9.24925422668457, 2.189872980117798]\n",
            "Target weights: [0.1764037311077118, 0.14011922478675842, 0.13084283471107483, 0.5526341795921326]\n",
            "Updated weights: [0.15326832234859467, 0.15286986529827118, 0.15523061156272888, 0.5386312007904053]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 40\n",
            "  Loss for client 0: 2.2587\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 40\n",
            "  Loss for client 1: 3.0157\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 40\n",
            "  Loss for client 2: 2.1074\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 40\n",
            "  Loss for client 3: 0.0816\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 41\n",
            "  Client 0: total grad norm = 79.055378 (253 params)\n",
            "  Client 1: total grad norm = 85.514685 (253 params)\n",
            "  Client 2: total grad norm = 76.642191 (253 params)\n",
            "  Client 3: total grad norm = 37.825767 (253 params)\n",
            "\n",
            "=== Optimizer Step 41 ===\n",
            "\n",
            "=== Updating Client Weights (Step 41) ===\n",
            "Gradient norms: [8.20962905883789, 9.862846374511719, 10.543885231018066, 2.9979920387268066]\n",
            "Target weights: [0.18693789839744568, 0.15560322999954224, 0.14555266499519348, 0.5119062066078186]\n",
            "Updated weights: [0.16336919367313385, 0.15368987619876862, 0.15232722461223602, 0.5306137204170227]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 41\n",
            "  Loss for client 0: 1.7497\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 41\n",
            "  Loss for client 1: 2.5446\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 41\n",
            "  Loss for client 2: 2.7566\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 41\n",
            "  Loss for client 3: 0.0327\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 42\n",
            "  Client 0: total grad norm = 94.845056 (253 params)\n",
            "  Client 1: total grad norm = 75.552968 (253 params)\n",
            "  Client 2: total grad norm = 82.144974 (253 params)\n",
            "  Client 3: total grad norm = 24.143490 (253 params)\n",
            "\n",
            "=== Optimizer Step 42 ===\n",
            "\n",
            "=== Updating Client Weights (Step 42) ===\n",
            "Gradient norms: [10.116947174072266, 8.557257652282715, 9.896587371826172, 1.8737568855285645]\n",
            "Target weights: [0.11622748523950577, 0.1374117136001587, 0.11881543695926666, 0.6275453567504883]\n",
            "Updated weights: [0.14922668039798737, 0.14880642294883728, 0.1422736942768097, 0.5596932172775269]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 42\n",
            "  Loss for client 0: 2.0560\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 42\n",
            "  Loss for client 1: 2.4928\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 42\n",
            "  Loss for client 2: 2.1872\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 42\n",
            "  Loss for client 3: 0.4885\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 43\n",
            "  Client 0: total grad norm = 72.345684 (253 params)\n",
            "  Client 1: total grad norm = 72.949795 (253 params)\n",
            "  Client 2: total grad norm = 114.288889 (253 params)\n",
            "  Client 3: total grad norm = 70.715527 (253 params)\n",
            "\n",
            "=== Optimizer Step 43 ===\n",
            "\n",
            "=== Updating Client Weights (Step 43) ===\n",
            "Gradient norms: [8.796662330627441, 7.4943952560424805, 9.679035186767578, 7.321612358093262]\n",
            "Target weights: [0.23342303931713104, 0.273983895778656, 0.2121434211730957, 0.28044965863227844]\n",
            "Updated weights: [0.17448559403419495, 0.18635967373847961, 0.1632346212863922, 0.4759201407432556]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 43\n",
            "  Loss for client 0: 2.1068\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 43\n",
            "  Loss for client 1: 2.5994\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 43\n",
            "  Loss for client 2: 2.4628\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 43\n",
            "  Loss for client 3: 0.0518\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 44\n",
            "  Client 0: total grad norm = 85.651028 (253 params)\n",
            "  Client 1: total grad norm = 78.607673 (253 params)\n",
            "  Client 2: total grad norm = 91.451816 (253 params)\n",
            "  Client 3: total grad norm = 74.206393 (253 params)\n",
            "\n",
            "=== Optimizer Step 44 ===\n",
            "\n",
            "=== Updating Client Weights (Step 44) ===\n",
            "Gradient norms: [10.495818138122559, 8.97111701965332, 10.07343864440918, 5.736213684082031]\n",
            "Target weights: [0.19834841787815094, 0.2320590317249298, 0.20666517317295074, 0.3629273474216461]\n",
            "Updated weights: [0.18164443969726562, 0.20006948709487915, 0.17626379430294037, 0.44202232360839844]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 44\n",
            "  Loss for client 0: 1.9795\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 44\n",
            "  Loss for client 1: 2.9151\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 44\n",
            "  Loss for client 2: 1.9738\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 44\n",
            "  Loss for client 3: 0.3635\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 45\n",
            "  Client 0: total grad norm = 67.031914 (253 params)\n",
            "  Client 1: total grad norm = 83.128923 (253 params)\n",
            "  Client 2: total grad norm = 75.183053 (253 params)\n",
            "  Client 3: total grad norm = 48.424655 (253 params)\n",
            "\n",
            "=== Optimizer Step 45 ===\n",
            "\n",
            "=== Updating Client Weights (Step 45) ===\n",
            "Gradient norms: [8.093432426452637, 10.268837928771973, 10.061341285705566, 5.384169578552246]\n",
            "Target weights: [0.24415527284145355, 0.19243212044239044, 0.19640067219734192, 0.3670119047164917]\n",
            "Updated weights: [0.20039768517017365, 0.19777828454971313, 0.18230485916137695, 0.41951918601989746]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 45\n",
            "  Loss for client 0: 2.2460\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 45\n",
            "  Loss for client 1: 3.4698\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 45\n",
            "  Loss for client 2: 1.9862\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 45\n",
            "  Loss for client 3: 0.0347\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 46\n",
            "  Client 0: total grad norm = 110.045223 (253 params)\n",
            "  Client 1: total grad norm = 99.899112 (253 params)\n",
            "  Client 2: total grad norm = 67.291228 (253 params)\n",
            "  Client 3: total grad norm = 15.307658 (253 params)\n",
            "\n",
            "=== Optimizer Step 46 ===\n",
            "\n",
            "=== Updating Client Weights (Step 46) ===\n",
            "Gradient norms: [10.420361518859863, 9.107452392578125, 7.940296173095703, 1.323230266571045]\n",
            "Target weights: [0.0882500484585762, 0.10097196698188782, 0.11581400036811829, 0.6949639916419983]\n",
            "Updated weights: [0.1667533963918686, 0.16873638331890106, 0.16235759854316711, 0.5021526217460632]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 46\n",
            "  Loss for client 0: 2.3919\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 46\n",
            "  Loss for client 1: 1.9305\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 46\n",
            "  Loss for client 2: 2.4375\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 46\n",
            "  Loss for client 3: 0.0291\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 47\n",
            "  Client 0: total grad norm = 71.962937 (253 params)\n",
            "  Client 1: total grad norm = 67.381163 (253 params)\n",
            "  Client 2: total grad norm = 69.266185 (253 params)\n",
            "  Client 3: total grad norm = 26.597752 (253 params)\n",
            "\n",
            "=== Optimizer Step 47 ===\n",
            "\n",
            "=== Updating Client Weights (Step 47) ===\n",
            "Gradient norms: [9.198540687561035, 7.1748576164245605, 8.353936195373535, 1.7925915718078613]\n",
            "Target weights: [0.1174456775188446, 0.15057146549224854, 0.12931974232196808, 0.6026630997657776]\n",
            "Updated weights: [0.151961088180542, 0.16328690946102142, 0.15244624018669128, 0.5323057770729065]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 47\n",
            "  Loss for client 0: 1.8668\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 47\n",
            "  Loss for client 1: 2.2476\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 47\n",
            "  Loss for client 2: 2.4466\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 47\n",
            "  Loss for client 3: 0.0498\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 48\n",
            "  Client 0: total grad norm = 86.908376 (253 params)\n",
            "  Client 1: total grad norm = 89.239232 (253 params)\n",
            "  Client 2: total grad norm = 92.341121 (253 params)\n",
            "  Client 3: total grad norm = 23.620845 (253 params)\n",
            "\n",
            "=== Optimizer Step 48 ===\n",
            "\n",
            "=== Updating Client Weights (Step 48) ===\n",
            "Gradient norms: [10.095733642578125, 10.469270706176758, 11.051312446594238, 1.639093279838562]\n",
            "Target weights: [0.110653817653656, 0.10670575499534607, 0.1010858565568924, 0.6815544962882996]\n",
            "Updated weights: [0.13956892490386963, 0.14631257951259613, 0.13703814148902893, 0.5770804286003113]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 48\n",
            "  Loss for client 0: 1.9246\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 48\n",
            "  Loss for client 1: 2.3412\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 48\n",
            "  Loss for client 2: 2.8636\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 48\n",
            "  Loss for client 3: 0.0195\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 49\n",
            "  Client 0: total grad norm = 175.725937 (253 params)\n",
            "  Client 1: total grad norm = 80.567411 (253 params)\n",
            "  Client 2: total grad norm = 78.233277 (253 params)\n",
            "  Client 3: total grad norm = 6.936991 (253 params)\n",
            "\n",
            "=== Optimizer Step 49 ===\n",
            "\n",
            "=== Updating Client Weights (Step 49) ===\n",
            "Gradient norms: [16.257495880126953, 8.692971229553223, 8.95897388458252, 0.5410250425338745]\n",
            "Target weights: [0.028790000826120377, 0.053842730820178986, 0.05224407836794853, 0.8651232123374939]\n",
            "Updated weights: [0.10633523762226105, 0.11857160925865173, 0.11159990727901459, 0.6634932160377502]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 49\n",
            "  Loss for client 0: 2.5337\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 49\n",
            "  Loss for client 1: 2.3877\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 49\n",
            "  Loss for client 2: 2.5112\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 49\n",
            "  Loss for client 3: 0.3704\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 50\n",
            "  Client 0: total grad norm = 91.821180 (253 params)\n",
            "  Client 1: total grad norm = 76.160026 (253 params)\n",
            "  Client 2: total grad norm = 69.278955 (253 params)\n",
            "  Client 3: total grad norm = 78.324603 (253 params)\n",
            "\n",
            "=== Optimizer Step 50 ===\n",
            "\n",
            "=== Updating Client Weights (Step 50) ===\n",
            "Gradient norms: [8.352619171142578, 9.871591567993164, 7.916680335998535, 6.351634979248047]\n",
            "Target weights: [0.23717884719371796, 0.20068341493606567, 0.2502393126487732, 0.3118983805179596]\n",
            "Updated weights: [0.14558832347393036, 0.14320515096187592, 0.1531917303800583, 0.5580147504806519]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 50\n",
            "  Val Loss = 1.3522 (4 batches)\n",
            "\n",
            "[Processing] Client 0, batch 50\n",
            "  Loss for client 0: 2.1456\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 50\n",
            "  Loss for client 1: 2.3622\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 50\n",
            "  Loss for client 2: 2.4146\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 50\n",
            "  Loss for client 3: 0.0351\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 51\n",
            "  Client 0: total grad norm = 107.823232 (253 params)\n",
            "  Client 1: total grad norm = 79.313981 (253 params)\n",
            "  Client 2: total grad norm = 71.375567 (253 params)\n",
            "  Client 3: total grad norm = 45.282004 (253 params)\n",
            "\n",
            "=== Optimizer Step 51 ===\n",
            "\n",
            "=== Updating Client Weights (Step 51) ===\n",
            "Gradient norms: [11.629666328430176, 9.640511512756348, 9.044002532958984, 3.649200916290283]\n",
            "Target weights: [0.14971990883350372, 0.18061205744743347, 0.19252455234527588, 0.4771435260772705]\n",
            "Updated weights: [0.1468278020620346, 0.15442723035812378, 0.1649915724992752, 0.5337533950805664]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 51\n",
            "  Loss for client 0: 1.8832\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 51\n",
            "  Loss for client 1: 2.3399\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 51\n",
            "  Loss for client 2: 2.0355\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 51\n",
            "  Loss for client 3: 0.0799\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 52\n",
            "  Client 0: total grad norm = 81.089074 (253 params)\n",
            "  Client 1: total grad norm = 134.053848 (253 params)\n",
            "  Client 2: total grad norm = 62.749278 (253 params)\n",
            "  Client 3: total grad norm = 61.223473 (253 params)\n",
            "\n",
            "=== Optimizer Step 52 ===\n",
            "\n",
            "=== Updating Client Weights (Step 52) ===\n",
            "Gradient norms: [9.718914985656738, 13.837989807128906, 7.786296844482422, 5.938453674316406]\n",
            "Target weights: [0.21800026297569275, 0.1531093716621399, 0.27210959792137146, 0.3567807674407959]\n",
            "Updated weights: [0.16817954182624817, 0.1540318727493286, 0.19712698459625244, 0.4806616008281708]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 52\n",
            "  Loss for client 0: 1.9042\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 52\n",
            "  Loss for client 1: 2.6143\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 52\n",
            "  Loss for client 2: 2.5491\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 52\n",
            "  Loss for client 3: 0.0196\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 53\n",
            "  Client 0: total grad norm = 84.648591 (253 params)\n",
            "  Client 1: total grad norm = 88.820391 (253 params)\n",
            "  Client 2: total grad norm = 79.386870 (253 params)\n",
            "  Client 3: total grad norm = 15.431780 (253 params)\n",
            "\n",
            "=== Optimizer Step 53 ===\n",
            "\n",
            "=== Updating Client Weights (Step 53) ===\n",
            "Gradient norms: [9.514982223510742, 9.003143310546875, 9.097780227661133, 1.1155844926834106]\n",
            "Target weights: [0.08597081899642944, 0.0908583477139473, 0.08991322666406631, 0.7332575917243958]\n",
            "Updated weights: [0.1435169279575348, 0.13507981598377228, 0.16496284306049347, 0.5564404129981995]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 53\n",
            "  Loss for client 0: 1.9346\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 53\n",
            "  Loss for client 1: 2.0266\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 53\n",
            "  Loss for client 2: 2.3884\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 53\n",
            "  Loss for client 3: 0.0247\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 54\n",
            "  Client 0: total grad norm = 73.774984 (253 params)\n",
            "  Client 1: total grad norm = 60.894663 (253 params)\n",
            "  Client 2: total grad norm = 84.329205 (253 params)\n",
            "  Client 3: total grad norm = 27.929057 (253 params)\n",
            "\n",
            "=== Optimizer Step 54 ===\n",
            "\n",
            "=== Updating Client Weights (Step 54) ===\n",
            "Gradient norms: [8.161697387695312, 7.24022102355957, 10.385313987731934, 1.8728911876678467]\n",
            "Target weights: [0.1375332921743393, 0.15503740310668945, 0.10808581858873367, 0.599343478679657]\n",
            "Updated weights: [0.14172184467315674, 0.14106708765029907, 0.14789973199367523, 0.5693113207817078]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 54\n",
            "  Loss for client 0: 2.2618\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 54\n",
            "  Loss for client 1: 1.5792\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 54\n",
            "  Loss for client 2: 1.8330\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 54\n",
            "  Loss for client 3: 0.0218\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 55\n",
            "  Client 0: total grad norm = 76.454909 (253 params)\n",
            "  Client 1: total grad norm = 65.849562 (253 params)\n",
            "  Client 2: total grad norm = 115.925524 (253 params)\n",
            "  Client 3: total grad norm = 14.449366 (253 params)\n",
            "\n",
            "=== Optimizer Step 55 ===\n",
            "\n",
            "=== Updating Client Weights (Step 55) ===\n",
            "Gradient norms: [8.664427757263184, 7.239546775817871, 13.331930160522461, 1.1257274150848389]\n",
            "Target weights: [0.09484556317329407, 0.11351297050714493, 0.0616401769220829, 0.7300013303756714]\n",
            "Updated weights: [0.12765896320343018, 0.1328008472919464, 0.1220218688249588, 0.6175183057785034]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 55\n",
            "  Loss for client 0: 2.2534\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 55\n",
            "  Loss for client 1: 2.7295\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 55\n",
            "  Loss for client 2: 2.3107\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 55\n",
            "  Loss for client 3: 0.0190\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 56\n",
            "  Client 0: total grad norm = 68.070478 (253 params)\n",
            "  Client 1: total grad norm = 78.047326 (253 params)\n",
            "  Client 2: total grad norm = 75.536697 (253 params)\n",
            "  Client 3: total grad norm = 5.661935 (253 params)\n",
            "\n",
            "=== Optimizer Step 56 ===\n",
            "\n",
            "=== Updating Client Weights (Step 56) ===\n",
            "Gradient norms: [7.964603424072266, 9.349136352539062, 8.392824172973633, 0.5451140403747559]\n",
            "Target weights: [0.0574323795735836, 0.048927098512649536, 0.05450205132365227, 0.839138388633728]\n",
            "Updated weights: [0.10659099370241165, 0.10763872414827347, 0.10176592320203781, 0.6840043663978577]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 56\n",
            "  Loss for client 0: 1.9291\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 56\n",
            "  Loss for client 1: 2.8686\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 56\n",
            "  Loss for client 2: 2.1521\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 56\n",
            "  Loss for client 3: 0.0139\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 57\n",
            "  Client 0: total grad norm = 64.878720 (253 params)\n",
            "  Client 1: total grad norm = 85.924600 (253 params)\n",
            "  Client 2: total grad norm = 128.728779 (253 params)\n",
            "  Client 3: total grad norm = 5.362329 (253 params)\n",
            "\n",
            "=== Optimizer Step 57 ===\n",
            "\n",
            "=== Updating Client Weights (Step 57) ===\n",
            "Gradient norms: [7.346921920776367, 9.530345916748047, 11.406241416931152, 0.4514167010784149]\n",
            "Target weights: [0.05350378155708313, 0.041245944797992706, 0.034462545067071915, 0.8707876801490784]\n",
            "Updated weights: [0.0906648337841034, 0.08772088587284088, 0.08157490938901901, 0.7400393486022949]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 57\n",
            "  Loss for client 0: 1.3148\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 57\n",
            "  Loss for client 1: 2.1833\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 57\n",
            "  Loss for client 2: 2.7144\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 57\n",
            "  Loss for client 3: 0.0369\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 58\n",
            "  Client 0: total grad norm = 67.501762 (253 params)\n",
            "  Client 1: total grad norm = 97.697674 (253 params)\n",
            "  Client 2: total grad norm = 96.023063 (253 params)\n",
            "  Client 3: total grad norm = 20.386368 (253 params)\n",
            "\n",
            "=== Optimizer Step 58 ===\n",
            "\n",
            "=== Updating Client Weights (Step 58) ===\n",
            "Gradient norms: [6.968747615814209, 10.71812629699707, 12.373772621154785, 1.610552430152893]\n",
            "Target weights: [0.1528981775045395, 0.09941185265779495, 0.08611026406288147, 0.6615797281265259]\n",
            "Updated weights: [0.10933483392000198, 0.0912281721830368, 0.08293551206588745, 0.7165014743804932]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 58\n",
            "  Loss for client 0: 1.7227\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 58\n",
            "  Loss for client 1: 2.5167\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 58\n",
            "  Loss for client 2: 2.1317\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 58\n",
            "  Loss for client 3: 0.0089\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 59\n",
            "  Client 0: total grad norm = 68.217299 (253 params)\n",
            "  Client 1: total grad norm = 73.733989 (253 params)\n",
            "  Client 2: total grad norm = 89.643581 (253 params)\n",
            "  Client 3: total grad norm = 4.985084 (253 params)\n",
            "\n",
            "=== Optimizer Step 59 ===\n",
            "\n",
            "=== Updating Client Weights (Step 59) ===\n",
            "Gradient norms: [8.153948783874512, 8.482911109924316, 8.360081672668457, 0.36229759454727173]\n",
            "Target weights: [0.03930388018488884, 0.03777969628572464, 0.038334768265485764, 0.8845816850662231]\n",
            "Updated weights: [0.08832554519176483, 0.0751936286687851, 0.06955529004335403, 0.766925573348999]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 59\n",
            "  Loss for client 0: 2.2750\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 59\n",
            "  Loss for client 1: 1.7719\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 59\n",
            "  Loss for client 2: 2.1255\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 59\n",
            "  Loss for client 3: 0.0109\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 60\n",
            "  Client 0: total grad norm = 101.204442 (253 params)\n",
            "  Client 1: total grad norm = 89.011477 (253 params)\n",
            "  Client 2: total grad norm = 93.449210 (253 params)\n",
            "  Client 3: total grad norm = 3.020765 (253 params)\n",
            "\n",
            "=== Optimizer Step 60 ===\n",
            "\n",
            "=== Updating Client Weights (Step 60) ===\n",
            "Gradient norms: [9.912219047546387, 11.034246444702148, 12.551369667053223, 0.3058997392654419]\n",
            "Target weights: [0.02849690057337284, 0.025599166750907898, 0.022504916414618492, 0.9233989715576172]\n",
            "Updated weights: [0.07037694752216339, 0.06031528860330582, 0.05544017627835274, 0.8138675689697266]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 60\n",
            "  Loss for client 0: 2.0172\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 60\n",
            "  Loss for client 1: 3.0153\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 60\n",
            "  Loss for client 2: 2.3636\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 60\n",
            "  Loss for client 3: 0.0169\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 61\n",
            "  Client 0: total grad norm = 82.963075 (253 params)\n",
            "  Client 1: total grad norm = 81.976411 (253 params)\n",
            "  Client 2: total grad norm = 71.198396 (253 params)\n",
            "  Client 3: total grad norm = 14.984241 (253 params)\n",
            "\n",
            "=== Optimizer Step 61 ===\n",
            "\n",
            "=== Updating Client Weights (Step 61) ===\n",
            "Gradient norms: [8.994719505310059, 9.174148559570312, 7.188112735748291, 1.564827561378479]\n",
            "Target weights: [0.11136065423488617, 0.10918264836072922, 0.13934922218322754, 0.6401075124740601]\n",
            "Updated weights: [0.08267205953598022, 0.07497549802064896, 0.08061289042234421, 0.7617395520210266]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 61\n",
            "  Loss for client 0: 2.7333\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 61\n",
            "  Loss for client 1: 2.5053\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 61\n",
            "  Loss for client 2: 2.5833\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 61\n",
            "  Loss for client 3: 0.0088\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 62\n",
            "  Client 0: total grad norm = 94.328198 (253 params)\n",
            "  Client 1: total grad norm = 87.389606 (253 params)\n",
            "  Client 2: total grad norm = 105.586817 (253 params)\n",
            "  Client 3: total grad norm = 2.526353 (253 params)\n",
            "\n",
            "=== Optimizer Step 62 ===\n",
            "\n",
            "=== Updating Client Weights (Step 62) ===\n",
            "Gradient norms: [9.434473991394043, 8.931503295898438, 10.272077560424805, 0.2612346112728119]\n",
            "Target weights: [0.025582173839211464, 0.027022816240787506, 0.02349615842103958, 0.9238988757133484]\n",
            "Updated weights: [0.06554508954286575, 0.06058969348669052, 0.06347786635160446, 0.810387372970581]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 62\n",
            "  Loss for client 0: 1.2741\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 62\n",
            "  Loss for client 1: 2.5551\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 62\n",
            "  Loss for client 2: 2.4506\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 62\n",
            "  Loss for client 3: 0.0121\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 63\n",
            "  Client 0: total grad norm = 117.232611 (253 params)\n",
            "  Client 1: total grad norm = 89.833297 (253 params)\n",
            "  Client 2: total grad norm = 79.682837 (253 params)\n",
            "  Client 3: total grad norm = 5.247624 (253 params)\n",
            "\n",
            "=== Optimizer Step 63 ===\n",
            "\n",
            "=== Updating Client Weights (Step 63) ===\n",
            "Gradient norms: [11.384357452392578, 10.178086280822754, 9.183493614196777, 0.43791088461875916]\n",
            "Target weights: [0.03406557813286781, 0.03810291364789009, 0.0422295406460762, 0.8856019377708435]\n",
            "Updated weights: [0.056101229041814804, 0.053843650966882706, 0.05710336193442345, 0.8329516649246216]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 63\n",
            "  Loss for client 0: 2.6900\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 63\n",
            "  Loss for client 1: 2.2140\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 63\n",
            "  Loss for client 2: 1.8211\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 63\n",
            "  Loss for client 3: 0.0090\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 64\n",
            "  Client 0: total grad norm = 80.291534 (253 params)\n",
            "  Client 1: total grad norm = 73.032516 (253 params)\n",
            "  Client 2: total grad norm = 68.963931 (253 params)\n",
            "  Client 3: total grad norm = 2.889458 (253 params)\n",
            "\n",
            "=== Optimizer Step 64 ===\n",
            "\n",
            "=== Updating Client Weights (Step 64) ===\n",
            "Gradient norms: [9.358972549438477, 8.730576515197754, 8.21120834350586, 0.27074477076530457]\n",
            "Target weights: [0.02646954357624054, 0.028374727815389633, 0.03016946092247963, 0.914986252784729]\n",
            "Updated weights: [0.047211721539497375, 0.046202972531318665, 0.04902319237589836, 0.8575620651245117]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 64\n",
            "  Loss for client 0: 2.2796\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 64\n",
            "  Loss for client 1: 1.7107\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 64\n",
            "  Loss for client 2: 2.5605\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 64\n",
            "  Loss for client 3: 0.0221\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 65\n",
            "  Client 0: total grad norm = 77.340649 (253 params)\n",
            "  Client 1: total grad norm = 93.744891 (253 params)\n",
            "  Client 2: total grad norm = 77.697611 (253 params)\n",
            "  Client 3: total grad norm = 22.906002 (253 params)\n",
            "\n",
            "=== Optimizer Step 65 ===\n",
            "\n",
            "=== Updating Client Weights (Step 65) ===\n",
            "Gradient norms: [8.561833381652832, 10.12939739227295, 8.673360824584961, 1.7224563360214233]\n",
            "Target weights: [0.1281542181968689, 0.10832184553146362, 0.12650632858276367, 0.6370176076889038]\n",
            "Updated weights: [0.07149447500705719, 0.06483863294124603, 0.07226813584566116, 0.7913987636566162]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 65\n",
            "  Loss for client 0: 1.3385\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 65\n",
            "  Loss for client 1: 1.7386\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 65\n",
            "  Loss for client 2: 2.4550\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 65\n",
            "  Loss for client 3: 0.0114\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 66\n",
            "  Client 0: total grad norm = 84.076984 (253 params)\n",
            "  Client 1: total grad norm = 71.301737 (253 params)\n",
            "  Client 2: total grad norm = 78.107054 (253 params)\n",
            "  Client 3: total grad norm = 2.851167 (253 params)\n",
            "\n",
            "=== Optimizer Step 66 ===\n",
            "\n",
            "=== Updating Client Weights (Step 66) ===\n",
            "Gradient norms: [10.7417631149292, 8.77574634552002, 8.684332847595215, 0.3157637417316437]\n",
            "Target weights: [0.02668139711022377, 0.032658789306879044, 0.03300256282091141, 0.9076572060585022]\n",
            "Updated weights: [0.058050550520420074, 0.05518467724323273, 0.06048846244812012, 0.8262763023376465]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 66\n",
            "  Loss for client 0: 1.4247\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 66\n",
            "  Loss for client 1: 2.1234\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 66\n",
            "  Loss for client 2: 2.3840\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 66\n",
            "  Loss for client 3: 0.2794\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 67\n",
            "  Client 0: total grad norm = 57.515812 (253 params)\n",
            "  Client 1: total grad norm = 82.627002 (253 params)\n",
            "  Client 2: total grad norm = 72.664308 (253 params)\n",
            "  Client 3: total grad norm = 18.227619 (253 params)\n",
            "\n",
            "=== Optimizer Step 67 ===\n",
            "\n",
            "=== Updating Client Weights (Step 67) ===\n",
            "Gradient norms: [6.021661281585693, 9.165074348449707, 9.20476245880127, 3.2109270095825195]\n",
            "Target weights: [0.23885856568813324, 0.15693549811840057, 0.15625882148742676, 0.44794708490371704]\n",
            "Updated weights: [0.11229296773672104, 0.08570993691682816, 0.0892195776104927, 0.7127775549888611]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 67\n",
            "  Loss for client 0: 1.9202\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 67\n",
            "  Loss for client 1: 2.2337\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 67\n",
            "  Loss for client 2: 2.6702\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 67\n",
            "  Loss for client 3: 0.0143\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 68\n",
            "  Client 0: total grad norm = 74.045522 (253 params)\n",
            "  Client 1: total grad norm = 84.939888 (253 params)\n",
            "  Client 2: total grad norm = 92.512023 (253 params)\n",
            "  Client 3: total grad norm = 4.216079 (253 params)\n",
            "\n",
            "=== Optimizer Step 68 ===\n",
            "\n",
            "=== Updating Client Weights (Step 68) ===\n",
            "Gradient norms: [8.75649356842041, 9.723373413085938, 11.942377090454102, 0.42841240763664246]\n",
            "Target weights: [0.04334033653140068, 0.03903063014149666, 0.031778380274772644, 0.8858506083488464]\n",
            "Updated weights: [0.09160718321800232, 0.0717061460018158, 0.07198721915483475, 0.7646994590759277]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 68\n",
            "  Loss for client 0: 2.3647\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 68\n",
            "  Loss for client 1: 2.7792\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 68\n",
            "  Loss for client 2: 2.1068\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 68\n",
            "  Loss for client 3: 0.3272\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 69\n",
            "  Client 0: total grad norm = 89.752637 (253 params)\n",
            "  Client 1: total grad norm = 91.378669 (253 params)\n",
            "  Client 2: total grad norm = 74.066370 (253 params)\n",
            "  Client 3: total grad norm = 24.669816 (253 params)\n",
            "\n",
            "=== Optimizer Step 69 ===\n",
            "\n",
            "=== Updating Client Weights (Step 69) ===\n",
            "Gradient norms: [10.236310005187988, 9.442707061767578, 8.975220680236816, 4.028717994689941]\n",
            "Target weights: [0.17344887554645538, 0.18802621960639954, 0.1978198140859604, 0.4407050907611847]\n",
            "Updated weights: [0.11615969985723495, 0.10660217702388763, 0.10973700135946274, 0.6675011515617371]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 69\n",
            "  Loss for client 0: 1.6284\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 69\n",
            "  Loss for client 1: 1.9001\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 69\n",
            "  Loss for client 2: 2.4833\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 69\n",
            "  Loss for client 3: 0.0085\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 70\n",
            "  Client 0: total grad norm = 77.349575 (253 params)\n",
            "  Client 1: total grad norm = 68.414844 (253 params)\n",
            "  Client 2: total grad norm = 82.532434 (253 params)\n",
            "  Client 3: total grad norm = 5.689054 (253 params)\n",
            "\n",
            "=== Optimizer Step 70 ===\n",
            "\n",
            "=== Updating Client Weights (Step 70) ===\n",
            "Gradient norms: [9.144956588745117, 8.151041984558105, 9.801313400268555, 0.41751500964164734]\n",
            "Target weights: [0.040066879242658615, 0.04495251923799515, 0.03738375008106232, 0.8775968551635742]\n",
            "Updated weights: [0.09333185106515884, 0.08810727298259735, 0.08803102374076843, 0.7305298447608948]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 70\n",
            "  Loss for client 0: 1.9707\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 70\n",
            "  Loss for client 1: 2.5291\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 70\n",
            "  Loss for client 2: 1.9456\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 70\n",
            "  Loss for client 3: 0.0316\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 71\n",
            "  Client 0: total grad norm = 72.219415 (253 params)\n",
            "  Client 1: total grad norm = 73.571292 (253 params)\n",
            "  Client 2: total grad norm = 81.874669 (253 params)\n",
            "  Client 3: total grad norm = 45.349984 (253 params)\n",
            "\n",
            "=== Optimizer Step 71 ===\n",
            "\n",
            "=== Updating Client Weights (Step 71) ===\n",
            "Gradient norms: [8.40432071685791, 8.335143089294434, 8.545441627502441, 3.3291282653808594]\n",
            "Target weights: [0.18128204345703125, 0.18278658390045166, 0.1782883256673813, 0.457643061876297]\n",
            "Updated weights: [0.11971691250801086, 0.11651106923818588, 0.11510821431875229, 0.6486638188362122]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 71\n",
            "  Loss for client 0: 1.9108\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 71\n",
            "  Loss for client 1: 2.6589\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 71\n",
            "  Loss for client 2: 1.6485\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 71\n",
            "  Loss for client 3: 0.0168\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 72\n",
            "  Client 0: total grad norm = 72.909192 (253 params)\n",
            "  Client 1: total grad norm = 78.675944 (253 params)\n",
            "  Client 2: total grad norm = 64.743235 (253 params)\n",
            "  Client 3: total grad norm = 56.594062 (253 params)\n",
            "\n",
            "=== Optimizer Step 72 ===\n",
            "\n",
            "=== Updating Client Weights (Step 72) ===\n",
            "Gradient norms: [8.084148406982422, 8.548688888549805, 7.291500568389893, 5.2815775871276855]\n",
            "Target weights: [0.21810252964496613, 0.20625071227550507, 0.24181212484836578, 0.33383458852767944]\n",
            "Updated weights: [0.14923259615898132, 0.14343297481536865, 0.15311938524246216, 0.5542150735855103]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 72\n",
            "  Loss for client 0: 1.7790\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 72\n",
            "  Loss for client 1: 1.9416\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 72\n",
            "  Loss for client 2: 1.7451\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 72\n",
            "  Loss for client 3: 0.0058\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 73\n",
            "  Client 0: total grad norm = 65.702807 (253 params)\n",
            "  Client 1: total grad norm = 79.434905 (253 params)\n",
            "  Client 2: total grad norm = 71.705220 (253 params)\n",
            "  Client 3: total grad norm = 1.298067 (253 params)\n",
            "\n",
            "=== Optimizer Step 73 ===\n",
            "\n",
            "=== Updating Client Weights (Step 73) ===\n",
            "Gradient norms: [8.723724365234375, 9.641135215759277, 8.815300941467285, 0.14474600553512573]\n",
            "Target weights: [0.01583189330995083, 0.014325395226478577, 0.015667425468564034, 0.9541752934455872]\n",
            "Updated weights: [0.10921238362789154, 0.10470069944858551, 0.11188379675149918, 0.6742031574249268]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 73\n",
            "  Loss for client 0: 1.8089\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 73\n",
            "  Loss for client 1: 1.6556\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 73\n",
            "  Loss for client 2: 2.3422\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 73\n",
            "  Loss for client 3: 0.0072\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 74\n",
            "  Client 0: total grad norm = 69.288046 (253 params)\n",
            "  Client 1: total grad norm = 74.079607 (253 params)\n",
            "  Client 2: total grad norm = 70.536004 (253 params)\n",
            "  Client 3: total grad norm = 1.827312 (253 params)\n",
            "\n",
            "=== Optimizer Step 74 ===\n",
            "\n",
            "=== Updating Client Weights (Step 74) ===\n",
            "Gradient norms: [7.583024978637695, 8.396173477172852, 7.569406986236572, 0.20696815848350525]\n",
            "Target weights: [0.025288570672273636, 0.022839436307549477, 0.02533406764268875, 0.9265379905700684]\n",
            "Updated weights: [0.0840352326631546, 0.0801423192024231, 0.08591887354850769, 0.7499036192893982]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 74\n",
            "  Loss for client 0: 2.3216\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 74\n",
            "  Loss for client 1: 2.9785\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 74\n",
            "  Loss for client 2: 2.3901\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 74\n",
            "  Loss for client 3: 0.0095\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 75\n",
            "  Client 0: total grad norm = 94.387217 (253 params)\n",
            "  Client 1: total grad norm = 84.673368 (253 params)\n",
            "  Client 2: total grad norm = 77.664541 (253 params)\n",
            "  Client 3: total grad norm = 7.050346 (253 params)\n",
            "\n",
            "=== Optimizer Step 75 ===\n",
            "\n",
            "=== Updating Client Weights (Step 75) ===\n",
            "Gradient norms: [10.883679389953613, 10.206411361694336, 8.87401294708252, 0.6251298189163208]\n",
            "Target weights: [0.04830195754766464, 0.0515071377158165, 0.059240732342004776, 0.8409501910209656]\n",
            "Updated weights: [0.07331524789333344, 0.07155176252126694, 0.07791543006896973, 0.7772175669670105]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 75\n",
            "  Loss for client 0: 1.8824\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 75\n",
            "  Loss for client 1: 2.5234\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 75\n",
            "  Loss for client 2: 2.5458\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 75\n",
            "  Loss for client 3: 0.0106\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 76\n",
            "  Client 0: total grad norm = 78.849168 (253 params)\n",
            "  Client 1: total grad norm = 72.676040 (253 params)\n",
            "  Client 2: total grad norm = 92.489600 (253 params)\n",
            "  Client 3: total grad norm = 16.983992 (253 params)\n",
            "\n",
            "=== Optimizer Step 76 ===\n",
            "\n",
            "=== Updating Client Weights (Step 76) ===\n",
            "Gradient norms: [9.405827522277832, 8.429839134216309, 10.879127502441406, 2.070626974105835]\n",
            "Target weights: [0.1329282522201538, 0.14831839501857758, 0.11492651700973511, 0.6038268804550171]\n",
            "Updated weights: [0.09119914472103119, 0.09458175301551819, 0.08901876211166382, 0.725200355052948]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 76\n",
            "  Loss for client 0: 1.5240\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 76\n",
            "  Loss for client 1: 1.6408\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 76\n",
            "  Loss for client 2: 2.2523\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 76\n",
            "  Loss for client 3: 0.0122\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 77\n",
            "  Client 0: total grad norm = 70.805778 (253 params)\n",
            "  Client 1: total grad norm = 74.600910 (253 params)\n",
            "  Client 2: total grad norm = 66.265635 (253 params)\n",
            "  Client 3: total grad norm = 16.173493 (253 params)\n",
            "\n",
            "=== Optimizer Step 77 ===\n",
            "\n",
            "=== Updating Client Weights (Step 77) ===\n",
            "Gradient norms: [8.438837051391602, 8.86516284942627, 7.042403697967529, 1.5822935104370117]\n",
            "Target weights: [0.11787596344947815, 0.1122073084115982, 0.14124949276447296, 0.6286672353744507]\n",
            "Updated weights: [0.09920218586921692, 0.09986941516399384, 0.10468798130750656, 0.6962404251098633]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 77\n",
            "  Loss for client 0: 2.0914\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 77\n",
            "  Loss for client 1: 2.2459\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 77\n",
            "  Loss for client 2: 2.6462\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 77\n",
            "  Loss for client 3: 0.0059\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 78\n",
            "  Client 0: total grad norm = 72.048364 (253 params)\n",
            "  Client 1: total grad norm = 82.395427 (253 params)\n",
            "  Client 2: total grad norm = 81.663912 (253 params)\n",
            "  Client 3: total grad norm = 1.522303 (253 params)\n",
            "\n",
            "=== Optimizer Step 78 ===\n",
            "\n",
            "=== Updating Client Weights (Step 78) ===\n",
            "Gradient norms: [8.765411376953125, 9.291754722595215, 10.225440979003906, 0.16097156703472137]\n",
            "Target weights: [0.017466111108660698, 0.016476720571517944, 0.014972229488193989, 0.9510849118232727]\n",
            "Updated weights: [0.0746813639998436, 0.07485160231590271, 0.07777325809001923, 0.7726937532424927]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 78\n",
            "  Loss for client 0: 1.4775\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 78\n",
            "  Loss for client 1: 2.6788\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 78\n",
            "  Loss for client 2: 2.3613\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 78\n",
            "  Loss for client 3: 0.0050\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 79\n",
            "  Client 0: total grad norm = 67.408923 (253 params)\n",
            "  Client 1: total grad norm = 91.945094 (253 params)\n",
            "  Client 2: total grad norm = 81.833651 (253 params)\n",
            "  Client 3: total grad norm = 1.084643 (253 params)\n",
            "\n",
            "=== Optimizer Step 79 ===\n",
            "\n",
            "=== Updating Client Weights (Step 79) ===\n",
            "Gradient norms: [7.2289652824401855, 10.690260887145996, 9.137187004089355, 0.1391008198261261]\n",
            "Target weights: [0.01836998574435711, 0.012422147206962109, 0.01453357469290495, 0.9546743035316467]\n",
            "Updated weights: [0.057787951081991196, 0.05612276494503021, 0.058801352977752686, 0.8272879123687744]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 79\n",
            "  Loss for client 0: 1.8883\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 79\n",
            "  Loss for client 1: 2.4932\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 79\n",
            "  Loss for client 2: 2.3374\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 79\n",
            "  Loss for client 3: 0.0072\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 80\n",
            "  Client 0: total grad norm = 80.899644 (253 params)\n",
            "  Client 1: total grad norm = 92.395586 (253 params)\n",
            "  Client 2: total grad norm = 76.269708 (253 params)\n",
            "  Client 3: total grad norm = 5.937885 (253 params)\n",
            "\n",
            "=== Optimizer Step 80 ===\n",
            "\n",
            "=== Updating Client Weights (Step 80) ===\n",
            "Gradient norms: [10.665959358215332, 10.346545219421387, 8.7639741897583, 0.3612864017486572]\n",
            "Target weights: [0.030515652149915695, 0.03145771846175194, 0.037138257175683975, 0.9008883833885193]\n",
            "Updated weights: [0.049606259912252426, 0.0487232506275177, 0.052302420139312744, 0.8493680357933044]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 80\n",
            "  Loss for client 0: 1.5193\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 80\n",
            "  Loss for client 1: 2.2353\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 80\n",
            "  Loss for client 2: 2.3117\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 80\n",
            "  Loss for client 3: 0.0057\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 81\n",
            "  Client 0: total grad norm = 126.078239 (253 params)\n",
            "  Client 1: total grad norm = 94.199603 (253 params)\n",
            "  Client 2: total grad norm = 81.007759 (253 params)\n",
            "  Client 3: total grad norm = 2.849770 (253 params)\n",
            "\n",
            "=== Optimizer Step 81 ===\n",
            "\n",
            "=== Updating Client Weights (Step 81) ===\n",
            "Gradient norms: [13.508450508117676, 8.831798553466797, 9.705199241638184, 0.24455204606056213]\n",
            "Target weights: [0.016903620213270187, 0.025854498147964478, 0.02352777309715748, 0.9337141513824463]\n",
            "Updated weights: [0.03979546204209328, 0.041862618178129196, 0.04367002472281456, 0.8746718168258667]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 81\n",
            "  Loss for client 0: 1.9577\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 81\n",
            "  Loss for client 1: 2.3879\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 81\n",
            "  Loss for client 2: 2.2315\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 81\n",
            "  Loss for client 3: 0.0053\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 82\n",
            "  Client 0: total grad norm = 74.805747 (253 params)\n",
            "  Client 1: total grad norm = 82.178717 (253 params)\n",
            "  Client 2: total grad norm = 65.942034 (253 params)\n",
            "  Client 3: total grad norm = 1.941994 (253 params)\n",
            "\n",
            "=== Optimizer Step 82 ===\n",
            "\n",
            "=== Updating Client Weights (Step 82) ===\n",
            "Gradient norms: [8.543107032775879, 9.180266380310059, 7.310802936553955, 0.1858401894569397]\n",
            "Target weights: [0.02037932723760605, 0.018964892253279686, 0.023814452812075615, 0.9368413090705872]\n",
            "Updated weights: [0.03397062420845032, 0.03499330207705498, 0.03771335631608963, 0.893322765827179]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 82\n",
            "  Loss for client 0: 1.5819\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 82\n",
            "  Loss for client 1: 1.9810\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 82\n",
            "  Loss for client 2: 1.7180\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 82\n",
            "  Loss for client 3: 0.3376\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 83\n",
            "  Client 0: total grad norm = 83.418613 (253 params)\n",
            "  Client 1: total grad norm = 74.047943 (253 params)\n",
            "  Client 2: total grad norm = 64.797853 (253 params)\n",
            "  Client 3: total grad norm = 32.523233 (253 params)\n",
            "\n",
            "=== Optimizer Step 83 ===\n",
            "\n",
            "=== Updating Client Weights (Step 83) ===\n",
            "Gradient norms: [8.640317916870117, 8.309861183166504, 7.631567001342773, 4.9995880126953125]\n",
            "Target weights: [0.20407520234584808, 0.2121906280517578, 0.23105013370513916, 0.35268399119377136]\n",
            "Updated weights: [0.08500200510025024, 0.08815250545740128, 0.09571439772844315, 0.7311311364173889]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 83\n",
            "  Loss for client 0: 2.2759\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 83\n",
            "  Loss for client 1: 2.5261\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 83\n",
            "  Loss for client 2: 2.2241\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 83\n",
            "  Loss for client 3: 0.0163\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 84\n",
            "  Client 0: total grad norm = 90.271981 (253 params)\n",
            "  Client 1: total grad norm = 79.149782 (253 params)\n",
            "  Client 2: total grad norm = 101.432873 (253 params)\n",
            "  Client 3: total grad norm = 9.829873 (253 params)\n",
            "\n",
            "=== Optimizer Step 84 ===\n",
            "\n",
            "=== Updating Client Weights (Step 84) ===\n",
            "Gradient norms: [10.526702880859375, 9.182829856872559, 10.055863380432129, 0.8269190788269043]\n",
            "Target weights: [0.06280145794153214, 0.07199221849441528, 0.06574197113513947, 0.7994642853736877]\n",
            "Updated weights: [0.07834184169769287, 0.08330442011356354, 0.08672267198562622, 0.7516310811042786]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 84\n",
            "  Loss for client 0: 2.1982\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 84\n",
            "  Loss for client 1: 2.6786\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 84\n",
            "  Loss for client 2: 2.6071\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 84\n",
            "  Loss for client 3: 0.0091\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 85\n",
            "  Client 0: total grad norm = 77.188501 (253 params)\n",
            "  Client 1: total grad norm = 94.826041 (253 params)\n",
            "  Client 2: total grad norm = 103.447937 (253 params)\n",
            "  Client 3: total grad norm = 2.956889 (253 params)\n",
            "\n",
            "=== Optimizer Step 85 ===\n",
            "\n",
            "=== Updating Client Weights (Step 85) ===\n",
            "Gradient norms: [9.42458438873291, 10.569445610046387, 10.658488273620605, 0.31973397731781006]\n",
            "Target weights: [0.03100559301674366, 0.02764713019132614, 0.027416160330176353, 0.9139310717582703]\n",
            "Updated weights: [0.06414096802473068, 0.06660723686218262, 0.0689307153224945, 0.800321102142334]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 85\n",
            "  Loss for client 0: 2.2174\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 85\n",
            "  Loss for client 1: 2.9748\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 85\n",
            "  Loss for client 2: 1.9154\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 85\n",
            "  Loss for client 3: 0.0797\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 86\n",
            "  Client 0: total grad norm = 82.039781 (253 params)\n",
            "  Client 1: total grad norm = 93.730338 (253 params)\n",
            "  Client 2: total grad norm = 68.615123 (253 params)\n",
            "  Client 3: total grad norm = 95.392209 (253 params)\n",
            "\n",
            "=== Optimizer Step 86 ===\n",
            "\n",
            "=== Updating Client Weights (Step 86) ===\n",
            "Gradient norms: [9.563467979431152, 10.733259201049805, 7.937180042266846, 8.764640808105469]\n",
            "Target weights: [0.23883165419101715, 0.2128020077943802, 0.28776705265045166, 0.260599285364151]\n",
            "Updated weights: [0.11654818058013916, 0.11046567559242249, 0.13458161056041718, 0.6384045481681824]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 86\n",
            "  Loss for client 0: 2.0311\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 86\n",
            "  Loss for client 1: 2.1314\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 86\n",
            "  Loss for client 2: 2.3218\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 86\n",
            "  Loss for client 3: 0.2465\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 87\n",
            "  Client 0: total grad norm = 80.330185 (253 params)\n",
            "  Client 1: total grad norm = 76.939511 (253 params)\n",
            "  Client 2: total grad norm = 89.447813 (253 params)\n",
            "  Client 3: total grad norm = 59.865818 (253 params)\n",
            "\n",
            "=== Optimizer Step 87 ===\n",
            "\n",
            "=== Updating Client Weights (Step 87) ===\n",
            "Gradient norms: [9.977888107299805, 8.567870140075684, 9.576478004455566, 7.307909965515137]\n",
            "Target weights: [0.21873025596141815, 0.254726767539978, 0.22789859771728516, 0.29864436388015747]\n",
            "Updated weights: [0.14720280468463898, 0.15374401211738586, 0.16257670521736145, 0.5364764928817749]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 87\n",
            "  Loss for client 0: 1.7653\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 87\n",
            "  Loss for client 1: 2.3419\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 87\n",
            "  Loss for client 2: 2.2945\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 87\n",
            "  Loss for client 3: 0.2566\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 88\n",
            "  Client 0: total grad norm = 79.265560 (253 params)\n",
            "  Client 1: total grad norm = 82.646344 (253 params)\n",
            "  Client 2: total grad norm = 85.511793 (253 params)\n",
            "  Client 3: total grad norm = 7.764899 (253 params)\n",
            "\n",
            "=== Optimizer Step 88 ===\n",
            "\n",
            "=== Updating Client Weights (Step 88) ===\n",
            "Gradient norms: [8.41629695892334, 9.395291328430176, 10.153626441955566, 2.202760696411133]\n",
            "Target weights: [0.15277694165706635, 0.1368575096130371, 0.12663614749908447, 0.5837293863296509]\n",
            "Updated weights: [0.14887504279613495, 0.14867806434631348, 0.15179453790187836, 0.5506523847579956]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 88\n",
            "  Loss for client 0: 1.4273\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 88\n",
            "  Loss for client 1: 2.2130\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 88\n",
            "  Loss for client 2: 1.8070\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 88\n",
            "  Loss for client 3: 0.0079\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 89\n",
            "  Client 0: total grad norm = 144.762197 (253 params)\n",
            "  Client 1: total grad norm = 81.135143 (253 params)\n",
            "  Client 2: total grad norm = 74.305043 (253 params)\n",
            "  Client 3: total grad norm = 14.657309 (253 params)\n",
            "\n",
            "=== Optimizer Step 89 ===\n",
            "\n",
            "=== Updating Client Weights (Step 89) ===\n",
            "Gradient norms: [14.267799377441406, 9.284565925598145, 7.670165538787842, 1.0399855375289917]\n",
            "Target weights: [0.055199459195137024, 0.08482623845338821, 0.10268028825521469, 0.7572939991950989]\n",
            "Updated weights: [0.12077236920595169, 0.12952251732349396, 0.1370602548122406, 0.6126448512077332]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 89\n",
            "  Loss for client 0: 1.0973\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 89\n",
            "  Loss for client 1: 2.6006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 89\n",
            "  Loss for client 2: 2.1494\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 89\n",
            "  Loss for client 3: 0.0097\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 90\n",
            "  Client 0: total grad norm = 62.814765 (253 params)\n",
            "  Client 1: total grad norm = 89.732361 (253 params)\n",
            "  Client 2: total grad norm = 72.750607 (253 params)\n",
            "  Client 3: total grad norm = 5.322225 (253 params)\n",
            "\n",
            "=== Optimizer Step 90 ===\n",
            "\n",
            "=== Updating Client Weights (Step 90) ===\n",
            "Gradient norms: [7.269606113433838, 9.822885513305664, 8.720856666564941, 0.4871876835823059]\n",
            "Target weights: [0.057158444076776505, 0.042301151901483536, 0.047646623104810715, 0.8528938293457031]\n",
            "Updated weights: [0.1016881912946701, 0.10335610061883926, 0.11023616790771484, 0.6847195625305176]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 90\n",
            "  Loss for client 0: 2.4863\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 90\n",
            "  Loss for client 1: 1.7544\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 90\n",
            "  Loss for client 2: 2.4174\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 90\n",
            "  Loss for client 3: 0.0643\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 91\n",
            "  Client 0: total grad norm = 105.197098 (253 params)\n",
            "  Client 1: total grad norm = 75.887017 (253 params)\n",
            "  Client 2: total grad norm = 76.648880 (253 params)\n",
            "  Client 3: total grad norm = 121.674687 (253 params)\n",
            "\n",
            "=== Optimizer Step 91 ===\n",
            "\n",
            "=== Updating Client Weights (Step 91) ===\n",
            "Gradient norms: [10.592012405395508, 8.1841402053833, 8.523868560791016, 8.97453784942627]\n",
            "Target weights: [0.2119959592819214, 0.27436771988868713, 0.26343250274658203, 0.25020384788513184]\n",
            "Updated weights: [0.13478051126003265, 0.15465958416461945, 0.15619505941867828, 0.554364800453186]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 91\n",
            "  Loss for client 0: 1.6193\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 91\n",
            "  Loss for client 1: 2.0777\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 91\n",
            "  Loss for client 2: 2.6697\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 91\n",
            "  Loss for client 3: 0.0049\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 92\n",
            "  Client 0: total grad norm = 73.567193 (253 params)\n",
            "  Client 1: total grad norm = 124.107936 (253 params)\n",
            "  Client 2: total grad norm = 96.198786 (253 params)\n",
            "  Client 3: total grad norm = 1.901619 (253 params)\n",
            "\n",
            "=== Optimizer Step 92 ===\n",
            "\n",
            "=== Updating Client Weights (Step 92) ===\n",
            "Gradient norms: [7.845588207244873, 9.375577926635742, 11.585077285766602, 0.16141778230667114]\n",
            "Target weights: [0.019562480971217155, 0.016370102763175964, 0.013248005881905556, 0.9508193731307983]\n",
            "Updated weights: [0.10021510720252991, 0.1131727322936058, 0.11331094056367874, 0.6733012199401855]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 92\n",
            "  Loss for client 0: 2.7488\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 92\n",
            "  Loss for client 1: 1.8703\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 92\n",
            "  Loss for client 2: 2.2526\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 92\n",
            "  Loss for client 3: 0.0092\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 93\n",
            "  Client 0: total grad norm = 87.337838 (253 params)\n",
            "  Client 1: total grad norm = 96.161249 (253 params)\n",
            "  Client 2: total grad norm = 105.487171 (253 params)\n",
            "  Client 3: total grad norm = 56.638220 (253 params)\n",
            "\n",
            "=== Optimizer Step 93 ===\n",
            "\n",
            "=== Updating Client Weights (Step 93) ===\n",
            "Gradient norms: [10.088065147399902, 9.634676933288574, 9.35801887512207, 7.2305827140808105]\n",
            "Target weights: [0.22122597694396973, 0.2316364049911499, 0.2384844571352005, 0.3086531460285187]\n",
            "Updated weights: [0.13651837408542633, 0.14871183037757874, 0.15086299180984497, 0.5639067888259888]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 93\n",
            "  Loss for client 0: 1.4084\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 93\n",
            "  Loss for client 1: 2.7432\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 93\n",
            "  Loss for client 2: 1.3458\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 93\n",
            "  Loss for client 3: 0.0047\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 94\n",
            "  Client 0: total grad norm = 74.981139 (253 params)\n",
            "  Client 1: total grad norm = 91.287417 (253 params)\n",
            "  Client 2: total grad norm = 70.959006 (253 params)\n",
            "  Client 3: total grad norm = 1.603903 (253 params)\n",
            "\n",
            "=== Optimizer Step 94 ===\n",
            "\n",
            "=== Updating Client Weights (Step 94) ===\n",
            "Gradient norms: [8.798710823059082, 9.594637870788574, 8.283258438110352, 0.13710616528987885]\n",
            "Target weights: [0.014891210943460464, 0.013655905611813068, 0.01581786572933197, 0.9556350111961365]\n",
            "Updated weights: [0.10003023594617844, 0.10819506645202637, 0.11034946143627167, 0.6814252734184265]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 94\n",
            "  Loss for client 0: 2.4211\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 94\n",
            "  Loss for client 1: 2.7754\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 94\n",
            "  Loss for client 2: 2.2829\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 94\n",
            "  Loss for client 3: 0.0046\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 95\n",
            "  Client 0: total grad norm = 81.707920 (253 params)\n",
            "  Client 1: total grad norm = 92.063622 (253 params)\n",
            "  Client 2: total grad norm = 86.998939 (253 params)\n",
            "  Client 3: total grad norm = 2.980250 (253 params)\n",
            "\n",
            "=== Optimizer Step 95 ===\n",
            "\n",
            "=== Updating Client Weights (Step 95) ===\n",
            "Gradient norms: [9.375019073486328, 10.757761001586914, 9.698968887329102, 0.35388073325157166]\n",
            "Target weights: [0.0340946726500988, 0.029712336137890816, 0.032955896109342575, 0.9032371044158936]\n",
            "Updated weights: [0.08024957031011581, 0.08465024828910828, 0.08713139593601227, 0.7479687929153442]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 95\n",
            "  Loss for client 0: 1.4980\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 95\n",
            "  Loss for client 1: 2.2415\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 95\n",
            "  Loss for client 2: 2.3095\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 95\n",
            "  Loss for client 3: 0.0056\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 96\n",
            "  Client 0: total grad norm = 65.295408 (253 params)\n",
            "  Client 1: total grad norm = 76.924127 (253 params)\n",
            "  Client 2: total grad norm = 75.797261 (253 params)\n",
            "  Client 3: total grad norm = 1.926403 (253 params)\n",
            "\n",
            "=== Optimizer Step 96 ===\n",
            "\n",
            "=== Updating Client Weights (Step 96) ===\n",
            "Gradient norms: [6.728490829467773, 8.218528747558594, 8.860344886779785, 0.16371746361255646]\n",
            "Target weights: [0.022895725443959236, 0.018744679167866707, 0.017386872321367264, 0.9409726858139038]\n",
            "Updated weights: [0.06304341554641724, 0.06487857550382614, 0.06620803475379944, 0.805869996547699]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 96\n",
            "  Loss for client 0: 1.6971\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 96\n",
            "  Loss for client 1: 1.4727\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 96\n",
            "  Loss for client 2: 1.5751\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 96\n",
            "  Loss for client 3: 0.0041\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 97\n",
            "  Client 0: total grad norm = 86.655796 (253 params)\n",
            "  Client 1: total grad norm = 65.801852 (253 params)\n",
            "  Client 2: total grad norm = 73.905555 (253 params)\n",
            "  Client 3: total grad norm = 1.142465 (253 params)\n",
            "\n",
            "=== Optimizer Step 97 ===\n",
            "\n",
            "=== Updating Client Weights (Step 97) ===\n",
            "Gradient norms: [8.473133087158203, 8.621146202087402, 7.994347095489502, 0.11586382985115051]\n",
            "Target weights: [0.01312804315239191, 0.012902653776109219, 0.013914290815591812, 0.9600549936294556]\n",
            "Updated weights: [0.048068806529045105, 0.049285802990198135, 0.0505199171602726, 0.8521255254745483]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 97\n",
            "  Loss for client 0: 1.8767\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 97\n",
            "  Loss for client 1: 2.6944\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 97\n",
            "  Loss for client 2: 2.5736\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 97\n",
            "  Loss for client 3: 0.0049\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 98\n",
            "  Client 0: total grad norm = 83.650106 (253 params)\n",
            "  Client 1: total grad norm = 95.102949 (253 params)\n",
            "  Client 2: total grad norm = 82.033606 (253 params)\n",
            "  Client 3: total grad norm = 1.229045 (253 params)\n",
            "\n",
            "=== Optimizer Step 98 ===\n",
            "\n",
            "=== Updating Client Weights (Step 98) ===\n",
            "Gradient norms: [9.529476165771484, 8.85891056060791, 8.694676399230957, 0.11887646466493607]\n",
            "Target weights: [0.0119998250156641, 0.01290813833475113, 0.013151961378753185, 0.9619400501251221]\n",
            "Updated weights: [0.03724810853600502, 0.038372501730918884, 0.03930952772498131, 0.8850697875022888]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 98\n",
            "  Loss for client 0: 1.4439\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 98\n",
            "  Loss for client 1: 2.4214\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 98\n",
            "  Loss for client 2: 2.6929\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 98\n",
            "  Loss for client 3: 0.2737\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 99\n",
            "  Client 0: total grad norm = 76.578421 (253 params)\n",
            "  Client 1: total grad norm = 81.961331 (253 params)\n",
            "  Client 2: total grad norm = 94.009891 (253 params)\n",
            "  Client 3: total grad norm = 19.037514 (253 params)\n",
            "\n",
            "=== Optimizer Step 99 ===\n",
            "\n",
            "=== Updating Client Weights (Step 99) ===\n",
            "Gradient norms: [7.826929569244385, 8.268125534057617, 10.33952522277832, 3.187675714492798]\n",
            "Target weights: [0.19383592903614044, 0.18349264562129974, 0.14673209190368652, 0.4759393036365509]\n",
            "Updated weights: [0.08422446250915527, 0.08190855383872986, 0.07153630256652832, 0.7623307108879089]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 99\n",
            "  Loss for client 0: 1.4830\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 99\n",
            "  Loss for client 1: 2.1411\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 99\n",
            "  Loss for client 2: 2.8269\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 99\n",
            "  Loss for client 3: 0.0221\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 100\n",
            "  Client 0: total grad norm = 74.291259 (253 params)\n",
            "  Client 1: total grad norm = 85.174102 (253 params)\n",
            "  Client 2: total grad norm = 112.345535 (253 params)\n",
            "  Client 3: total grad norm = 51.554040 (253 params)\n",
            "\n",
            "=== Optimizer Step 100 ===\n",
            "\n",
            "=== Updating Client Weights (Step 100) ===\n",
            "Gradient norms: [7.445208549499512, 11.018498420715332, 10.779463768005371, 3.083251476287842]\n",
            "Target weights: [0.20915637910366058, 0.1413271427154541, 0.14446106553077698, 0.5050554275512695]\n",
            "Updated weights: [0.12170404195785522, 0.09973412752151489, 0.09341373294591904, 0.6851481199264526]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 100\n",
            "  Val Loss = 1.2283 (4 batches)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62517]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ –æ–±—Ä–µ–∑–∞–Ω—ã –¥–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç—Ä–æ–∫ (5000).\u001b[0m\n",
            "[Processing] Client 0, batch 334\n",
            "  Loss for client 0: 2.0615\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 334\n",
            "  Loss for client 1: 1.9625\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 334\n",
            "  Loss for client 2: 2.3843\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 334\n",
            "  Loss for client 3: 0.0005\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 335\n",
            "  Client 0: total grad norm = 97.129739 (253 params)\n",
            "  Client 1: total grad norm = 76.983929 (253 params)\n",
            "  Client 2: total grad norm = 83.175102 (253 params)\n",
            "  Client 3: total grad norm = 0.171415 (253 params)\n",
            "\n",
            "=== Optimizer Step 335 ===\n",
            "\n",
            "=== Updating Client Weights (Step 335) ===\n",
            "Gradient norms: [10.212879180908203, 8.8919095993042, 8.625021934509277, 0.016005506739020348]\n",
            "Target weights: [0.0015590466791763902, 0.0017906564753502607, 0.0018460655119270086, 0.9948042631149292]\n",
            "Updated weights: [0.01663251779973507, 0.017688851803541183, 0.015255182050168514, 0.9504233598709106]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 335\n",
            "  Loss for client 0: 1.0498\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 335\n",
            "  Loss for client 1: 2.3132\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 335\n",
            "  Loss for client 2: 2.1958\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 335\n",
            "  Loss for client 3: 0.0009\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 336\n",
            "  Client 0: total grad norm = 85.055870 (253 params)\n",
            "  Client 1: total grad norm = 76.304651 (253 params)\n",
            "  Client 2: total grad norm = 86.773851 (253 params)\n",
            "  Client 3: total grad norm = 1.045174 (253 params)\n",
            "\n",
            "=== Optimizer Step 336 ===\n",
            "\n",
            "=== Updating Client Weights (Step 336) ===\n",
            "Gradient norms: [9.888398170471191, 9.76965618133545, 8.615028381347656, 0.06887009739875793]\n",
            "Target weights: [0.0068147568963468075, 0.006897584535181522, 0.00782203208655119, 0.9784656167030334]\n",
            "Updated weights: [0.013687191531062126, 0.014451473951339722, 0.013025239109992981, 0.9588361382484436]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 336\n",
            "  Loss for client 0: 0.8788\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 336\n",
            "  Loss for client 1: 1.7487\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 336\n",
            "  Loss for client 2: 1.7620\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 336\n",
            "  Loss for client 3: 0.1858\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 337\n",
            "  Client 0: total grad norm = 89.370873 (253 params)\n",
            "  Client 1: total grad norm = 74.198226 (253 params)\n",
            "  Client 2: total grad norm = 82.912379 (253 params)\n",
            "  Client 3: total grad norm = 9.992080 (253 params)\n",
            "\n",
            "=== Optimizer Step 337 ===\n",
            "\n",
            "=== Updating Client Weights (Step 337) ===\n",
            "Gradient norms: [9.983386039733887, 7.7392048835754395, 9.182952880859375, 2.1758460998535156]\n",
            "Target weights: [0.12554268538951874, 0.16194701194763184, 0.13648562133312225, 0.5760247111320496]\n",
            "Updated weights: [0.047243840992450714, 0.058700136840343475, 0.0500633530318737, 0.8439927101135254]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 337\n",
            "  Loss for client 0: 1.3132\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 337\n",
            "  Loss for client 1: 2.2398\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 337\n",
            "  Loss for client 2: 2.3695\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 337\n",
            "  Loss for client 3: 0.0007\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 338\n",
            "  Client 0: total grad norm = 84.520763 (253 params)\n",
            "  Client 1: total grad norm = 83.189465 (253 params)\n",
            "  Client 2: total grad norm = 97.596861 (253 params)\n",
            "  Client 3: total grad norm = 0.229127 (253 params)\n",
            "\n",
            "=== Optimizer Step 338 ===\n",
            "\n",
            "=== Updating Client Weights (Step 338) ===\n",
            "Gradient norms: [8.084819793701172, 8.459383964538574, 11.659518241882324, 0.022862045094370842]\n",
            "Target weights: [0.0028067496605217457, 0.0026824725791811943, 0.0019462266936898232, 0.9925645589828491]\n",
            "Updated weights: [0.03391271084547043, 0.041894834488630295, 0.03562821447849274, 0.8885642290115356]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 338\n",
            "  Loss for client 0: 1.3938\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 338\n",
            "  Loss for client 1: 1.8989\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 338\n",
            "  Loss for client 2: 1.9996\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 338\n",
            "  Loss for client 3: 0.0007\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 339\n",
            "  Client 0: total grad norm = 71.009950 (253 params)\n",
            "  Client 1: total grad norm = 81.298564 (253 params)\n",
            "  Client 2: total grad norm = 85.388865 (253 params)\n",
            "  Client 3: total grad norm = 0.321878 (253 params)\n",
            "\n",
            "=== Optimizer Step 339 ===\n",
            "\n",
            "=== Updating Client Weights (Step 339) ===\n",
            "Gradient norms: [7.489126205444336, 8.543895721435547, 9.768588066101074, 0.030926154926419258]\n",
            "Target weights: [0.0040848893113434315, 0.003580597462132573, 0.003131696255877614, 0.9892029166221619]\n",
            "Updated weights: [0.024964358657598495, 0.030400559306144714, 0.025879254564642906, 0.9187557697296143]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 339\n",
            "  Loss for client 0: 2.1084\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 339\n",
            "  Loss for client 1: 1.1626\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 339\n",
            "  Loss for client 2: 1.5664\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 339\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 340\n",
            "  Client 0: total grad norm = 75.441156 (253 params)\n",
            "  Client 1: total grad norm = 54.157072 (253 params)\n",
            "  Client 2: total grad norm = 72.905724 (253 params)\n",
            "  Client 3: total grad norm = 0.111523 (253 params)\n",
            "\n",
            "=== Optimizer Step 340 ===\n",
            "\n",
            "=== Updating Client Weights (Step 340) ===\n",
            "Gradient norms: [8.08183765411377, 5.876067638397217, 8.435904502868652, 0.010795151814818382]\n",
            "Target weights: [0.0013298102421686053, 0.001828997046686709, 0.0012739961966872215, 0.995567262172699]\n",
            "Updated weights: [0.017873991280794144, 0.021829089149832726, 0.01849767379462719, 0.9417991638183594]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 340\n",
            "  Loss for client 0: 1.4931\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 340\n",
            "  Loss for client 1: 2.2820\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 340\n",
            "  Loss for client 2: 2.1101\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 340\n",
            "  Loss for client 3: 0.2335\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 341\n",
            "  Client 0: total grad norm = 85.132983 (253 params)\n",
            "  Client 1: total grad norm = 71.976888 (253 params)\n",
            "  Client 2: total grad norm = 88.367379 (253 params)\n",
            "  Client 3: total grad norm = 6.770165 (253 params)\n",
            "\n",
            "=== Optimizer Step 341 ===\n",
            "\n",
            "=== Updating Client Weights (Step 341) ===\n",
            "Gradient norms: [9.664770126342773, 8.041119575500488, 10.729227066040039, 2.180267333984375]\n",
            "Target weights: [0.13270440697669983, 0.1594998836517334, 0.11953867971897125, 0.5882570743560791]\n",
            "Updated weights: [0.052323117852211, 0.06313033401966095, 0.048809975385665894, 0.8357365131378174]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 341\n",
            "  Loss for client 0: 1.3633\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 341\n",
            "  Loss for client 1: 2.0064\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 341\n",
            "  Loss for client 2: 1.9407\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 341\n",
            "  Loss for client 3: 0.2106\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 342\n",
            "  Client 0: total grad norm = 104.002884 (253 params)\n",
            "  Client 1: total grad norm = 83.952980 (253 params)\n",
            "  Client 2: total grad norm = 71.253998 (253 params)\n",
            "  Client 3: total grad norm = 9.291926 (253 params)\n",
            "\n",
            "=== Optimizer Step 342 ===\n",
            "\n",
            "=== Updating Client Weights (Step 342) ===\n",
            "Gradient norms: [11.540513038635254, 9.136475563049316, 8.676417350769043, 2.345660448074341]\n",
            "Target weights: [0.117465078830719, 0.14837311208248138, 0.15624044835567474, 0.5779213905334473]\n",
            "Updated weights: [0.07186570763587952, 0.08870317041873932, 0.08103911578655243, 0.7583919763565063]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 342\n",
            "  Loss for client 0: 1.4978\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 342\n",
            "  Loss for client 1: 1.9735\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 342\n",
            "  Loss for client 2: 1.6995\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 342\n",
            "  Loss for client 3: 0.0006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 343\n",
            "  Client 0: total grad norm = 96.785813 (253 params)\n",
            "  Client 1: total grad norm = 82.524438 (253 params)\n",
            "  Client 2: total grad norm = 73.353787 (253 params)\n",
            "  Client 3: total grad norm = 0.181444 (253 params)\n",
            "\n",
            "=== Optimizer Step 343 ===\n",
            "\n",
            "=== Updating Client Weights (Step 343) ===\n",
            "Gradient norms: [9.495560646057129, 8.919057846069336, 8.996137619018555, 0.01909603178501129]\n",
            "Target weights: [0.0019985090475529432, 0.0021276872139424086, 0.002109457040205598, 0.993764340877533]\n",
            "Updated weights: [0.05090554803609848, 0.06273052841424942, 0.0573602169752121, 0.8290036916732788]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 343\n",
            "  Loss for client 0: 1.7635\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 343\n",
            "  Loss for client 1: 2.5355\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 343\n",
            "  Loss for client 2: 1.7508\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 343\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 344\n",
            "  Client 0: total grad norm = 81.268785 (253 params)\n",
            "  Client 1: total grad norm = 91.748204 (253 params)\n",
            "  Client 2: total grad norm = 68.201488 (253 params)\n",
            "  Client 3: total grad norm = 0.115225 (253 params)\n",
            "\n",
            "=== Optimizer Step 344 ===\n",
            "\n",
            "=== Updating Client Weights (Step 344) ===\n",
            "Gradient norms: [9.452802658081055, 11.080538749694824, 7.617993354797363, 0.011492513120174408]\n",
            "Target weights: [0.0012112233089283109, 0.0010332940146327019, 0.0015029489295557141, 0.9962524771690369]\n",
            "Updated weights: [0.03599725291132927, 0.04422135651111603, 0.04060303419828415, 0.8791783452033997]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 344\n",
            "  Loss for client 0: 2.4912\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 344\n",
            "  Loss for client 1: 1.8469\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 344\n",
            "  Loss for client 2: 1.2714\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 344\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 345\n",
            "  Client 0: total grad norm = 86.315680 (253 params)\n",
            "  Client 1: total grad norm = 69.772531 (253 params)\n",
            "  Client 2: total grad norm = 63.974593 (253 params)\n",
            "  Client 3: total grad norm = 0.116296 (253 params)\n",
            "\n",
            "=== Optimizer Step 345 ===\n",
            "\n",
            "=== Updating Client Weights (Step 345) ===\n",
            "Gradient norms: [9.982732772827148, 8.384836196899414, 7.871040344238281, 0.01375692617148161]\n",
            "Target weights: [0.0013715357054024935, 0.0016329089412465692, 0.0017395000904798508, 0.9952560663223267]\n",
            "Updated weights: [0.025609537959098816, 0.03144482523202896, 0.028943974524736404, 0.9140017032623291]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 345\n",
            "  Loss for client 0: 1.7068\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 345\n",
            "  Loss for client 1: 2.5001\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 345\n",
            "  Loss for client 2: 2.0781\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 345\n",
            "  Loss for client 3: 0.0003\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 346\n",
            "  Client 0: total grad norm = 87.569839 (253 params)\n",
            "  Client 1: total grad norm = 79.717912 (253 params)\n",
            "  Client 2: total grad norm = 76.623863 (253 params)\n",
            "  Client 3: total grad norm = 0.081061 (253 params)\n",
            "\n",
            "=== Optimizer Step 346 ===\n",
            "\n",
            "=== Updating Client Weights (Step 346) ===\n",
            "Gradient norms: [9.550619125366211, 8.374372482299805, 8.292906761169434, 0.009622610174119473]\n",
            "Target weights: [0.0010042079957202077, 0.0011452570324763656, 0.0011565074091777205, 0.9966940879821777]\n",
            "Updated weights: [0.018227938562631607, 0.022354954853653908, 0.020607734099030495, 0.9388094544410706]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 346\n",
            "  Loss for client 0: 2.2517\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 346\n",
            "  Loss for client 1: 2.3235\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 346\n",
            "  Loss for client 2: 1.8690\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 346\n",
            "  Loss for client 3: 0.0007\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 347\n",
            "  Client 0: total grad norm = 91.199004 (253 params)\n",
            "  Client 1: total grad norm = 104.084816 (253 params)\n",
            "  Client 2: total grad norm = 87.643317 (253 params)\n",
            "  Client 3: total grad norm = 0.397531 (253 params)\n",
            "\n",
            "=== Optimizer Step 347 ===\n",
            "\n",
            "=== Updating Client Weights (Step 347) ===\n",
            "Gradient norms: [10.698684692382812, 12.112892150878906, 9.357742309570312, 0.02953018806874752]\n",
            "Target weights: [0.002737303962931037, 0.002417717594653368, 0.00312955304980278, 0.9917153716087341]\n",
            "Updated weights: [0.0135807478800416, 0.016373783349990845, 0.015364279970526695, 0.9546812176704407]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 347\n",
            "  Loss for client 0: 1.9259\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 347\n",
            "  Loss for client 1: 2.3380\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 347\n",
            "  Loss for client 2: 1.9276\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 347\n",
            "  Loss for client 3: 0.4146\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 348\n",
            "  Client 0: total grad norm = 88.338112 (253 params)\n",
            "  Client 1: total grad norm = 98.903753 (253 params)\n",
            "  Client 2: total grad norm = 80.167885 (253 params)\n",
            "  Client 3: total grad norm = 11.230981 (253 params)\n",
            "\n",
            "=== Optimizer Step 348 ===\n",
            "\n",
            "=== Updating Client Weights (Step 348) ===\n",
            "Gradient norms: [8.972088813781738, 9.517542839050293, 8.65552043914795, 3.1535520553588867]\n",
            "Target weights: [0.171693354845047, 0.16185353696346283, 0.17797288298606873, 0.48848026990890503]\n",
            "Updated weights: [0.06101452559232712, 0.06001770496368408, 0.06414685398340225, 0.8148208260536194]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 348\n",
            "  Loss for client 0: 1.1750\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 348\n",
            "  Loss for client 1: 1.8126\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 348\n",
            "  Loss for client 2: 1.3134\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 348\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 349\n",
            "  Client 0: total grad norm = 54.113435 (253 params)\n",
            "  Client 1: total grad norm = 89.172901 (253 params)\n",
            "  Client 2: total grad norm = 63.864570 (253 params)\n",
            "  Client 3: total grad norm = 0.103238 (253 params)\n",
            "\n",
            "=== Optimizer Step 349 ===\n",
            "\n",
            "=== Updating Client Weights (Step 349) ===\n",
            "Gradient norms: [6.023616313934326, 9.797537803649902, 6.947973251342773, 0.011877921409904957]\n",
            "Target weights: [0.0019622906111180782, 0.0012064343318343163, 0.001701227854937315, 0.9951300024986267]\n",
            "Updated weights: [0.043298859149217606, 0.04237432777881622, 0.045413170009851456, 0.8689136505126953]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 349\n",
            "  Loss for client 0: 2.4791\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 349\n",
            "  Loss for client 1: 2.0844\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 349\n",
            "  Loss for client 2: 1.8969\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 349\n",
            "  Loss for client 3: 0.2132\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 350\n",
            "  Client 0: total grad norm = 96.193383 (253 params)\n",
            "  Client 1: total grad norm = 79.358324 (253 params)\n",
            "  Client 2: total grad norm = 91.494127 (253 params)\n",
            "  Client 3: total grad norm = 7.858227 (253 params)\n",
            "\n",
            "=== Optimizer Step 350 ===\n",
            "\n",
            "=== Updating Client Weights (Step 350) ===\n",
            "Gradient norms: [9.627263069152832, 8.503401756286621, 9.803500175476074, 2.257091760635376]\n",
            "Target weights: [0.13550999760627747, 0.1534198373556137, 0.13307395577430725, 0.5779961943626404]\n",
            "Updated weights: [0.07096220552921295, 0.07568798214197159, 0.07171140611171722, 0.7816383838653564]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 350\n",
            "  Val Loss = 1.0365 (4 batches)\n",
            "\n",
            "[Processing] Client 0, batch 350\n",
            "  Loss for client 0: 2.1676\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 350\n",
            "  Loss for client 1: 2.0011\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 350\n",
            "  Loss for client 2: 1.4633\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 350\n",
            "  Loss for client 3: 0.0008\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 351\n",
            "  Client 0: total grad norm = 86.703254 (253 params)\n",
            "  Client 1: total grad norm = 73.842165 (253 params)\n",
            "  Client 2: total grad norm = 74.010592 (253 params)\n",
            "  Client 3: total grad norm = 0.340961 (253 params)\n",
            "\n",
            "=== Optimizer Step 351 ===\n",
            "\n",
            "=== Updating Client Weights (Step 351) ===\n",
            "Gradient norms: [11.018105506896973, 7.950813293457031, 7.47784423828125, 0.03605486452579498]\n",
            "Target weights: [0.003231520066037774, 0.004478186834603548, 0.0047614299692213535, 0.9875288605690002]\n",
            "Updated weights: [0.05064299702644348, 0.05432504042983055, 0.05162641406059265, 0.8434054851531982]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 351\n",
            "  Loss for client 0: 1.4197\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 351\n",
            "  Loss for client 1: 1.6448\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 351\n",
            "  Loss for client 2: 2.0421\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 351\n",
            "  Loss for client 3: 0.0005\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 352\n",
            "  Client 0: total grad norm = 73.667674 (253 params)\n",
            "  Client 1: total grad norm = 78.419054 (253 params)\n",
            "  Client 2: total grad norm = 77.506699 (253 params)\n",
            "  Client 3: total grad norm = 0.160149 (253 params)\n",
            "\n",
            "=== Optimizer Step 352 ===\n",
            "\n",
            "=== Updating Client Weights (Step 352) ===\n",
            "Gradient norms: [8.709860801696777, 8.145869255065918, 9.160916328430176, 0.019178392365574837]\n",
            "Target weights: [0.002187372650951147, 0.0023388187400996685, 0.002079673344269395, 0.993394136428833]\n",
            "Updated weights: [0.03610631078481674, 0.03872917592525482, 0.03676239028573036, 0.8884021043777466]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 352\n",
            "  Loss for client 0: 2.1165\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 352\n",
            "  Loss for client 1: 1.6718\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 352\n",
            "  Loss for client 2: 1.9678\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 352\n",
            "  Loss for client 3: 0.0005\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 353\n",
            "  Client 0: total grad norm = 85.510023 (253 params)\n",
            "  Client 1: total grad norm = 80.271716 (253 params)\n",
            "  Client 2: total grad norm = 91.218777 (253 params)\n",
            "  Client 3: total grad norm = 0.137970 (253 params)\n",
            "\n",
            "=== Optimizer Step 353 ===\n",
            "\n",
            "=== Updating Client Weights (Step 353) ===\n",
            "Gradient norms: [9.282666206359863, 9.871606826782227, 10.254594802856445, 0.016586726531386375]\n",
            "Target weights: [0.0017778108594939113, 0.0016717466060072184, 0.0016093101585283875, 0.9949410557746887]\n",
            "Updated weights: [0.025807762518525124, 0.027611948549747467, 0.02621646784245968, 0.9203638434410095]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 353\n",
            "  Loss for client 0: 0.7442\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 353\n",
            "  Loss for client 1: 1.7521\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 353\n",
            "  Loss for client 2: 2.1665\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 353\n",
            "  Loss for client 3: 0.0006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 354\n",
            "  Client 0: total grad norm = 65.878353 (253 params)\n",
            "  Client 1: total grad norm = 83.805165 (253 params)\n",
            "  Client 2: total grad norm = 77.931552 (253 params)\n",
            "  Client 3: total grad norm = 0.225951 (253 params)\n",
            "\n",
            "=== Optimizer Step 354 ===\n",
            "\n",
            "=== Updating Client Weights (Step 354) ===\n",
            "Gradient norms: [7.6224684715271, 8.165837287902832, 8.750001907348633, 0.024174358695745468]\n",
            "Target weights: [0.003143501468002796, 0.0029343278147280216, 0.002738427137956023, 0.9911838173866272]\n",
            "Updated weights: [0.019008483737707138, 0.020208662375807762, 0.01917305588722229, 0.9416098594665527]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 354\n",
            "  Loss for client 0: 1.5376\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 354\n",
            "  Loss for client 1: 1.9857\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 354\n",
            "  Loss for client 2: 1.6352\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 354\n",
            "  Loss for client 3: 0.0007\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 355\n",
            "  Client 0: total grad norm = 69.339771 (253 params)\n",
            "  Client 1: total grad norm = 95.504725 (253 params)\n",
            "  Client 2: total grad norm = 81.662681 (253 params)\n",
            "  Client 3: total grad norm = 0.501302 (253 params)\n",
            "\n",
            "=== Optimizer Step 355 ===\n",
            "\n",
            "=== Updating Client Weights (Step 355) ===\n",
            "Gradient norms: [7.859602451324463, 9.958138465881348, 10.191084861755371, 0.03265342116355896]\n",
            "Target weights: [0.004110860172659159, 0.003244555089622736, 0.0031703913118690252, 0.9894741773605347]\n",
            "Updated weights: [0.014539197087287903, 0.015119429677724838, 0.014372256584465504, 0.9559691548347473]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 355\n",
            "  Loss for client 0: 2.0647\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 355\n",
            "  Loss for client 1: 1.9523\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 355\n",
            "  Loss for client 2: 1.5373\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 355\n",
            "  Loss for client 3: 0.0014\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 356\n",
            "  Client 0: total grad norm = 92.570617 (253 params)\n",
            "  Client 1: total grad norm = 80.094343 (253 params)\n",
            "  Client 2: total grad norm = 61.633813 (253 params)\n",
            "  Client 3: total grad norm = 2.253874 (253 params)\n",
            "\n",
            "=== Optimizer Step 356 ===\n",
            "\n",
            "=== Updating Client Weights (Step 356) ===\n",
            "Gradient norms: [9.539332389831543, 9.626654624938965, 6.815277099609375, 0.29397791624069214]\n",
            "Target weights: [0.027901962399482727, 0.027648866176605225, 0.03905433043837547, 0.9053948521614075]\n",
            "Updated weights: [0.01854802668094635, 0.018878262490034103, 0.021776879206299782, 0.9407968521118164]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 356\n",
            "  Loss for client 0: 1.4197\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 356\n",
            "  Loss for client 1: 2.3266\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 356\n",
            "  Loss for client 2: 1.9708\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 356\n",
            "  Loss for client 3: 0.0007\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 357\n",
            "  Client 0: total grad norm = 73.631180 (253 params)\n",
            "  Client 1: total grad norm = 78.528149 (253 params)\n",
            "  Client 2: total grad norm = 74.374489 (253 params)\n",
            "  Client 3: total grad norm = 0.357273 (253 params)\n",
            "\n",
            "=== Optimizer Step 357 ===\n",
            "\n",
            "=== Updating Client Weights (Step 357) ===\n",
            "Gradient norms: [7.774148941040039, 8.300188064575195, 8.18621826171875, 0.03903913125395775]\n",
            "Target weights: [0.004949917551130056, 0.0046362075954675674, 0.004700753837823868, 0.9857131242752075]\n",
            "Updated weights: [0.014468593522906303, 0.014605646021664143, 0.016654040664434433, 0.9542717337608337]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 357\n",
            "  Loss for client 0: 1.5770\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 357\n",
            "  Loss for client 1: 1.8732\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 357\n",
            "  Loss for client 2: 1.5953\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 357\n",
            "  Loss for client 3: 0.0057\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 358\n",
            "  Client 0: total grad norm = 80.356446 (253 params)\n",
            "  Client 1: total grad norm = 73.997385 (253 params)\n",
            "  Client 2: total grad norm = 68.839447 (253 params)\n",
            "  Client 3: total grad norm = 3.724497 (253 params)\n",
            "\n",
            "=== Optimizer Step 358 ===\n",
            "\n",
            "=== Updating Client Weights (Step 358) ===\n",
            "Gradient norms: [8.827611923217773, 7.235420227050781, 7.5481157302856445, 0.3953907787799835]\n",
            "Target weights: [0.03888649865984917, 0.04744367301464081, 0.045478228479623795, 0.8681915998458862]\n",
            "Updated weights: [0.021793965250253677, 0.0244570542126894, 0.025301296263933182, 0.9284477233886719]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 358\n",
            "  Loss for client 0: 1.9200\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 358\n",
            "  Loss for client 1: 2.3702\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 358\n",
            "  Loss for client 2: 2.4027\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 358\n",
            "  Loss for client 3: 0.0017\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 359\n",
            "  Client 0: total grad norm = 94.460009 (253 params)\n",
            "  Client 1: total grad norm = 86.383381 (253 params)\n",
            "  Client 2: total grad norm = 99.831315 (253 params)\n",
            "  Client 3: total grad norm = 1.294611 (253 params)\n",
            "\n",
            "=== Optimizer Step 359 ===\n",
            "\n",
            "=== Updating Client Weights (Step 359) ===\n",
            "Gradient norms: [9.893410682678223, 9.060312271118164, 11.255254745483398, 0.1277015507221222]\n",
            "Target weights: [0.012431030161678791, 0.013574066571891308, 0.010926921851933002, 0.9630679488182068]\n",
            "Updated weights: [0.0189850851893425, 0.02119215950369835, 0.020988984033465385, 0.9388337731361389]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 359\n",
            "  Loss for client 0: 1.5180\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 359\n",
            "  Loss for client 1: 1.9056\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 359\n",
            "  Loss for client 2: 2.4202\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 359\n",
            "  Loss for client 3: 0.0006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 360\n",
            "  Client 0: total grad norm = 58.689053 (253 params)\n",
            "  Client 1: total grad norm = 94.176875 (253 params)\n",
            "  Client 2: total grad norm = 99.024109 (253 params)\n",
            "  Client 3: total grad norm = 0.274737 (253 params)\n",
            "\n",
            "=== Optimizer Step 360 ===\n",
            "\n",
            "=== Updating Client Weights (Step 360) ===\n",
            "Gradient norms: [5.870249271392822, 9.658858299255371, 11.191210746765137, 0.026682620868086815]\n",
            "Target weights: [0.004501767922192812, 0.0027359856758266687, 0.0023613618686795235, 0.9904008507728577]\n",
            "Updated weights: [0.014640090055763721, 0.015655307099223137, 0.015400697477161884, 0.9543039202690125]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 360\n",
            "  Loss for client 0: 1.4378\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 360\n",
            "  Loss for client 1: 2.5850\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 360\n",
            "  Loss for client 2: 1.6206\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 360\n",
            "  Loss for client 3: 0.0005\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 361\n",
            "  Client 0: total grad norm = 68.005426 (253 params)\n",
            "  Client 1: total grad norm = 82.491573 (253 params)\n",
            "  Client 2: total grad norm = 82.800012 (253 params)\n",
            "  Client 3: total grad norm = 0.153151 (253 params)\n",
            "\n",
            "=== Optimizer Step 361 ===\n",
            "\n",
            "=== Updating Client Weights (Step 361) ===\n",
            "Gradient norms: [7.557229042053223, 8.931855201721191, 8.563376426696777, 0.017836663872003555]\n",
            "Target weights: [0.0023451107554137707, 0.0019841946195811033, 0.002069573849439621, 0.9936011433601379]\n",
            "Updated weights: [0.010951596312224865, 0.011553972959518433, 0.011401359923183918, 0.9660930633544922]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 361\n",
            "  Loss for client 0: 1.1446\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 361\n",
            "  Loss for client 1: 2.1435\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 361\n",
            "  Loss for client 2: 1.5053\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 361\n",
            "  Loss for client 3: 0.0008\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 362\n",
            "  Client 0: total grad norm = 74.609017 (253 params)\n",
            "  Client 1: total grad norm = 101.016804 (253 params)\n",
            "  Client 2: total grad norm = 73.531500 (253 params)\n",
            "  Client 3: total grad norm = 0.576574 (253 params)\n",
            "\n",
            "=== Optimizer Step 362 ===\n",
            "\n",
            "=== Updating Client Weights (Step 362) ===\n",
            "Gradient norms: [7.7008562088012695, 10.521642684936523, 8.47637939453125, 0.07705625146627426]\n",
            "Target weights: [0.009748629294335842, 0.0071350825019180775, 0.008856705389916897, 0.9742594957351685]\n",
            "Updated weights: [0.010590706951916218, 0.010228306986391544, 0.010637964121997356, 0.9685430526733398]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 362\n",
            "  Loss for client 0: 1.6602\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 362\n",
            "  Loss for client 1: 2.0116\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 362\n",
            "  Loss for client 2: 1.5689\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 362\n",
            "  Loss for client 3: 0.0009\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 363\n",
            "  Client 0: total grad norm = 81.363021 (253 params)\n",
            "  Client 1: total grad norm = 100.267231 (253 params)\n",
            "  Client 2: total grad norm = 68.261885 (253 params)\n",
            "  Client 3: total grad norm = 0.613165 (253 params)\n",
            "\n",
            "=== Optimizer Step 363 ===\n",
            "\n",
            "=== Updating Client Weights (Step 363) ===\n",
            "Gradient norms: [9.380382537841797, 10.016866683959961, 7.422826766967773, 0.05758325010538101]\n",
            "Target weights: [0.006020419765263796, 0.005637874826788902, 0.007608131039887667, 0.9807336330413818]\n",
            "Updated weights: [0.009219620376825333, 0.00885117705911398, 0.009729014709591866, 0.9722002148628235]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 363\n",
            "  Loss for client 0: 1.7254\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 363\n",
            "  Loss for client 1: 1.8521\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 363\n",
            "  Loss for client 2: 2.1774\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 363\n",
            "  Loss for client 3: 0.0017\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 364\n",
            "  Client 0: total grad norm = 89.108175 (253 params)\n",
            "  Client 1: total grad norm = 80.861553 (253 params)\n",
            "  Client 2: total grad norm = 85.733145 (253 params)\n",
            "  Client 3: total grad norm = 1.123131 (253 params)\n",
            "\n",
            "=== Optimizer Step 364 ===\n",
            "\n",
            "=== Updating Client Weights (Step 364) ===\n",
            "Gradient norms: [9.538436889648438, 8.855825424194336, 9.83010482788086, 0.10052403062582016]\n",
            "Target weights: [0.01021090243011713, 0.010997964069247246, 0.009907935746014118, 0.9688831567764282]\n",
            "Updated weights: [0.0095170047134161, 0.009495212696492672, 0.009782691486179829, 0.9712051153182983]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 364\n",
            "  Loss for client 0: 1.8785\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 364\n",
            "  Loss for client 1: 1.9802\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 364\n",
            "  Loss for client 2: 2.4368\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 364\n",
            "  Loss for client 3: 0.0009\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 365\n",
            "  Client 0: total grad norm = 87.678983 (253 params)\n",
            "  Client 1: total grad norm = 75.979016 (253 params)\n",
            "  Client 2: total grad norm = 85.636666 (253 params)\n",
            "  Client 3: total grad norm = 0.358562 (253 params)\n",
            "\n",
            "=== Optimizer Step 365 ===\n",
            "\n",
            "=== Updating Client Weights (Step 365) ===\n",
            "Gradient norms: [9.41237735748291, 8.134368896484375, 10.156892776489258, 0.04356863349676132]\n",
            "Target weights: [0.0045637222938239574, 0.0052807386964559555, 0.004229194950312376, 0.9859263896942139]\n",
            "Updated weights: [0.008031019940972328, 0.008230870589613914, 0.008116642944514751, 0.9756214618682861]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 365\n",
            "  Loss for client 0: 1.8648\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 365\n",
            "  Loss for client 1: 2.1252\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 365\n",
            "  Loss for client 2: 2.1536\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 365\n",
            "  Loss for client 3: 0.0007\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 366\n",
            "  Client 0: total grad norm = 90.316485 (253 params)\n",
            "  Client 1: total grad norm = 79.867089 (253 params)\n",
            "  Client 2: total grad norm = 139.907956 (253 params)\n",
            "  Client 3: total grad norm = 0.258871 (253 params)\n",
            "\n",
            "=== Optimizer Step 366 ===\n",
            "\n",
            "=== Updating Client Weights (Step 366) ===\n",
            "Gradient norms: [10.571715354919434, 9.032414436340332, 14.485363006591797, 0.02623121440410614]\n",
            "Target weights: [0.0024635365698486567, 0.002883371664211154, 0.0017979395342990756, 0.9928551912307739]\n",
            "Updated weights: [0.006360774859786034, 0.006626620888710022, 0.00622103177011013, 0.9807915687561035]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 366\n",
            "  Loss for client 0: 1.9426\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 366\n",
            "  Loss for client 1: 2.2023\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 366\n",
            "  Loss for client 2: 1.6762\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 366\n",
            "  Loss for client 3: 0.0005\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 367\n",
            "  Client 0: total grad norm = 107.909632 (253 params)\n",
            "  Client 1: total grad norm = 107.735598 (253 params)\n",
            "  Client 2: total grad norm = 85.861184 (253 params)\n",
            "  Client 3: total grad norm = 0.159401 (253 params)\n",
            "\n",
            "=== Optimizer Step 367 ===\n",
            "\n",
            "=== Updating Client Weights (Step 367) ===\n",
            "Gradient norms: [11.080423355102539, 9.988105773925781, 8.788156509399414, 0.017103124409914017]\n",
            "Target weights: [0.0015355570940300822, 0.0017034884076565504, 0.0019360855221748352, 0.9948248863220215]\n",
            "Updated weights: [0.004913209471851587, 0.005149681121110916, 0.004935548175126314, 0.9850015640258789]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 367\n",
            "  Loss for client 0: 1.2657\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 367\n",
            "  Loss for client 1: 2.2716\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 367\n",
            "  Loss for client 2: 2.1555\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 367\n",
            "  Loss for client 3: 0.0018\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 368\n",
            "  Client 0: total grad norm = 72.869683 (253 params)\n",
            "  Client 1: total grad norm = 87.571039 (253 params)\n",
            "  Client 2: total grad norm = 92.243285 (253 params)\n",
            "  Client 3: total grad norm = 1.469733 (253 params)\n",
            "\n",
            "=== Optimizer Step 368 ===\n",
            "\n",
            "=== Updating Client Weights (Step 368) ===\n",
            "Gradient norms: [7.239175319671631, 11.46530532836914, 10.354269981384277, 0.14064253866672516]\n",
            "Target weights: [0.018586425110697746, 0.01173543930053711, 0.012994677759706974, 0.9566834568977356]\n",
            "Updated weights: [0.009015174582600594, 0.007125408388674259, 0.007353287190198898, 0.9765061140060425]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 368\n",
            "  Loss for client 0: 1.5794\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 368\n",
            "  Loss for client 1: 1.3582\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 368\n",
            "  Loss for client 2: 2.0942\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 368\n",
            "  Loss for client 3: 0.0005\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 369\n",
            "  Client 0: total grad norm = 71.988288 (253 params)\n",
            "  Client 1: total grad norm = 82.821143 (253 params)\n",
            "  Client 2: total grad norm = 127.134753 (253 params)\n",
            "  Client 3: total grad norm = 0.145380 (253 params)\n",
            "\n",
            "=== Optimizer Step 369 ===\n",
            "\n",
            "=== Updating Client Weights (Step 369) ===\n",
            "Gradient norms: [8.251678466796875, 7.618241786956787, 13.669595718383789, 0.016535760834813118]\n",
            "Target weights: [0.0019931963179260492, 0.0021589254029095173, 0.001203196938149631, 0.9946446418762207]\n",
            "Updated weights: [0.006908581592142582, 0.005635463632643223, 0.005508260801434517, 0.9819477200508118]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 369\n",
            "  Loss for client 0: 1.2793\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 369\n",
            "  Loss for client 1: 1.5624\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 369\n",
            "  Loss for client 2: 2.4818\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 369\n",
            "  Loss for client 3: 0.2016\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 370\n",
            "  Client 0: total grad norm = 76.675312 (253 params)\n",
            "  Client 1: total grad norm = 68.424455 (253 params)\n",
            "  Client 2: total grad norm = 102.113328 (253 params)\n",
            "  Client 3: total grad norm = 6.823912 (253 params)\n",
            "\n",
            "=== Optimizer Step 370 ===\n",
            "\n",
            "=== Updating Client Weights (Step 370) ===\n",
            "Gradient norms: [7.687315940856934, 7.909756660461426, 11.203130722045898, 2.279660224914551]\n",
            "Target weights: [0.16583237051963806, 0.16116878390312195, 0.11379013955593109, 0.5592086911201477]\n",
            "Updated weights: [0.05458572134375572, 0.05229546129703522, 0.03799282759428024, 0.8551260232925415]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 370\n",
            "  Loss for client 0: 1.2511\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 370\n",
            "  Loss for client 1: 2.0080\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 370\n",
            "  Loss for client 2: 1.6494\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 370\n",
            "  Loss for client 3: 0.1952\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 371\n",
            "  Client 0: total grad norm = 83.860024 (253 params)\n",
            "  Client 1: total grad norm = 87.448453 (253 params)\n",
            "  Client 2: total grad norm = 82.953940 (253 params)\n",
            "  Client 3: total grad norm = 8.186650 (253 params)\n",
            "\n",
            "=== Optimizer Step 371 ===\n",
            "\n",
            "=== Updating Client Weights (Step 371) ===\n",
            "Gradient norms: [9.810879707336426, 10.218352317810059, 10.369271278381348, 2.206099510192871]\n",
            "Target weights: [0.13599088788032532, 0.13056804239749908, 0.1286676973104477, 0.6047734022140503]\n",
            "Updated weights: [0.07900726795196533, 0.07577723264694214, 0.06519529223442078, 0.7800202369689941]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 371\n",
            "  Loss for client 0: 1.8380\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 371\n",
            "  Loss for client 1: 2.1965\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 371\n",
            "  Loss for client 2: 2.0201\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 371\n",
            "  Loss for client 3: 0.0003\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 372\n",
            "  Client 0: total grad norm = 86.151924 (253 params)\n",
            "  Client 1: total grad norm = 88.959315 (253 params)\n",
            "  Client 2: total grad norm = 81.870138 (253 params)\n",
            "  Client 3: total grad norm = 0.084828 (253 params)\n",
            "\n",
            "=== Optimizer Step 372 ===\n",
            "\n",
            "=== Updating Client Weights (Step 372) ===\n",
            "Gradient norms: [8.938061714172363, 10.429939270019531, 8.393678665161133, 0.009524403139948845]\n",
            "Target weights: [0.0010622942354530096, 0.000910345814190805, 0.0011311906855553389, 0.9968961477279663]\n",
            "Updated weights: [0.055623773485422134, 0.05331716686487198, 0.04597606137394905, 0.8450829982757568]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 372\n",
            "  Loss for client 0: 1.1432\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 372\n",
            "  Loss for client 1: 1.6198\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 372\n",
            "  Loss for client 2: 1.7723\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 372\n",
            "  Loss for client 3: 0.2442\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 373\n",
            "  Client 0: total grad norm = 72.183442 (253 params)\n",
            "  Client 1: total grad norm = 75.576717 (253 params)\n",
            "  Client 2: total grad norm = 90.512614 (253 params)\n",
            "  Client 3: total grad norm = 6.947988 (253 params)\n",
            "\n",
            "=== Optimizer Step 373 ===\n",
            "\n",
            "=== Updating Client Weights (Step 373) ===\n",
            "Gradient norms: [7.545403003692627, 8.957968711853027, 9.416356086730957, 2.2849178314208984]\n",
            "Target weights: [0.1681835651397705, 0.14166299998760223, 0.1347668617963791, 0.5553866028785706]\n",
            "Updated weights: [0.08939170837402344, 0.079820916056633, 0.07261329889297485, 0.7581740617752075]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 373\n",
            "  Loss for client 0: 1.5354\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 373\n",
            "  Loss for client 1: 1.8601\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 373\n",
            "  Loss for client 2: 2.2890\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 373\n",
            "  Loss for client 3: 0.0008\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 374\n",
            "  Client 0: total grad norm = 111.595733 (253 params)\n",
            "  Client 1: total grad norm = 75.703616 (253 params)\n",
            "  Client 2: total grad norm = 89.513703 (253 params)\n",
            "  Client 3: total grad norm = 0.369846 (253 params)\n",
            "\n",
            "=== Optimizer Step 374 ===\n",
            "\n",
            "=== Updating Client Weights (Step 374) ===\n",
            "Gradient norms: [12.373291969299316, 8.088930130004883, 10.400069236755371, 0.04413808509707451]\n",
            "Target weights: [0.0035204975865781307, 0.005385155323892832, 0.0041884477250278, 0.9869058728218079]\n",
            "Updated weights: [0.0636303499341011, 0.05749019235372543, 0.05208584666252136, 0.8267936706542969]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 374\n",
            "  Loss for client 0: 1.7226\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 374\n",
            "  Loss for client 1: 1.5168\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 374\n",
            "  Loss for client 2: 2.2595\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 374\n",
            "  Loss for client 3: 0.0003\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 375\n",
            "  Client 0: total grad norm = 86.312666 (253 params)\n",
            "  Client 1: total grad norm = 83.463724 (253 params)\n",
            "  Client 2: total grad norm = 92.950859 (253 params)\n",
            "  Client 3: total grad norm = 0.116949 (253 params)\n",
            "\n",
            "=== Optimizer Step 375 ===\n",
            "\n",
            "=== Updating Client Weights (Step 375) ===\n",
            "Gradient norms: [9.997045516967773, 7.649458885192871, 9.790616035461426, 0.010784260928630829]\n",
            "Target weights: [0.0010748868808150291, 0.001404765178449452, 0.0010975502664223313, 0.9964227676391602]\n",
            "Updated weights: [0.044863708317279816, 0.040664564818143845, 0.03678935393691063, 0.8776823878288269]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 375\n",
            "  Loss for client 0: 1.1639\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 375\n",
            "  Loss for client 1: 2.1935\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 375\n",
            "  Loss for client 2: 2.0754\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 375\n",
            "  Loss for client 3: 0.0009\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 376\n",
            "  Client 0: total grad norm = 64.948766 (253 params)\n",
            "  Client 1: total grad norm = 77.042296 (253 params)\n",
            "  Client 2: total grad norm = 83.196045 (253 params)\n",
            "  Client 3: total grad norm = 0.451608 (253 params)\n",
            "\n",
            "=== Optimizer Step 376 ===\n",
            "\n",
            "=== Updating Client Weights (Step 376) ===\n",
            "Gradient norms: [7.176831245422363, 8.81436538696289, 9.38577651977539, 0.048575509339571]\n",
            "Target weights: [0.006652266252785921, 0.005416407249867916, 0.005086653400212526, 0.9828446507453918]\n",
            "Updated weights: [0.0334002748131752, 0.03009011596441269, 0.027278544381260872, 0.9092310667037964]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 376\n",
            "  Loss for client 0: 1.8352\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 376\n",
            "  Loss for client 1: 1.6440\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 376\n",
            "  Loss for client 2: 1.8300\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 376\n",
            "  Loss for client 3: 0.0003\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 377\n",
            "  Client 0: total grad norm = 82.452243 (253 params)\n",
            "  Client 1: total grad norm = 84.752610 (253 params)\n",
            "  Client 2: total grad norm = 83.040982 (253 params)\n",
            "  Client 3: total grad norm = 0.097396 (253 params)\n",
            "\n",
            "=== Optimizer Step 377 ===\n",
            "\n",
            "=== Updating Client Weights (Step 377) ===\n",
            "Gradient norms: [9.816665649414062, 9.77536392211914, 8.450736999511719, 0.009001780301332474]\n",
            "Target weights: [0.0009143362985923886, 0.0009181994246318936, 0.001062124385498464, 0.9971053600311279]\n",
            "Updated weights: [0.02365449257194996, 0.021338539198040962, 0.019413618370890617, 0.9355933666229248]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 377\n",
            "  Loss for client 0: 1.8122\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 377\n",
            "  Loss for client 1: 1.9775\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 377\n",
            "  Loss for client 2: 1.9391\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 377\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 378\n",
            "  Client 0: total grad norm = 79.326620 (253 params)\n",
            "  Client 1: total grad norm = 83.413822 (253 params)\n",
            "  Client 2: total grad norm = 76.738950 (253 params)\n",
            "  Client 3: total grad norm = 0.138525 (253 params)\n",
            "\n",
            "=== Optimizer Step 378 ===\n",
            "\n",
            "=== Updating Client Weights (Step 378) ===\n",
            "Gradient norms: [9.112139701843262, 9.060638427734375, 8.596652030944824, 0.014477216638624668]\n",
            "Target weights: [0.0015810836339369416, 0.0015900706639513373, 0.0016758914571255445, 0.99515300989151]\n",
            "Updated weights: [0.01703246682882309, 0.015413996763527393, 0.014092298224568367, 0.9534611701965332]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 378\n",
            "  Loss for client 0: 1.5181\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 378\n",
            "  Loss for client 1: 1.8410\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 378\n",
            "  Loss for client 2: 2.1334\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 378\n",
            "  Loss for client 3: 0.0005\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 379\n",
            "  Client 0: total grad norm = 77.429474 (253 params)\n",
            "  Client 1: total grad norm = 71.865600 (253 params)\n",
            "  Client 2: total grad norm = 95.680191 (253 params)\n",
            "  Client 3: total grad norm = 0.171270 (253 params)\n",
            "\n",
            "=== Optimizer Step 379 ===\n",
            "\n",
            "=== Updating Client Weights (Step 379) ===\n",
            "Gradient norms: [8.021968841552734, 8.812935829162598, 10.26379108428955, 0.014606459997594357]\n",
            "Target weights: [0.0018119278829544783, 0.0016493061557412148, 0.0014161657309159636, 0.9951226115226746]\n",
            "Updated weights: [0.012466304935514927, 0.011284589767456055, 0.010289458557963371, 0.9659596681594849]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 379\n",
            "  Loss for client 0: 1.9450\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 379\n",
            "  Loss for client 1: 2.1816\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 379\n",
            "  Loss for client 2: 1.8719\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 379\n",
            "  Loss for client 3: 0.0009\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 380\n",
            "  Client 0: total grad norm = 79.545221 (253 params)\n",
            "  Client 1: total grad norm = 94.372897 (253 params)\n",
            "  Client 2: total grad norm = 81.625173 (253 params)\n",
            "  Client 3: total grad norm = 0.447107 (253 params)\n",
            "\n",
            "=== Optimizer Step 380 ===\n",
            "\n",
            "=== Updating Client Weights (Step 380) ===\n",
            "Gradient norms: [8.337748527526855, 9.60216999053955, 8.288960456848145, 0.052414678037166595]\n",
            "Target weights: [0.006174861919134855, 0.005361751187592745, 0.006211206316947937, 0.9822522401809692]\n",
            "Updated weights: [0.01057887077331543, 0.009507737122476101, 0.009065981954336166, 0.9708473682403564]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 380\n",
            "  Loss for client 0: 2.0563\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 380\n",
            "  Loss for client 1: 1.8602\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 380\n",
            "  Loss for client 2: 1.5557\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 380\n",
            "  Loss for client 3: 0.0007\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 381\n",
            "  Client 0: total grad norm = 118.346635 (253 params)\n",
            "  Client 1: total grad norm = 73.529623 (253 params)\n",
            "  Client 2: total grad norm = 78.160554 (253 params)\n",
            "  Client 3: total grad norm = 0.409017 (253 params)\n",
            "\n",
            "=== Optimizer Step 381 ===\n",
            "\n",
            "=== Updating Client Weights (Step 381) ===\n",
            "Gradient norms: [15.517095565795898, 8.01383113861084, 8.641350746154785, 0.04703676700592041]\n",
            "Target weights: [0.002988421590998769, 0.00578644871711731, 0.005366247147321701, 0.9858588576316833]\n",
            "Updated weights: [0.00830173585563898, 0.008391350507736206, 0.007956061512231827, 0.9753508567810059]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 381\n",
            "  Loss for client 0: 1.4628\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 381\n",
            "  Loss for client 1: 1.5514\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 381\n",
            "  Loss for client 2: 1.8959\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 381\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 382\n",
            "  Client 0: total grad norm = 72.985188 (253 params)\n",
            "  Client 1: total grad norm = 87.836643 (253 params)\n",
            "  Client 2: total grad norm = 97.976695 (253 params)\n",
            "  Client 3: total grad norm = 0.112018 (253 params)\n",
            "\n",
            "=== Optimizer Step 382 ===\n",
            "\n",
            "=== Updating Client Weights (Step 382) ===\n",
            "Gradient norms: [9.755924224853516, 8.81096363067627, 9.735660552978516, 0.01153175625950098]\n",
            "Target weights: [0.0011776987230405211, 0.0013040048070251942, 0.001180149964056909, 0.9963381886482239]\n",
            "Updated weights: [0.006164525169879198, 0.006265147123485804, 0.005923288408666849, 0.9816470742225647]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 382\n",
            "  Loss for client 0: 2.0828\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 382\n",
            "  Loss for client 1: 2.1798\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 382\n",
            "  Loss for client 2: 1.5090\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 382\n",
            "  Loss for client 3: 0.0008\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 383\n",
            "  Client 0: total grad norm = 83.974589 (253 params)\n",
            "  Client 1: total grad norm = 87.280589 (253 params)\n",
            "  Client 2: total grad norm = 73.846791 (253 params)\n",
            "  Client 3: total grad norm = 0.403433 (253 params)\n",
            "\n",
            "=== Optimizer Step 383 ===\n",
            "\n",
            "=== Updating Client Weights (Step 383) ===\n",
            "Gradient norms: [8.353291511535645, 9.943015098571777, 7.702073097229004, 0.036065150052309036]\n",
            "Target weights: [0.004263641312718391, 0.0035819553304463625, 0.004624136257916689, 0.9875302314758301]\n",
            "Updated weights: [0.005594260059297085, 0.005460189655423164, 0.005533542484045029, 0.9834120273590088]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 383\n",
            "  Loss for client 0: 1.5753\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 383\n",
            "  Loss for client 1: 2.0739\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 383\n",
            "  Loss for client 2: 1.6579\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 383\n",
            "  Loss for client 3: 0.0003\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 384\n",
            "  Client 0: total grad norm = 85.680907 (253 params)\n",
            "  Client 1: total grad norm = 81.018392 (253 params)\n",
            "  Client 2: total grad norm = 83.157646 (253 params)\n",
            "  Client 3: total grad norm = 0.102865 (253 params)\n",
            "\n",
            "=== Optimizer Step 384 ===\n",
            "\n",
            "=== Updating Client Weights (Step 384) ===\n",
            "Gradient norms: [9.568132400512695, 9.417685508728027, 9.411234855651855, 0.01091763749718666]\n",
            "Target weights: [0.0011371079599484801, 0.0011552731739357114, 0.0011560650309547782, 0.9965516328811646]\n",
            "Updated weights: [0.004257114138454199, 0.004168714862316847, 0.004220299422740936, 0.9873539209365845]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 384\n",
            "  Loss for client 0: 1.8723\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 384\n",
            "  Loss for client 1: 1.8513\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 384\n",
            "  Loss for client 2: 1.9734\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 384\n",
            "  Loss for client 3: 0.2275\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 385\n",
            "  Client 0: total grad norm = 77.504346 (253 params)\n",
            "  Client 1: total grad norm = 91.720458 (253 params)\n",
            "  Client 2: total grad norm = 93.932028 (253 params)\n",
            "  Client 3: total grad norm = 5.109514 (253 params)\n",
            "\n",
            "=== Optimizer Step 385 ===\n",
            "\n",
            "=== Updating Client Weights (Step 385) ===\n",
            "Gradient norms: [8.384530067443848, 10.81080150604248, 11.8595609664917, 2.1874771118164062]\n",
            "Target weights: [0.15834003686904907, 0.12280373275279999, 0.11194399744272232, 0.6069122552871704]\n",
            "Updated weights: [0.050481993705034256, 0.03975922241806984, 0.03653741255402565, 0.8732213973999023]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 385\n",
            "  Loss for client 0: 1.6995\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 385\n",
            "  Loss for client 1: 2.4523\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 385\n",
            "  Loss for client 2: 2.2585\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 385\n",
            "  Loss for client 3: 0.0008\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 386\n",
            "  Client 0: total grad norm = 71.481840 (253 params)\n",
            "  Client 1: total grad norm = 91.240000 (253 params)\n",
            "  Client 2: total grad norm = 95.563966 (253 params)\n",
            "  Client 3: total grad norm = 0.339710 (253 params)\n",
            "\n",
            "=== Optimizer Step 386 ===\n",
            "\n",
            "=== Updating Client Weights (Step 386) ===\n",
            "Gradient norms: [7.617856979370117, 11.395659446716309, 12.493707656860352, 0.04488534480333328]\n",
            "Target weights: [0.005814078263938427, 0.003886639140546322, 0.0035450500436127186, 0.9867542386054993]\n",
            "Updated weights: [0.03708161786198616, 0.028997447341680527, 0.026639703661203384, 0.9072812795639038]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 386\n",
            "  Loss for client 0: 1.9562\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 386\n",
            "  Loss for client 1: 1.5644\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 386\n",
            "  Loss for client 2: 2.1117\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 386\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 387\n",
            "  Client 0: total grad norm = 85.094803 (253 params)\n",
            "  Client 1: total grad norm = 77.074089 (253 params)\n",
            "  Client 2: total grad norm = 83.299842 (253 params)\n",
            "  Client 3: total grad norm = 0.100909 (253 params)\n",
            "\n",
            "=== Optimizer Step 387 ===\n",
            "\n",
            "=== Updating Client Weights (Step 387) ===\n",
            "Gradient norms: [9.109224319458008, 8.727519989013672, 9.119339942932129, 0.011608867906033993]\n",
            "Target weights: [0.0012694866163656116, 0.0013250084593892097, 0.0012680783402174711, 0.9961374402046204]\n",
            "Updated weights: [0.02633797749876976, 0.02069571614265442, 0.01902821473777294, 0.9339381456375122]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 387\n",
            "  Loss for client 0: 1.7677\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 387\n",
            "  Loss for client 1: 2.2657\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 387\n",
            "  Loss for client 2: 2.1524\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 387\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 388\n",
            "  Client 0: total grad norm = 112.271779 (253 params)\n",
            "  Client 1: total grad norm = 89.050689 (253 params)\n",
            "  Client 2: total grad norm = 88.434870 (253 params)\n",
            "  Client 3: total grad norm = 0.144955 (253 params)\n",
            "\n",
            "=== Optimizer Step 388 ===\n",
            "\n",
            "=== Updating Client Weights (Step 388) ===\n",
            "Gradient norms: [9.7846040725708, 11.547738075256348, 9.93745231628418, 0.016928421333432198]\n",
            "Target weights: [0.001721673528663814, 0.0014588047051802278, 0.0016951924189925194, 0.9951243996620178]\n",
            "Updated weights: [0.01895308680832386, 0.014924642629921436, 0.013828308321535587, 0.9522939920425415]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 388\n",
            "  Loss for client 0: 2.0656\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 388\n",
            "  Loss for client 1: 2.2303\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 388\n",
            "  Loss for client 2: 1.5201\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 388\n",
            "  Loss for client 3: 0.0006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 389\n",
            "  Client 0: total grad norm = 88.076948 (253 params)\n",
            "  Client 1: total grad norm = 90.168059 (253 params)\n",
            "  Client 2: total grad norm = 67.008659 (253 params)\n",
            "  Client 3: total grad norm = 0.379502 (253 params)\n",
            "\n",
            "=== Optimizer Step 389 ===\n",
            "\n",
            "=== Updating Client Weights (Step 389) ===\n",
            "Gradient norms: [10.798405647277832, 11.288617134094238, 7.959086894989014, 0.03329692780971527]\n",
            "Target weights: [0.0030523205641657114, 0.0029197728727012873, 0.004141203127801418, 0.9898867011070251]\n",
            "Updated weights: [0.014182856306433678, 0.011323181912302971, 0.010922176763415337, 0.9635717868804932]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 389\n",
            "  Loss for client 0: 1.3775\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 389\n",
            "  Loss for client 1: 1.2684\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 389\n",
            "  Loss for client 2: 1.6451\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 389\n",
            "  Loss for client 3: 0.0007\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 390\n",
            "  Client 0: total grad norm = 72.458402 (253 params)\n",
            "  Client 1: total grad norm = 69.660897 (253 params)\n",
            "  Client 2: total grad norm = 77.880157 (253 params)\n",
            "  Client 3: total grad norm = 0.265310 (253 params)\n",
            "\n",
            "=== Optimizer Step 390 ===\n",
            "\n",
            "=== Updating Client Weights (Step 390) ===\n",
            "Gradient norms: [8.735732078552246, 7.22983455657959, 8.067451477050781, 0.032031603157520294]\n",
            "Target weights: [0.0036230136174708605, 0.004377648700028658, 0.003923132084310055, 0.9880762100219727]\n",
            "Updated weights: [0.011014903895556927, 0.009239522740244865, 0.008822464384138584, 0.970923125743866]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 390\n",
            "  Loss for client 0: 1.1073\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 390\n",
            "  Loss for client 1: 1.2172\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 390\n",
            "  Loss for client 2: 2.0346\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 390\n",
            "  Loss for client 3: 0.0006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 391\n",
            "  Client 0: total grad norm = 66.340321 (253 params)\n",
            "  Client 1: total grad norm = 57.687257 (253 params)\n",
            "  Client 2: total grad norm = 91.824042 (253 params)\n",
            "  Client 3: total grad norm = 0.273843 (253 params)\n",
            "\n",
            "=== Optimizer Step 391 ===\n",
            "\n",
            "=== Updating Client Weights (Step 391) ===\n",
            "Gradient norms: [8.068700790405273, 6.253476142883301, 11.114645957946777, 0.03247181698679924]\n",
            "Target weights: [0.003976153675466776, 0.005130330100655556, 0.002886497415602207, 0.9880070686340332]\n",
            "Updated weights: [0.008903278037905693, 0.008006764575839043, 0.0070416731759905815, 0.9760482311248779]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 391\n",
            "  Loss for client 0: 1.8366\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 391\n",
            "  Loss for client 1: 1.5762\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 391\n",
            "  Loss for client 2: 1.8511\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 391\n",
            "  Loss for client 3: 0.2106\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 392\n",
            "  Client 0: total grad norm = 88.491842 (253 params)\n",
            "  Client 1: total grad norm = 70.553363 (253 params)\n",
            "  Client 2: total grad norm = 78.595074 (253 params)\n",
            "  Client 3: total grad norm = 5.343469 (253 params)\n",
            "\n",
            "=== Optimizer Step 392 ===\n",
            "\n",
            "=== Updating Client Weights (Step 392) ===\n",
            "Gradient norms: [10.363362312316895, 7.3331732749938965, 8.386106491088867, 2.252044916152954]\n",
            "Target weights: [0.12120109796524048, 0.17128339409828186, 0.1497776061296463, 0.5577379465103149]\n",
            "Updated weights: [0.04259262979030609, 0.05698975920677185, 0.04986245557665825, 0.8505551815032959]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 392\n",
            "  Loss for client 0: 1.7322\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 392\n",
            "  Loss for client 1: 1.8542\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 392\n",
            "  Loss for client 2: 1.9316\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 392\n",
            "  Loss for client 3: 0.0003\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 393\n",
            "  Client 0: total grad norm = 72.730681 (253 params)\n",
            "  Client 1: total grad norm = 79.791431 (253 params)\n",
            "  Client 2: total grad norm = 75.343740 (253 params)\n",
            "  Client 3: total grad norm = 0.076438 (253 params)\n",
            "\n",
            "=== Optimizer Step 393 ===\n",
            "\n",
            "=== Updating Client Weights (Step 393) ===\n",
            "Gradient norms: [8.078420639038086, 9.458268165588379, 9.390497207641602, 0.010058389976620674]\n",
            "Target weights: [0.0012409009505063295, 0.0010598684893921018, 0.0010675175581127405, 0.9966316819190979]\n",
            "Updated weights: [0.03018711321055889, 0.040210794657468796, 0.035223979502916336, 0.894378125667572]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 393\n",
            "  Loss for client 0: 2.4967\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 393\n",
            "  Loss for client 1: 1.2180\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 393\n",
            "  Loss for client 2: 1.9690\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 393\n",
            "  Loss for client 3: 0.0006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 394\n",
            "  Client 0: total grad norm = 91.760647 (253 params)\n",
            "  Client 1: total grad norm = 84.236286 (253 params)\n",
            "  Client 2: total grad norm = 73.272847 (253 params)\n",
            "  Client 3: total grad norm = 0.372157 (253 params)\n",
            "\n",
            "=== Optimizer Step 394 ===\n",
            "\n",
            "=== Updating Client Weights (Step 394) ===\n",
            "Gradient norms: [9.697609901428223, 9.025464057922363, 7.752504825592041, 0.0379633866250515]\n",
            "Target weights: [0.0038644105661660433, 0.00415220158174634, 0.004833991639316082, 0.9871494174003601]\n",
            "Updated weights: [0.0222903024405241, 0.029393216595053673, 0.026106983423233032, 0.9222095012664795]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 394\n",
            "  Loss for client 0: 2.2228\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 394\n",
            "  Loss for client 1: 1.6811\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 394\n",
            "  Loss for client 2: 2.3199\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 394\n",
            "  Loss for client 3: 0.2186\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 395\n",
            "  Client 0: total grad norm = 75.005005 (253 params)\n",
            "  Client 1: total grad norm = 80.043681 (253 params)\n",
            "  Client 2: total grad norm = 88.085923 (253 params)\n",
            "  Client 3: total grad norm = 5.226463 (253 params)\n",
            "\n",
            "=== Optimizer Step 395 ===\n",
            "\n",
            "=== Updating Client Weights (Step 395) ===\n",
            "Gradient norms: [8.549384117126465, 7.501956462860107, 10.211302757263184, 2.2645626068115234]\n",
            "Target weights: [0.14810077846050262, 0.1687786877155304, 0.12399695068597794, 0.5591235756874084]\n",
            "Updated weights: [0.060033444315195084, 0.07120886445045471, 0.05547397583723068, 0.8132836818695068]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 395\n",
            "  Loss for client 0: 1.3165\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 395\n",
            "  Loss for client 1: 2.2501\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 395\n",
            "  Loss for client 2: 2.1938\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 395\n",
            "  Loss for client 3: 0.0005\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 396\n",
            "  Client 0: total grad norm = 75.223432 (253 params)\n",
            "  Client 1: total grad norm = 97.650149 (253 params)\n",
            "  Client 2: total grad norm = 95.788066 (253 params)\n",
            "  Client 3: total grad norm = 0.173314 (253 params)\n",
            "\n",
            "=== Optimizer Step 396 ===\n",
            "\n",
            "=== Updating Client Weights (Step 396) ===\n",
            "Gradient norms: [9.009415626525879, 10.819634437561035, 9.840136528015137, 0.023449856787919998]\n",
            "Target weights: [0.0025843314360827208, 0.0021519504953175783, 0.0023661579471081495, 0.992897629737854]\n",
            "Updated weights: [0.042798712849617004, 0.05049179494380951, 0.03954163193702698, 0.8671678900718689]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 396\n",
            "  Loss for client 0: 2.0521\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 396\n",
            "  Loss for client 1: 1.8215\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 396\n",
            "  Loss for client 2: 2.2679\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 396\n",
            "  Loss for client 3: 0.0015\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 397\n",
            "  Client 0: total grad norm = 94.438515 (253 params)\n",
            "  Client 1: total grad norm = 74.908535 (253 params)\n",
            "  Client 2: total grad norm = 86.072439 (253 params)\n",
            "  Client 3: total grad norm = 0.789892 (253 params)\n",
            "\n",
            "=== Optimizer Step 397 ===\n",
            "\n",
            "=== Updating Client Weights (Step 397) ===\n",
            "Gradient norms: [9.176762580871582, 8.692633628845215, 9.08308219909668, 0.1178017184138298]\n",
            "Target weights: [0.012350850738584995, 0.01303872186690569, 0.012478234246373177, 0.9621322154998779]\n",
            "Updated weights: [0.03366435319185257, 0.03925587236881256, 0.03142261132597923, 0.8956571817398071]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 397\n",
            "  Loss for client 0: 2.0748\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 397\n",
            "  Loss for client 1: 1.2376\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 397\n",
            "  Loss for client 2: 2.2864\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 397\n",
            "  Loss for client 3: 0.0010\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 398\n",
            "  Client 0: total grad norm = 89.749629 (253 params)\n",
            "  Client 1: total grad norm = 60.713737 (253 params)\n",
            "  Client 2: total grad norm = 92.652753 (253 params)\n",
            "  Client 3: total grad norm = 1.288883 (253 params)\n",
            "\n",
            "=== Optimizer Step 398 ===\n",
            "\n",
            "=== Updating Client Weights (Step 398) ===\n",
            "Gradient norms: [8.896751403808594, 7.1300201416015625, 10.180102348327637, 0.07610629498958588]\n",
            "Target weights: [0.008331893011927605, 0.010396433994174004, 0.007281536236405373, 0.9739900827407837]\n",
            "Updated weights: [0.026064613834023476, 0.03059804067015648, 0.02418028935790062, 0.919157087802887]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 398\n",
            "  Loss for client 0: 1.5899\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 398\n",
            "  Loss for client 1: 1.9068\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 398\n",
            "  Loss for client 2: 2.2460\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 398\n",
            "  Loss for client 3: 0.0003\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 399\n",
            "  Client 0: total grad norm = 70.463459 (253 params)\n",
            "  Client 1: total grad norm = 91.760275 (253 params)\n",
            "  Client 2: total grad norm = 125.697412 (253 params)\n",
            "  Client 3: total grad norm = 0.073155 (253 params)\n",
            "\n",
            "=== Optimizer Step 399 ===\n",
            "\n",
            "=== Updating Client Weights (Step 399) ===\n",
            "Gradient norms: [8.420557975769043, 10.034539222717285, 10.667512893676758, 0.00906181801110506]\n",
            "Target weights: [0.001073119812645018, 0.0009005164611153305, 0.0008470828761346638, 0.9971792697906494]\n",
            "Updated weights: [0.018567165359854698, 0.021688781678676605, 0.01718032732605934, 0.9425637722015381]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 399\n",
            "  Loss for client 0: 1.4678\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 399\n",
            "  Loss for client 1: 1.8997\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 399\n",
            "  Loss for client 2: 1.8572\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 399\n",
            "  Loss for client 3: 0.0008\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 400\n",
            "  Client 0: total grad norm = 79.592620 (253 params)\n",
            "  Client 1: total grad norm = 116.154392 (253 params)\n",
            "  Client 2: total grad norm = 93.773628 (253 params)\n",
            "  Client 3: total grad norm = 0.315381 (253 params)\n",
            "\n",
            "=== Optimizer Step 400 ===\n",
            "\n",
            "=== Updating Client Weights (Step 400) ===\n",
            "Gradient norms: [8.054498672485352, 14.176159858703613, 8.763969421386719, 0.038298219442367554]\n",
            "Target weights: [0.004699310753494501, 0.002670017536729574, 0.004318886902183294, 0.9883118271827698]\n",
            "Updated weights: [0.014406808651983738, 0.015983151271939278, 0.013321895152330399, 0.95628821849823]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 400\n",
            "  Val Loss = 1.0231 (4 batches)\n",
            "–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ /content/drive/MyDrive/meritopt_checkpoints/step_400\n",
            "\n",
            "[Processing] Client 0, batch 400\n",
            "  Loss for client 0: 2.0944\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 400\n",
            "  Loss for client 1: 1.9319\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 400\n",
            "  Loss for client 2: 2.2673\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 400\n",
            "  Loss for client 3: 0.0013\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 401\n",
            "  Client 0: total grad norm = 96.395618 (253 params)\n",
            "  Client 1: total grad norm = 104.649093 (253 params)\n",
            "  Client 2: total grad norm = 118.508773 (253 params)\n",
            "  Client 3: total grad norm = 1.283432 (253 params)\n",
            "\n",
            "=== Optimizer Step 401 ===\n",
            "\n",
            "=== Updating Client Weights (Step 401) ===\n",
            "Gradient norms: [9.915736198425293, 10.339200973510742, 11.073664665222168, 0.08392230421304703]\n",
            "Target weights: [0.008263900876045227, 0.007925434969365597, 0.007399778347462416, 0.9764109253883362]\n",
            "Updated weights: [0.012563934549689293, 0.013565834611654282, 0.011545258574187756, 0.9623249769210815]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 401\n",
            "  Loss for client 0: 1.1798\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 401\n",
            "  Loss for client 1: 1.2360\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 401\n",
            "  Loss for client 2: 2.0877\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 401\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 402\n",
            "  Client 0: total grad norm = 58.724255 (253 params)\n",
            "  Client 1: total grad norm = 76.349476 (253 params)\n",
            "  Client 2: total grad norm = 84.394551 (253 params)\n",
            "  Client 3: total grad norm = 0.117366 (253 params)\n",
            "\n",
            "=== Optimizer Step 402 ===\n",
            "\n",
            "=== Updating Client Weights (Step 402) ===\n",
            "Gradient norms: [6.529614448547363, 9.096128463745117, 9.777626991271973, 0.013377232477068901]\n",
            "Target weights: [0.0020387389231473207, 0.0014634996186941862, 0.0013614939525723457, 0.9951363205909729]\n",
            "Updated weights: [0.009406374767422676, 0.009935133159160614, 0.008490128442645073, 0.9721683263778687]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 402\n",
            "  Loss for client 0: 1.6938\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 402\n",
            "  Loss for client 1: 1.3934\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 402\n",
            "  Loss for client 2: 1.7755\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 402\n",
            "  Loss for client 3: 0.1856\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 403\n",
            "  Client 0: total grad norm = 82.151518 (253 params)\n",
            "  Client 1: total grad norm = 68.729107 (253 params)\n",
            "  Client 2: total grad norm = 81.233620 (253 params)\n",
            "  Client 3: total grad norm = 5.717437 (253 params)\n",
            "\n",
            "=== Optimizer Step 403 ===\n",
            "\n",
            "=== Updating Client Weights (Step 403) ===\n",
            "Gradient norms: [8.64448356628418, 7.607625484466553, 9.067431449890137, 2.2278945446014404]\n",
            "Target weights: [0.1434769481420517, 0.1630317121744156, 0.13678450882434845, 0.5567068457603455]\n",
            "Updated weights: [0.04962754622101784, 0.055864106863737106, 0.04697844386100769, 0.8475298881530762]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 403\n",
            "  Loss for client 0: 1.9372\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 403\n",
            "  Loss for client 1: 2.0909\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 403\n",
            "  Loss for client 2: 1.8567\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 403\n",
            "  Loss for client 3: 0.2201\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 404\n",
            "  Client 0: total grad norm = 78.503858 (253 params)\n",
            "  Client 1: total grad norm = 79.820083 (253 params)\n",
            "  Client 2: total grad norm = 87.978803 (253 params)\n",
            "  Client 3: total grad norm = 7.880729 (253 params)\n",
            "\n",
            "=== Optimizer Step 404 ===\n",
            "\n",
            "=== Updating Client Weights (Step 404) ===\n",
            "Gradient norms: [8.38593578338623, 8.5668363571167, 9.542898178100586, 2.364330291748047]\n",
            "Target weights: [0.15614020824432373, 0.15284310281276703, 0.13721007108688354, 0.5538066029548645]\n",
            "Updated weights: [0.08158134669065475, 0.08495780825614929, 0.07404793798923492, 0.7594128847122192]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 404\n",
            "  Loss for client 0: 1.5749\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 404\n",
            "  Loss for client 1: 1.6240\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 404\n",
            "  Loss for client 2: 1.6623\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 404\n",
            "  Loss for client 3: 0.0006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 405\n",
            "  Client 0: total grad norm = 85.799807 (253 params)\n",
            "  Client 1: total grad norm = 86.820300 (253 params)\n",
            "  Client 2: total grad norm = 75.998537 (253 params)\n",
            "  Client 3: total grad norm = 0.296279 (253 params)\n",
            "\n",
            "=== Optimizer Step 405 ===\n",
            "\n",
            "=== Updating Client Weights (Step 405) ===\n",
            "Gradient norms: [10.147851943969727, 9.259592056274414, 7.891894817352295, 0.036022402346134186]\n",
            "Target weights: [0.00350764999166131, 0.003844133811071515, 0.004510337486863136, 0.9881378412246704]\n",
            "Updated weights: [0.0581592358648777, 0.06062370538711548, 0.053186655044555664, 0.8280304074287415]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 405\n",
            "  Loss for client 0: 2.0641\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 405\n",
            "  Loss for client 1: 1.5194\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 405\n",
            "  Loss for client 2: 1.8306\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 405\n",
            "  Loss for client 3: 0.0005\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 406\n",
            "  Client 0: total grad norm = 83.072604 (253 params)\n",
            "  Client 1: total grad norm = 72.419303 (253 params)\n",
            "  Client 2: total grad norm = 90.049941 (253 params)\n",
            "  Client 3: total grad norm = 0.173703 (253 params)\n",
            "\n",
            "=== Optimizer Step 406 ===\n",
            "\n",
            "=== Updating Client Weights (Step 406) ===\n",
            "Gradient norms: [8.384757041931152, 8.06208324432373, 9.719521522521973, 0.022462937980890274]\n",
            "Target weights: [0.0026583492290228605, 0.0027647463139146566, 0.0022932831197977066, 0.9922835826873779]\n",
            "Updated weights: [0.04150897637009621, 0.043266020715236664, 0.03791864961385727, 0.8773064017295837]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 406\n",
            "  Loss for client 0: 1.5805\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 406\n",
            "  Loss for client 1: 1.7150\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 406\n",
            "  Loss for client 2: 1.7002\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 406\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 407\n",
            "  Client 0: total grad norm = 79.203318 (253 params)\n",
            "  Client 1: total grad norm = 88.200295 (253 params)\n",
            "  Client 2: total grad norm = 73.251056 (253 params)\n",
            "  Client 3: total grad norm = 0.124520 (253 params)\n",
            "\n",
            "=== Optimizer Step 407 ===\n",
            "\n",
            "=== Updating Client Weights (Step 407) ===\n",
            "Gradient norms: [8.656367301940918, 9.895235061645508, 8.173287391662598, 0.014649850316345692]\n",
            "Target weights: [0.0016840180614963174, 0.0014731816481798887, 0.0017835514154285192, 0.9950591921806335]\n",
            "Updated weights: [0.029561487957835197, 0.030728168785572052, 0.027078120037913322, 0.9126322269439697]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 407\n",
            "  Loss for client 0: 1.8582\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 407\n",
            "  Loss for client 1: 1.8089\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 407\n",
            "  Loss for client 2: 1.7104\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 407\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 408\n",
            "  Client 0: total grad norm = 75.755467 (253 params)\n",
            "  Client 1: total grad norm = 80.003764 (253 params)\n",
            "  Client 2: total grad norm = 71.538253 (253 params)\n",
            "  Client 3: total grad norm = 0.140217 (253 params)\n",
            "\n",
            "=== Optimizer Step 408 ===\n",
            "\n",
            "=== Updating Client Weights (Step 408) ===\n",
            "Gradient norms: [7.458375930786133, 8.570759773254395, 8.366713523864746, 0.017165636643767357]\n",
            "Target weights: [0.002286989940330386, 0.001990165561437607, 0.0020387014374136925, 0.9936841726303101]\n",
            "Updated weights: [0.02137913927435875, 0.022106768563389778, 0.01956629380583763, 0.9369478225708008]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 408\n",
            "  Loss for client 0: 2.1088\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 408\n",
            "  Loss for client 1: 2.1616\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 408\n",
            "  Loss for client 2: 2.0620\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 408\n",
            "  Loss for client 3: 0.0005\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 409\n",
            "  Client 0: total grad norm = 89.483275 (253 params)\n",
            "  Client 1: total grad norm = 91.880356 (253 params)\n",
            "  Client 2: total grad norm = 74.409055 (253 params)\n",
            "  Client 3: total grad norm = 0.166358 (253 params)\n",
            "\n",
            "=== Optimizer Step 409 ===\n",
            "\n",
            "=== Updating Client Weights (Step 409) ===\n",
            "Gradient norms: [9.965598106384277, 8.985812187194824, 7.878157615661621, 0.01923951506614685]\n",
            "Target weights: [0.001918099820613861, 0.002127243671566248, 0.0024263302329927683, 0.9935283064842224]\n",
            "Updated weights: [0.015540827065706253, 0.016112910583615303, 0.014424304477870464, 0.9539219737052917]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 409\n",
            "  Loss for client 0: 2.4165\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 409\n",
            "  Loss for client 1: 1.9436\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 409\n",
            "  Loss for client 2: 1.7254\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 409\n",
            "  Loss for client 3: 0.0005\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 410\n",
            "  Client 0: total grad norm = 96.869678 (253 params)\n",
            "  Client 1: total grad norm = 102.352789 (253 params)\n",
            "  Client 2: total grad norm = 83.278923 (253 params)\n",
            "  Client 3: total grad norm = 0.207079 (253 params)\n",
            "\n",
            "=== Optimizer Step 410 ===\n",
            "\n",
            "=== Updating Client Weights (Step 410) ===\n",
            "Gradient norms: [10.80013656616211, 12.081507682800293, 8.539450645446777, 0.018587978556752205]\n",
            "Target weights: [0.0017117824172601104, 0.0015302299289032817, 0.002164950128644705, 0.99459308385849]\n",
            "Updated weights: [0.011392113752663136, 0.011738106608390808, 0.01074649766087532, 0.9661232829093933]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 410\n",
            "  Loss for client 0: 1.3571\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 410\n",
            "  Loss for client 1: 2.4621\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 410\n",
            "  Loss for client 2: 2.2974\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 410\n",
            "  Loss for client 3: 0.0003\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 411\n",
            "  Client 0: total grad norm = 62.839359 (253 params)\n",
            "  Client 1: total grad norm = 89.452031 (253 params)\n",
            "  Client 2: total grad norm = 91.864255 (253 params)\n",
            "  Client 3: total grad norm = 0.097660 (253 params)\n",
            "\n",
            "=== Optimizer Step 411 ===\n",
            "\n",
            "=== Updating Client Weights (Step 411) ===\n",
            "Gradient norms: [6.184369087219238, 8.398893356323242, 9.34029483795166, 0.01039820909500122]\n",
            "Target weights: [0.0016746178735047579, 0.0012330737663432956, 0.0011087931925430894, 0.9959834814071655]\n",
            "Updated weights: [0.008476865477859974, 0.008586597628891468, 0.0078551871702075, 0.9750813841819763]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 411\n",
            "  Loss for client 0: 1.8627\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 411\n",
            "  Loss for client 1: 1.8304\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 411\n",
            "  Loss for client 2: 2.0971\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 411\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 412\n",
            "  Client 0: total grad norm = 79.965492 (253 params)\n",
            "  Client 1: total grad norm = 80.873151 (253 params)\n",
            "  Client 2: total grad norm = 90.638201 (253 params)\n",
            "  Client 3: total grad norm = 0.101164 (253 params)\n",
            "\n",
            "=== Optimizer Step 412 ===\n",
            "\n",
            "=== Updating Client Weights (Step 412) ===\n",
            "Gradient norms: [8.58499526977539, 8.58203411102295, 9.05072021484375, 0.012020228430628777]\n",
            "Target weights: [0.001394387916661799, 0.0013948689447715878, 0.001322636497206986, 0.9958881139755249]\n",
            "Updated weights: [0.0063521224074065685, 0.0064290789887309074, 0.005895421840250492, 0.9813234210014343]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 412\n",
            "  Loss for client 0: 1.3456\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 412\n",
            "  Loss for client 1: 1.5527\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 412\n",
            "  Loss for client 2: 2.2821\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 412\n",
            "  Loss for client 3: 0.0043\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 413\n",
            "  Client 0: total grad norm = 84.970873 (253 params)\n",
            "  Client 1: total grad norm = 80.301459 (253 params)\n",
            "  Client 2: total grad norm = 103.812000 (253 params)\n",
            "  Client 3: total grad norm = 3.349792 (253 params)\n",
            "\n",
            "=== Optimizer Step 413 ===\n",
            "\n",
            "=== Updating Client Weights (Step 413) ===\n",
            "Gradient norms: [9.570005416870117, 7.746175289154053, 9.670190811157227, 0.39141011238098145]\n",
            "Target weights: [0.03613348305225372, 0.04464108124375343, 0.035759132355451584, 0.8834663033485413]\n",
            "Updated weights: [0.015286531299352646, 0.017892679199576378, 0.014854535460472107, 0.9519662857055664]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 413\n",
            "  Loss for client 0: 1.6621\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 413\n",
            "  Loss for client 1: 1.6930\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 413\n",
            "  Loss for client 2: 2.0154\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 413\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 414\n",
            "  Client 0: total grad norm = 69.275118 (253 params)\n",
            "  Client 1: total grad norm = 80.167582 (253 params)\n",
            "  Client 2: total grad norm = 84.361820 (253 params)\n",
            "  Client 3: total grad norm = 0.159527 (253 params)\n",
            "\n",
            "=== Optimizer Step 414 ===\n",
            "\n",
            "=== Updating Client Weights (Step 414) ===\n",
            "Gradient norms: [7.591169357299805, 8.677499771118164, 7.766412734985352, 0.020418912172317505]\n",
            "Target weights: [0.00266934628598392, 0.002335172612220049, 0.002609114395454526, 0.9923863410949707]\n",
            "Updated weights: [0.011501375585794449, 0.013225426897406578, 0.011180909350514412, 0.9640923142433167]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 414\n",
            "  Loss for client 0: 1.5423\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 414\n",
            "  Loss for client 1: 1.7772\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 414\n",
            "  Loss for client 2: 2.0886\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 414\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 415\n",
            "  Client 0: total grad norm = 74.042713 (253 params)\n",
            "  Client 1: total grad norm = 69.145106 (253 params)\n",
            "  Client 2: total grad norm = 107.888215 (253 params)\n",
            "  Client 3: total grad norm = 0.112344 (253 params)\n",
            "\n",
            "=== Optimizer Step 415 ===\n",
            "\n",
            "=== Updating Client Weights (Step 415) ===\n",
            "Gradient norms: [7.097940444946289, 7.643243789672852, 10.698823928833008, 0.013950783759355545]\n",
            "Target weights: [0.0019555077888071537, 0.0018159932224079967, 0.001297346199862659, 0.994931161403656]\n",
            "Updated weights: [0.00863761454820633, 0.009802596643567085, 0.008215839974582195, 0.9733439683914185]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 415\n",
            "  Loss for client 0: 0.9818\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 415\n",
            "  Loss for client 1: 1.7277\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 415\n",
            "  Loss for client 2: 1.8127\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 415\n",
            "  Loss for client 3: 0.0025\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 416\n",
            "  Client 0: total grad norm = 58.815375 (253 params)\n",
            "  Client 1: total grad norm = 75.087306 (253 params)\n",
            "  Client 2: total grad norm = 96.715247 (253 params)\n",
            "  Client 3: total grad norm = 1.454364 (253 params)\n",
            "\n",
            "=== Optimizer Step 416 ===\n",
            "\n",
            "=== Updating Client Weights (Step 416) ===\n",
            "Gradient norms: [6.766026496887207, 7.981829643249512, 11.064797401428223, 0.2221236228942871]\n",
            "Target weights: [0.030376851558685303, 0.02574980817735195, 0.01857518032193184, 0.9252981543540955]\n",
            "Updated weights: [0.015159385278820992, 0.014586759731173515, 0.011323641985654831, 0.958930253982544]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 416\n",
            "  Loss for client 0: 2.1597\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 416\n",
            "  Loss for client 1: 2.2177\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 416\n",
            "  Loss for client 2: 1.8932\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 416\n",
            "  Loss for client 3: 0.0003\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 417\n",
            "  Client 0: total grad norm = 79.184302 (253 params)\n",
            "  Client 1: total grad norm = 106.761308 (253 params)\n",
            "  Client 2: total grad norm = 77.593377 (253 params)\n",
            "  Client 3: total grad norm = 0.069766 (253 params)\n",
            "\n",
            "=== Optimizer Step 417 ===\n",
            "\n",
            "=== Updating Client Weights (Step 417) ===\n",
            "Gradient norms: [9.943093299865723, 9.794572830200195, 8.487253189086914, 0.007505511865019798]\n",
            "Target weights: [0.0007530363509431481, 0.000764455005992204, 0.0008822065428830683, 0.997600257396698]\n",
            "Updated weights: [0.010837480425834656, 0.010440068319439888, 0.008191211149096489, 0.9705312252044678]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 417\n",
            "  Loss for client 0: 1.7093\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 417\n",
            "  Loss for client 1: 2.2328\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 417\n",
            "  Loss for client 2: 2.0636\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 417\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 418\n",
            "  Client 0: total grad norm = 89.582880 (253 params)\n",
            "  Client 1: total grad norm = 124.771476 (253 params)\n",
            "  Client 2: total grad norm = 75.668630 (253 params)\n",
            "  Client 3: total grad norm = 0.095622 (253 params)\n",
            "\n",
            "=== Optimizer Step 418 ===\n",
            "\n",
            "=== Updating Client Weights (Step 418) ===\n",
            "Gradient norms: [10.658231735229492, 10.99039363861084, 8.55755615234375, 0.011046584695577621]\n",
            "Target weights: [0.0010329954093322158, 0.0010017752647399902, 0.0012865711469203234, 0.9966785907745361]\n",
            "Updated weights: [0.00789613462984562, 0.007608580403029919, 0.006119818892329931, 0.9783754348754883]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 418\n",
            "  Loss for client 0: 2.1773\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 418\n",
            "  Loss for client 1: 1.7390\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 418\n",
            "  Loss for client 2: 1.6838\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 418\n",
            "  Loss for client 3: 0.0002\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 419\n",
            "  Client 0: total grad norm = 94.023286 (253 params)\n",
            "  Client 1: total grad norm = 76.798823 (253 params)\n",
            "  Client 2: total grad norm = 77.303826 (253 params)\n",
            "  Client 3: total grad norm = 0.047568 (253 params)\n",
            "\n",
            "=== Optimizer Step 419 ===\n",
            "\n",
            "=== Updating Client Weights (Step 419) ===\n",
            "Gradient norms: [9.77653980255127, 7.424909591674805, 8.371406555175781, 0.006600807886570692]\n",
            "Target weights: [0.000673584348987788, 0.0008869231678545475, 0.000786644930485636, 0.9976527690887451]\n",
            "Updated weights: [0.005729369819164276, 0.0055920835584402084, 0.004519867245107889, 0.9841586947441101]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 419\n",
            "  Loss for client 0: 1.3884\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 419\n",
            "  Loss for client 1: 1.9143\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 419\n",
            "  Loss for client 2: 1.9581\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 419\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 420\n",
            "  Client 0: total grad norm = 84.968632 (253 params)\n",
            "  Client 1: total grad norm = 83.826784 (253 params)\n",
            "  Client 2: total grad norm = 83.300989 (253 params)\n",
            "  Client 3: total grad norm = 0.221227 (253 params)\n",
            "\n",
            "=== Optimizer Step 420 ===\n",
            "\n",
            "=== Updating Client Weights (Step 420) ===\n",
            "Gradient norms: [9.890070915222168, 10.712100982666016, 9.401238441467285, 0.016834810376167297]\n",
            "Target weights: [0.0016936167376115918, 0.001563651254400611, 0.0017816791078075767, 0.9949610233306885]\n",
            "Updated weights: [0.0045186434872448444, 0.004383553750813007, 0.0036984106991440058, 0.9873993992805481]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 420\n",
            "  Loss for client 0: 1.6271\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 420\n",
            "  Loss for client 1: 1.6400\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 420\n",
            "  Loss for client 2: 2.1378\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 420\n",
            "  Loss for client 3: 0.2369\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 421\n",
            "  Client 0: total grad norm = 76.215611 (253 params)\n",
            "  Client 1: total grad norm = 70.422575 (253 params)\n",
            "  Client 2: total grad norm = 98.972158 (253 params)\n",
            "  Client 3: total grad norm = 11.444037 (253 params)\n",
            "\n",
            "=== Optimizer Step 421 ===\n",
            "\n",
            "=== Updating Client Weights (Step 421) ===\n",
            "Gradient norms: [8.735495567321777, 7.882992744445801, 9.488691329956055, 2.4032578468322754]\n",
            "Target weights: [0.15006853640079498, 0.16629762947559357, 0.13815635442733765, 0.5454774498939514]\n",
            "Updated weights: [0.048183612525463104, 0.05295778065919876, 0.04403579607605934, 0.8548228144645691]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 421\n",
            "  Loss for client 0: 1.0417\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 421\n",
            "  Loss for client 1: 1.9586\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 421\n",
            "  Loss for client 2: 2.0641\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 421\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 422\n",
            "  Client 0: total grad norm = 60.167821 (253 params)\n",
            "  Client 1: total grad norm = 78.388247 (253 params)\n",
            "  Client 2: total grad norm = 81.306896 (253 params)\n",
            "  Client 3: total grad norm = 0.164023 (253 params)\n",
            "\n",
            "=== Optimizer Step 422 ===\n",
            "\n",
            "=== Updating Client Weights (Step 422) ===\n",
            "Gradient norms: [5.997011184692383, 9.227045059204102, 8.24986743927002, 0.018039636313915253]\n",
            "Target weights: [0.002986751263961196, 0.0019412044202908874, 0.0021711355075240135, 0.9929008483886719]\n",
            "Updated weights: [0.034624554216861725, 0.03765280544757843, 0.031476397067308426, 0.8962462544441223]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 422\n",
            "  Loss for client 0: 1.6284\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 422\n",
            "  Loss for client 1: 2.2821\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 422\n",
            "  Loss for client 2: 2.1122\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 422\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 423\n",
            "  Client 0: total grad norm = 79.691167 (253 params)\n",
            "  Client 1: total grad norm = 103.113592 (253 params)\n",
            "  Client 2: total grad norm = 87.200177 (253 params)\n",
            "  Client 3: total grad norm = 0.150041 (253 params)\n",
            "\n",
            "=== Optimizer Step 423 ===\n",
            "\n",
            "=== Updating Client Weights (Step 423) ===\n",
            "Gradient norms: [9.284832954406738, 11.575109481811523, 9.571232795715332, 0.014164065010845661]\n",
            "Target weights: [0.0015190825797617435, 0.001218513585627079, 0.0014736271696165204, 0.9957888722419739]\n",
            "Updated weights: [0.02469291165471077, 0.026722516864538193, 0.02247556485235691, 0.9261090755462646]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 423\n",
            "  Loss for client 0: 1.7716\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 423\n",
            "  Loss for client 1: 1.9935\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 423\n",
            "  Loss for client 2: 2.3857\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 423\n",
            "  Loss for client 3: 0.0022\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 424\n",
            "  Client 0: total grad norm = 83.671375 (253 params)\n",
            "  Client 1: total grad norm = 92.232533 (253 params)\n",
            "  Client 2: total grad norm = 98.048070 (253 params)\n",
            "  Client 3: total grad norm = 1.386034 (253 params)\n",
            "\n",
            "=== Optimizer Step 424 ===\n",
            "\n",
            "=== Updating Client Weights (Step 424) ===\n",
            "Gradient norms: [8.553258895874023, 9.98106575012207, 11.378543853759766, 0.19681933522224426]\n",
            "Target weights: [0.02170795388519764, 0.01860259845852852, 0.016317883506417274, 0.9433714747428894]\n",
            "Updated weights: [0.023797424510121346, 0.0242865402251482, 0.02062826044857502, 0.9312877655029297]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 424\n",
            "  Loss for client 0: 1.4283\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 424\n",
            "  Loss for client 1: 2.2713\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 424\n",
            "  Loss for client 2: 1.7145\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 424\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 425\n",
            "  Client 0: total grad norm = 85.424010 (253 params)\n",
            "  Client 1: total grad norm = 103.729239 (253 params)\n",
            "  Client 2: total grad norm = 80.681160 (253 params)\n",
            "  Client 3: total grad norm = 0.096337 (253 params)\n",
            "\n",
            "=== Optimizer Step 425 ===\n",
            "\n",
            "=== Updating Client Weights (Step 425) ===\n",
            "Gradient norms: [9.08999252319336, 10.2745943069458, 8.553681373596191, 0.011946444399654865]\n",
            "Target weights: [0.0013091714354231954, 0.0011582314036786556, 0.0013912557624280453, 0.9961414337158203]\n",
            "Updated weights: [0.01705094799399376, 0.017348047345876694, 0.014857158996164799, 0.9507438540458679]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 425\n",
            "  Loss for client 0: 1.4587\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 425\n",
            "  Loss for client 1: 1.7277\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 425\n",
            "  Loss for client 2: 2.1735\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 425\n",
            "  Loss for client 3: 0.0008\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 426\n",
            "  Client 0: total grad norm = 89.248894 (253 params)\n",
            "  Client 1: total grad norm = 83.870858 (253 params)\n",
            "  Client 2: total grad norm = 85.264900 (253 params)\n",
            "  Client 3: total grad norm = 0.306714 (253 params)\n",
            "\n",
            "=== Optimizer Step 426 ===\n",
            "\n",
            "=== Updating Client Weights (Step 426) ===\n",
            "Gradient norms: [8.961865425109863, 7.957637786865234, 9.820396423339844, 0.04026618227362633]\n",
            "Target weights: [0.00443253992125392, 0.004991911817342043, 0.004045032896101475, 0.9865304827690125]\n",
            "Updated weights: [0.013265426270663738, 0.013641207478940487, 0.011613521724939346, 0.9614798426628113]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 426\n",
            "  Loss for client 0: 2.2022\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 426\n",
            "  Loss for client 1: 1.3197\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 426\n",
            "  Loss for client 2: 1.8367\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 426\n",
            "  Loss for client 3: 0.0003\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 427\n",
            "  Client 0: total grad norm = 95.208232 (253 params)\n",
            "  Client 1: total grad norm = 66.699867 (253 params)\n",
            "  Client 2: total grad norm = 74.629234 (253 params)\n",
            "  Client 3: total grad norm = 0.091788 (253 params)\n",
            "\n",
            "=== Optimizer Step 427 ===\n",
            "\n",
            "=== Updating Client Weights (Step 427) ===\n",
            "Gradient norms: [9.680967330932617, 7.2369303703308105, 8.727179527282715, 0.010203647427260876]\n",
            "Target weights: [0.0010501760989427567, 0.0014048387529328465, 0.001164949149824679, 0.9963799715042114]\n",
            "Updated weights: [0.009600851684808731, 0.00997029710561037, 0.008478950709104538, 0.9719499349594116]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 427\n",
            "  Loss for client 0: 1.3806\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 427\n",
            "  Loss for client 1: 1.6807\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 427\n",
            "  Loss for client 2: 1.5080\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 427\n",
            "  Loss for client 3: 0.0002\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 428\n",
            "  Client 0: total grad norm = 89.385094 (253 params)\n",
            "  Client 1: total grad norm = 84.456683 (253 params)\n",
            "  Client 2: total grad norm = 65.069531 (253 params)\n",
            "  Client 3: total grad norm = 0.059070 (253 params)\n",
            "\n",
            "=== Optimizer Step 428 ===\n",
            "\n",
            "=== Updating Client Weights (Step 428) ===\n",
            "Gradient norms: [10.104267120361328, 8.465728759765625, 6.978662490844727, 0.007317895069718361]\n",
            "Target weights: [0.0007223340799100697, 0.0008621415472589433, 0.0010458531323820353, 0.9973697066307068]\n",
            "Updated weights: [0.006937296129763126, 0.007237850688397884, 0.006249021273106337, 0.9795758724212646]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 428\n",
            "  Loss for client 0: 2.2129\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 428\n",
            "  Loss for client 1: 2.0628\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 428\n",
            "  Loss for client 2: 1.7898\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 428\n",
            "  Loss for client 3: 0.0007\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 429\n",
            "  Client 0: total grad norm = 110.728712 (253 params)\n",
            "  Client 1: total grad norm = 84.558665 (253 params)\n",
            "  Client 2: total grad norm = 80.169219 (253 params)\n",
            "  Client 3: total grad norm = 0.314321 (253 params)\n",
            "\n",
            "=== Optimizer Step 429 ===\n",
            "\n",
            "=== Updating Client Weights (Step 429) ===\n",
            "Gradient norms: [12.176712036132812, 8.325146675109863, 9.1428861618042, 0.03502209857106209]\n",
            "Target weights: [0.0028451047837734222, 0.0041613709181547165, 0.0037891778629273176, 0.9892043471336365]\n",
            "Updated weights: [0.005709638819098473, 0.006314906757324934, 0.005511068273335695, 0.9824644327163696]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 429\n",
            "  Loss for client 0: 1.5344\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 429\n",
            "  Loss for client 1: 1.8381\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 429\n",
            "  Loss for client 2: 2.0761\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 429\n",
            "  Loss for client 3: 0.2094\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 430\n",
            "  Client 0: total grad norm = 84.907004 (253 params)\n",
            "  Client 1: total grad norm = 67.621255 (253 params)\n",
            "  Client 2: total grad norm = 87.685433 (253 params)\n",
            "  Client 3: total grad norm = 19.876305 (253 params)\n",
            "\n",
            "=== Optimizer Step 430 ===\n",
            "\n",
            "=== Updating Client Weights (Step 430) ===\n",
            "Gradient norms: [8.775351524353027, 6.563321113586426, 9.499343872070312, 3.297309398651123]\n",
            "Target weights: [0.1688566952943802, 0.22576631605625153, 0.15598729252815247, 0.4493897259235382]\n",
            "Updated weights: [0.05465376004576683, 0.0721503347158432, 0.050653934478759766, 0.8225420117378235]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 430\n",
            "  Loss for client 0: 1.7669\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 430\n",
            "  Loss for client 1: 2.3089\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 430\n",
            "  Loss for client 2: 1.9620\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 430\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 431\n",
            "  Client 0: total grad norm = 77.309426 (253 params)\n",
            "  Client 1: total grad norm = 93.718582 (253 params)\n",
            "  Client 2: total grad norm = 102.881150 (253 params)\n",
            "  Client 3: total grad norm = 0.106782 (253 params)\n",
            "\n",
            "=== Optimizer Step 431 ===\n",
            "\n",
            "=== Updating Client Weights (Step 431) ===\n",
            "Gradient norms: [8.687057495117188, 12.021574020385742, 11.579134941101074, 0.011112945154309273]\n",
            "Target weights: [0.0012752203037962317, 0.0009215025929734111, 0.0009567132219672203, 0.9968465566635132]\n",
            "Updated weights: [0.03864019736647606, 0.05078168213367462, 0.035744767636060715, 0.874833345413208]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 431\n",
            "  Loss for client 0: 1.6150\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 431\n",
            "  Loss for client 1: 2.0080\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 431\n",
            "  Loss for client 2: 1.7862\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 431\n",
            "  Loss for client 3: 0.0003\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 432\n",
            "  Client 0: total grad norm = 77.098419 (253 params)\n",
            "  Client 1: total grad norm = 84.847024 (253 params)\n",
            "  Client 2: total grad norm = 78.107260 (253 params)\n",
            "  Client 3: total grad norm = 0.095557 (253 params)\n",
            "\n",
            "=== Optimizer Step 432 ===\n",
            "\n",
            "=== Updating Client Weights (Step 432) ===\n",
            "Gradient norms: [8.579360961914062, 8.475046157836914, 7.7953267097473145, 0.01142048928886652]\n",
            "Target weights: [0.0013256665552034974, 0.0013419834431260824, 0.0014589986531063914, 0.9958733916282654]\n",
            "Updated weights: [0.02744583785533905, 0.03594977408647537, 0.025459036231040955, 0.9111453294754028]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 432\n",
            "  Loss for client 0: 1.2309\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 432\n",
            "  Loss for client 1: 1.4682\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 432\n",
            "  Loss for client 2: 1.7028\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 432\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 433\n",
            "  Client 0: total grad norm = 73.781010 (253 params)\n",
            "  Client 1: total grad norm = 64.186934 (253 params)\n",
            "  Client 2: total grad norm = 73.560312 (253 params)\n",
            "  Client 3: total grad norm = 0.108581 (253 params)\n",
            "\n",
            "=== Optimizer Step 433 ===\n",
            "\n",
            "=== Updating Client Weights (Step 433) ===\n",
            "Gradient norms: [7.681639671325684, 6.947965145111084, 7.576189994812012, 0.01513180136680603]\n",
            "Target weights: [0.001957836328074336, 0.002164575271308422, 0.0019850865937769413, 0.9938924312591553]\n",
            "Updated weights: [0.01979943737387657, 0.02581421285867691, 0.01841685175895691, 0.9359694719314575]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 433\n",
            "  Loss for client 0: 1.8604\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 433\n",
            "  Loss for client 1: 1.3554\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 433\n",
            "  Loss for client 2: 2.3009\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 433\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 434\n",
            "  Client 0: total grad norm = 103.135856 (253 params)\n",
            "  Client 1: total grad norm = 65.216805 (253 params)\n",
            "  Client 2: total grad norm = 93.189289 (253 params)\n",
            "  Client 3: total grad norm = 0.114264 (253 params)\n",
            "\n",
            "=== Optimizer Step 434 ===\n",
            "\n",
            "=== Updating Client Weights (Step 434) ===\n",
            "Gradient norms: [11.684222221374512, 7.485807418823242, 10.166569709777832, 0.01319598127156496]\n",
            "Target weights: [0.001124673057347536, 0.0017554459627717733, 0.001292562810704112, 0.9958273768424988]\n",
            "Updated weights: [0.014197009615600109, 0.018596583977341652, 0.013279566541314125, 0.9539268612861633]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 434\n",
            "  Loss for client 0: 2.0180\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 434\n",
            "  Loss for client 1: 1.9860\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 434\n",
            "  Loss for client 2: 1.8573\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 434\n",
            "  Loss for client 3: 0.1961\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 435\n",
            "  Client 0: total grad norm = 82.694498 (253 params)\n",
            "  Client 1: total grad norm = 80.167194 (253 params)\n",
            "  Client 2: total grad norm = 88.676565 (253 params)\n",
            "  Client 3: total grad norm = 7.852190 (253 params)\n",
            "\n",
            "=== Optimizer Step 435 ===\n",
            "\n",
            "=== Updating Client Weights (Step 435) ===\n",
            "Gradient norms: [8.854379653930664, 8.54883861541748, 9.181160926818848, 2.2550904750823975]\n",
            "Target weights: [0.1443721204996109, 0.14953207969665527, 0.1392335593700409, 0.5668622255325317]\n",
            "Updated weights: [0.0532495453953743, 0.05787723511457443, 0.05106576532125473, 0.8378074765205383]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 435\n",
            "  Loss for client 0: 1.8123\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 435\n",
            "  Loss for client 1: 1.2604\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 435\n",
            "  Loss for client 2: 1.9243\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 435\n",
            "  Loss for client 3: 0.2125\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 436\n",
            "  Client 0: total grad norm = 81.132444 (253 params)\n",
            "  Client 1: total grad norm = 61.902528 (253 params)\n",
            "  Client 2: total grad norm = 88.005646 (253 params)\n",
            "  Client 3: total grad norm = 7.605882 (253 params)\n",
            "\n",
            "=== Optimizer Step 436 ===\n",
            "\n",
            "=== Updating Client Weights (Step 436) ===\n",
            "Gradient norms: [9.226471900939941, 6.61105489730835, 8.65810775756836, 2.3528473377227783]\n",
            "Target weights: [0.13545244932174683, 0.18903915584087372, 0.1443442702293396, 0.5311641693115234]\n",
            "Updated weights: [0.077910415828228, 0.09722581505775452, 0.07904931902885437, 0.7458145022392273]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 436\n",
            "  Loss for client 0: 1.6137\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 436\n",
            "  Loss for client 1: 1.8364\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 436\n",
            "  Loss for client 2: 1.8662\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 436\n",
            "  Loss for client 3: 0.0438\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 437\n",
            "  Client 0: total grad norm = 75.857952 (253 params)\n",
            "  Client 1: total grad norm = 65.502264 (253 params)\n",
            "  Client 2: total grad norm = 84.139711 (253 params)\n",
            "  Client 3: total grad norm = 23.679198 (253 params)\n",
            "\n",
            "=== Optimizer Step 437 ===\n",
            "\n",
            "=== Updating Client Weights (Step 437) ===\n",
            "Gradient norms: [8.307674407958984, 7.973005771636963, 8.410544395446777, 2.921548366546631]\n",
            "Target weights: [0.17026114463806152, 0.1774078905582428, 0.16817867755889893, 0.48415225744247437]\n",
            "Updated weights: [0.10561563074588776, 0.12128043174743652, 0.10578812658786774, 0.6673158407211304]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 437\n",
            "  Loss for client 0: 1.2017\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 437\n",
            "  Loss for client 1: 1.5869\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 437\n",
            "  Loss for client 2: 2.5246\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 437\n",
            "  Loss for client 3: 0.2259\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 438\n",
            "  Client 0: total grad norm = 76.015800 (253 params)\n",
            "  Client 1: total grad norm = 67.458245 (253 params)\n",
            "  Client 2: total grad norm = 86.831249 (253 params)\n",
            "  Client 3: total grad norm = 6.744960 (253 params)\n",
            "\n",
            "=== Optimizer Step 438 ===\n",
            "\n",
            "=== Updating Client Weights (Step 438) ===\n",
            "Gradient norms: [8.857892990112305, 7.456235885620117, 9.714871406555176, 2.285339593887329]\n",
            "Target weights: [0.14335405826568604, 0.1703024059534073, 0.13070836663246155, 0.5556350946426392]\n",
            "Updated weights: [0.11693716049194336, 0.1359870284795761, 0.11326419562101364, 0.6338115930557251]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 438\n",
            "  Loss for client 0: 1.8664\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 438\n",
            "  Loss for client 1: 1.8973\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 438\n",
            "  Loss for client 2: 1.6091\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 438\n",
            "  Loss for client 3: 0.0003\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 439\n",
            "  Client 0: total grad norm = 86.760499 (253 params)\n",
            "  Client 1: total grad norm = 77.677814 (253 params)\n",
            "  Client 2: total grad norm = 81.931136 (253 params)\n",
            "  Client 3: total grad norm = 0.092937 (253 params)\n",
            "\n",
            "=== Optimizer Step 439 ===\n",
            "\n",
            "=== Updating Client Weights (Step 439) ===\n",
            "Gradient norms: [8.939706802368164, 9.80232048034668, 9.488072395324707, 0.011358402669429779]\n",
            "Target weights: [0.0012659667991101742, 0.0011545604793354869, 0.0011928000021725893, 0.996386706829071]\n",
            "Updated weights: [0.08223580569028854, 0.09553728997707367, 0.07964277267456055, 0.7425841093063354]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 439\n",
            "  Loss for client 0: 1.7211\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 439\n",
            "  Loss for client 1: 1.3507\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 439\n",
            "  Loss for client 2: 1.9531\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 439\n",
            "  Loss for client 3: 0.0012\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 440\n",
            "  Client 0: total grad norm = 68.637359 (253 params)\n",
            "  Client 1: total grad norm = 75.385163 (253 params)\n",
            "  Client 2: total grad norm = 75.200991 (253 params)\n",
            "  Client 3: total grad norm = 1.041359 (253 params)\n",
            "\n",
            "=== Optimizer Step 440 ===\n",
            "\n",
            "=== Updating Client Weights (Step 440) ===\n",
            "Gradient norms: [7.046358108520508, 8.610282897949219, 8.145634651184082, 0.09177429974079132]\n",
            "Target weights: [0.012584534473717213, 0.010298747569322586, 0.01088621560484171, 0.9662304520606995]\n",
            "Updated weights: [0.06134042516350746, 0.06996572017669678, 0.05901580676436424, 0.8096780776977539]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 440\n",
            "  Loss for client 0: 1.6885\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 440\n",
            "  Loss for client 1: 2.2232\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 440\n",
            "  Loss for client 2: 2.1218\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 440\n",
            "  Loss for client 3: 0.0007\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 441\n",
            "  Client 0: total grad norm = 88.638827 (253 params)\n",
            "  Client 1: total grad norm = 88.942255 (253 params)\n",
            "  Client 2: total grad norm = 84.344174 (253 params)\n",
            "  Client 3: total grad norm = 0.439568 (253 params)\n",
            "\n",
            "=== Optimizer Step 441 ===\n",
            "\n",
            "=== Updating Client Weights (Step 441) ===\n",
            "Gradient norms: [8.735274314880371, 9.791474342346191, 9.296663284301758, 0.04800037294626236]\n",
            "Target weights: [0.005410811398178339, 0.004827151075005531, 0.005084074102342129, 0.9846779704093933]\n",
            "Updated weights: [0.044561535120010376, 0.05042414367198944, 0.04283628240227699, 0.8621779680252075]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 441\n",
            "  Loss for client 0: 1.4318\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 441\n",
            "  Loss for client 1: 2.0598\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 441\n",
            "  Loss for client 2: 1.6939\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 441\n",
            "  Loss for client 3: 0.0012\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 442\n",
            "  Client 0: total grad norm = 76.217719 (253 params)\n",
            "  Client 1: total grad norm = 82.105944 (253 params)\n",
            "  Client 2: total grad norm = 81.864170 (253 params)\n",
            "  Client 3: total grad norm = 0.799471 (253 params)\n",
            "\n",
            "=== Optimizer Step 442 ===\n",
            "\n",
            "=== Updating Client Weights (Step 442) ===\n",
            "Gradient norms: [7.972548484802246, 8.492284774780273, 8.596271514892578, 0.0717151090502739]\n",
            "Target weights: [0.00876916479319334, 0.008232483640313148, 0.008132897317409515, 0.9748654961585999]\n",
            "Updated weights: [0.03382382169365883, 0.03776664286851883, 0.03242526575922966, 0.8959842324256897]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 442\n",
            "  Loss for client 0: 1.4562\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 442\n",
            "  Loss for client 1: 1.7013\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 442\n",
            "  Loss for client 2: 1.7705\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 442\n",
            "  Loss for client 3: 0.0006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 443\n",
            "  Client 0: total grad norm = 77.273955 (253 params)\n",
            "  Client 1: total grad norm = 87.413480 (253 params)\n",
            "  Client 2: total grad norm = 81.589229 (253 params)\n",
            "  Client 3: total grad norm = 0.405034 (253 params)\n",
            "\n",
            "=== Optimizer Step 443 ===\n",
            "\n",
            "=== Updating Client Weights (Step 443) ===\n",
            "Gradient norms: [8.506423950195312, 10.458697319030762, 8.555534362792969, 0.03832494094967842]\n",
            "Target weights: [0.004449133761227131, 0.003618636168539524, 0.004423595033586025, 0.9875085949897766]\n",
            "Updated weights: [0.025011414662003517, 0.027522239834070206, 0.024024764075875282, 0.9234415292739868]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 443\n",
            "  Loss for client 0: 1.7472\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 443\n",
            "  Loss for client 1: 2.1471\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 443\n",
            "  Loss for client 2: 2.0247\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 443\n",
            "  Loss for client 3: 0.0006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 444\n",
            "  Client 0: total grad norm = 76.583448 (253 params)\n",
            "  Client 1: total grad norm = 111.624497 (253 params)\n",
            "  Client 2: total grad norm = 81.673428 (253 params)\n",
            "  Client 3: total grad norm = 0.289894 (253 params)\n",
            "\n",
            "=== Optimizer Step 444 ===\n",
            "\n",
            "=== Updating Client Weights (Step 444) ===\n",
            "Gradient norms: [8.675114631652832, 12.768362998962402, 9.263720512390137, 0.029942242428660393]\n",
            "Target weights: [0.0034206267446279526, 0.0023240509908646345, 0.003203284228220582, 0.991051971912384]\n",
            "Updated weights: [0.018534181639552116, 0.01996278576552868, 0.017778322100639343, 0.9437246918678284]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 444\n",
            "  Loss for client 0: 1.9713\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 444\n",
            "  Loss for client 1: 1.9575\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 444\n",
            "  Loss for client 2: 1.3967\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 444\n",
            "  Loss for client 3: 0.0097\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 445\n",
            "  Client 0: total grad norm = 76.881512 (253 params)\n",
            "  Client 1: total grad norm = 78.190933 (253 params)\n",
            "  Client 2: total grad norm = 118.153464 (253 params)\n",
            "  Client 3: total grad norm = 7.562828 (253 params)\n",
            "\n",
            "=== Optimizer Step 445 ===\n",
            "\n",
            "=== Updating Client Weights (Step 445) ===\n",
            "Gradient norms: [9.324396133422852, 8.604918479919434, 13.107732772827148, 0.868211030960083]\n",
            "Target weights: [0.07388383895158768, 0.08006144315004349, 0.05255845561623573, 0.7934962511062622]\n",
            "Updated weights: [0.03513908386230469, 0.037992388010025024, 0.02821236290037632, 0.8986561894416809]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 445\n",
            "  Loss for client 0: 1.6998\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 445\n",
            "  Loss for client 1: 1.5830\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 445\n",
            "  Loss for client 2: 1.6356\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 445\n",
            "  Loss for client 3: 0.0004\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 446\n",
            "  Client 0: total grad norm = 85.676131 (253 params)\n",
            "  Client 1: total grad norm = 79.339665 (253 params)\n",
            "  Client 2: total grad norm = 75.866967 (253 params)\n",
            "  Client 3: total grad norm = 0.165897 (253 params)\n",
            "\n",
            "=== Optimizer Step 446 ===\n",
            "\n",
            "=== Updating Client Weights (Step 446) ===\n",
            "Gradient norms: [7.741964340209961, 9.046631813049316, 8.034761428833008, 0.014926662668585777]\n",
            "Target weights: [0.0019175978377461433, 0.0016410498647019267, 0.001847718027420342, 0.994593620300293]\n",
            "Updated weights: [0.025172637775540352, 0.027086986228823662, 0.020302969962358475, 0.927437424659729]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 446\n",
            "  Loss for client 0: 1.1998\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 446\n",
            "  Loss for client 1: 1.8766\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 446\n",
            "  Loss for client 2: 1.9684\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 446\n",
            "  Loss for client 3: 0.0012\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 447\n",
            "  Client 0: total grad norm = 80.678098 (253 params)\n",
            "  Client 1: total grad norm = 75.473848 (253 params)\n",
            "  Client 2: total grad norm = 85.754707 (253 params)\n",
            "  Client 3: total grad norm = 0.978273 (253 params)\n",
            "\n",
            "=== Optimizer Step 447 ===\n",
            "\n",
            "=== Updating Client Weights (Step 447) ===\n",
            "Gradient norms: [7.688599109649658, 8.900052070617676, 8.966180801391602, 0.08106215298175812]\n",
            "Target weights: [0.010249095968902111, 0.008854013867676258, 0.00878871325403452, 0.9721081852912903]\n",
            "Updated weights: [0.020695574581623077, 0.021617094054818153, 0.016848692670464516, 0.940838634967804]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 447\n",
            "  Loss for client 0: 1.6755\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 447\n",
            "  Loss for client 1: 2.4064\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 447\n",
            "  Loss for client 2: 2.3282\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 447\n",
            "  Loss for client 3: 0.0009\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 448\n",
            "  Client 0: total grad norm = 104.284144 (253 params)\n",
            "  Client 1: total grad norm = 91.956492 (253 params)\n",
            "  Client 2: total grad norm = 87.733474 (253 params)\n",
            "  Client 3: total grad norm = 0.564746 (253 params)\n",
            "\n",
            "=== Optimizer Step 448 ===\n",
            "\n",
            "=== Updating Client Weights (Step 448) ===\n",
            "Gradient norms: [10.877903938293457, 12.313755989074707, 9.950507164001465, 0.055369239300489426]\n",
            "Target weights: [0.005014096852391958, 0.0044294255785644054, 0.005481415428221226, 0.9850749969482422]\n",
            "Updated weights: [0.015991130843758583, 0.01646079309284687, 0.013438509777188301, 0.9541095495223999]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 448\n",
            "  Loss for client 0: 2.2027\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 448\n",
            "  Loss for client 1: 1.4983\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 448\n",
            "  Loss for client 2: 1.8802\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 448\n",
            "  Loss for client 3: 0.0006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 449\n",
            "  Client 0: total grad norm = 74.909491 (253 params)\n",
            "  Client 1: total grad norm = 69.712588 (253 params)\n",
            "  Client 2: total grad norm = 81.053318 (253 params)\n",
            "  Client 3: total grad norm = 0.370896 (253 params)\n",
            "\n",
            "=== Optimizer Step 449 ===\n",
            "\n",
            "=== Updating Client Weights (Step 449) ===\n",
            "Gradient norms: [7.732054710388184, 6.616824626922607, 9.92767333984375, 0.037196170538663864]\n",
            "Target weights: [0.004743390250951052, 0.00554286316037178, 0.003694335464388132, 0.9860193729400635]\n",
            "Updated weights: [0.012616809457540512, 0.013185414485633373, 0.010515258647501469, 0.9636825323104858]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 449\n",
            "  Loss for client 0: 2.1796\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 449\n",
            "  Loss for client 1: 1.4613\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 449\n",
            "  Loss for client 2: 1.8832\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 449\n",
            "  Loss for client 3: 0.0006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 450\n",
            "  Client 0: total grad norm = 97.864458 (253 params)\n",
            "  Client 1: total grad norm = 81.573286 (253 params)\n",
            "  Client 2: total grad norm = 79.289011 (253 params)\n",
            "  Client 3: total grad norm = 0.244810 (253 params)\n",
            "\n",
            "=== Optimizer Step 450 ===\n",
            "\n",
            "=== Updating Client Weights (Step 450) ===\n",
            "Gradient norms: [10.20834732055664, 9.436240196228027, 8.037317276000977, 0.027840174734592438]\n",
            "Target weights: [0.0027024932205677032, 0.0029236210975795984, 0.0034324871376156807, 0.9909414052963257]\n",
            "Updated weights: [0.009642514400184155, 0.010106876492500305, 0.008390426635742188, 0.9718601703643799]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 450\n",
            "  Val Loss = 1.0132 (4 batches)\n",
            "\n",
            "[Processing] Client 0, batch 450\n",
            "  Loss for client 0: 1.6909\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 450\n",
            "  Loss for client 1: 2.1415\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 450\n",
            "  Loss for client 2: 1.9018\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 450\n",
            "  Loss for client 3: 0.0005\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 451\n",
            "  Client 0: total grad norm = 89.863658 (253 params)\n",
            "  Client 1: total grad norm = 103.467844 (253 params)\n",
            "  Client 2: total grad norm = 77.601199 (253 params)\n",
            "  Client 3: total grad norm = 0.185871 (253 params)\n",
            "\n",
            "=== Optimizer Step 451 ===\n",
            "\n",
            "=== Updating Client Weights (Step 451) ===\n",
            "Gradient norms: [8.566014289855957, 11.216198921203613, 8.800537109375, 0.01865350641310215]\n",
            "Target weights: [0.00216471659950912, 0.0016532333102077246, 0.00210703001357615, 0.9940750002861023]\n",
            "Updated weights: [0.007399175316095352, 0.007570784073323011, 0.006505408324301243, 0.9785246253013611]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 451\n",
            "  Loss for client 0: 1.9324\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 451\n",
            "  Loss for client 1: 2.1217\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 451\n",
            "  Loss for client 2: 1.7747\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 451\n",
            "  Loss for client 3: 0.1809\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 452\n",
            "  Client 0: total grad norm = 80.250158 (253 params)\n",
            "  Client 1: total grad norm = 86.361375 (253 params)\n",
            "  Client 2: total grad norm = 83.198520 (253 params)\n",
            "  Client 3: total grad norm = 6.022502 (253 params)\n",
            "\n",
            "=== Optimizer Step 452 ===\n",
            "\n",
            "=== Updating Client Weights (Step 452) ===\n",
            "Gradient norms: [8.419038772583008, 10.018049240112305, 9.615230560302734, 2.2470803260803223]\n",
            "Target weights: [0.15473558008670807, 0.13003776967525482, 0.13548554480075836, 0.5797411203384399]\n",
            "Updated weights: [0.051600098609924316, 0.044310882687568665, 0.045199453830718994, 0.8588895797729492]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 452\n",
            "  Loss for client 0: 1.7252\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 452\n",
            "  Loss for client 1: 1.8599\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 452\n",
            "  Loss for client 2: 2.1258\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 452\n",
            "  Loss for client 3: 0.0006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 453\n",
            "  Client 0: total grad norm = 83.960875 (253 params)\n",
            "  Client 1: total grad norm = 83.737644 (253 params)\n",
            "  Client 2: total grad norm = 92.098002 (253 params)\n",
            "  Client 3: total grad norm = 0.227752 (253 params)\n",
            "\n",
            "=== Optimizer Step 453 ===\n",
            "\n",
            "=== Updating Client Weights (Step 453) ===\n",
            "Gradient norms: [9.600253105163574, 9.606801986694336, 8.906673431396484, 0.023766472935676575]\n",
            "Target weights: [0.0024568939115852118, 0.002455218927934766, 0.0026482166722416878, 0.9924397468566895]\n",
            "Updated weights: [0.036857135593891144, 0.031754184514284134, 0.03243408352136612, 0.8989546298980713]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 453\n",
            "  Loss for client 0: 1.8188\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 453\n",
            "  Loss for client 1: 2.4112\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 453\n",
            "  Loss for client 2: 1.1847\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 453\n",
            "  Loss for client 3: 0.0003\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 454\n",
            "  Client 0: total grad norm = 86.246994 (253 params)\n",
            "  Client 1: total grad norm = 94.774241 (253 params)\n",
            "  Client 2: total grad norm = 66.916566 (253 params)\n",
            "  Client 3: total grad norm = 0.093939 (253 params)\n",
            "\n",
            "=== Optimizer Step 454 ===\n",
            "\n",
            "=== Updating Client Weights (Step 454) ===\n",
            "Gradient norms: [8.56886100769043, 9.442973136901855, 6.964205265045166, 0.010158262215554714]\n",
            "Target weights: [0.0011810933938249946, 0.0010717625264078379, 0.0014532346976920962, 0.996293842792511]\n",
            "Updated weights: [0.026154322549700737, 0.022549455985426903, 0.02313982881605625, 0.9281563758850098]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 454\n",
            "  Loss for client 0: 1.8368\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 454\n",
            "  Loss for client 1: 2.3603\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 454\n",
            "  Loss for client 2: 2.1445\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 454\n",
            "  Loss for client 3: 0.1758\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 455\n",
            "  Client 0: total grad norm = 113.126644 (253 params)\n",
            "  Client 1: total grad norm = 99.237984 (253 params)\n",
            "  Client 2: total grad norm = 94.647884 (253 params)\n",
            "  Client 3: total grad norm = 5.910779 (253 params)\n",
            "\n",
            "=== Optimizer Step 455 ===\n",
            "\n",
            "=== Updating Client Weights (Step 455) ===\n",
            "Gradient norms: [13.695940971374512, 10.951526641845703, 10.304655075073242, 2.225191354751587]\n",
            "Target weights: [0.10272583365440369, 0.1284685730934143, 0.13653315603733063, 0.6322723627090454]\n",
            "Updated weights: [0.049125779420137405, 0.05432519689202309, 0.05715783312916756, 0.8393912315368652]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 455\n",
            "  Loss for client 0: 1.3691\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 455\n",
            "  Loss for client 1: 2.1830\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 455\n",
            "  Loss for client 2: 2.9123\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 455\n",
            "  Loss for client 3: 0.0003\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 456\n",
            "  Client 0: total grad norm = 64.869177 (253 params)\n",
            "  Client 1: total grad norm = 98.569384 (253 params)\n",
            "  Client 2: total grad norm = 97.180730 (253 params)\n",
            "  Client 3: total grad norm = 0.089481 (253 params)\n",
            "\n",
            "=== Optimizer Step 456 ===\n",
            "\n",
            "=== Updating Client Weights (Step 456) ===\n",
            "Gradient norms: [7.378694534301758, 9.30314826965332, 10.95771312713623, 0.008393518626689911]\n",
            "Target weights: [0.0011343529913574457, 0.00089970015687868, 0.0007638495299033821, 0.9972020387649536]\n",
            "Updated weights: [0.03472835198044777, 0.03829754516482353, 0.04023963585495949, 0.8867344856262207]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 456\n",
            "  Loss for client 0: 1.0015\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 456\n",
            "  Loss for client 1: 2.2327\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 456\n",
            "  Loss for client 2: 2.5875\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 456\n",
            "  Loss for client 3: 0.0015\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 457\n",
            "  Client 0: total grad norm = 75.476477 (253 params)\n",
            "  Client 1: total grad norm = 83.228161 (253 params)\n",
            "  Client 2: total grad norm = 100.104816 (253 params)\n",
            "  Client 3: total grad norm = 1.297474 (253 params)\n",
            "\n",
            "=== Optimizer Step 457 ===\n",
            "\n",
            "=== Updating Client Weights (Step 457) ===\n",
            "Gradient norms: [6.719259262084961, 9.535445213317871, 11.091541290283203, 0.09455510228872299]\n",
            "Target weights: [0.013629124499857426, 0.00960391666740179, 0.008256527595221996, 0.9685105085372925]\n",
            "Updated weights: [0.028398582711815834, 0.029689455404877663, 0.030644703656435013, 0.9112672805786133]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 457\n",
            "  Loss for client 0: 1.0472\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 457\n",
            "  Loss for client 1: 1.5211\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 457\n",
            "  Loss for client 2: 2.1943\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 457\n",
            "  Loss for client 3: 0.0005\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 458\n",
            "  Client 0: total grad norm = 85.138717 (253 params)\n",
            "  Client 1: total grad norm = 70.024445 (253 params)\n",
            "  Client 2: total grad norm = 90.597329 (253 params)\n",
            "  Client 3: total grad norm = 0.235168 (253 params)\n",
            "\n",
            "=== Optimizer Step 458 ===\n",
            "\n",
            "=== Updating Client Weights (Step 458) ===\n",
            "Gradient norms: [8.980317115783691, 8.069385528564453, 9.89602279663086, 0.025021804496645927]\n",
            "Target weights: [0.0027630424592643976, 0.0030749549623578787, 0.002507370663806796, 0.991654634475708]\n",
            "Updated weights: [0.02070792019367218, 0.021705104038119316, 0.02220350317656994, 0.9353834390640259]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 458\n",
            "  Loss for client 0: 2.0138\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 458\n",
            "  Loss for client 1: 1.2651\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 458\n",
            "  Loss for client 2: 2.3767\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 458\n",
            "  Loss for client 3: 0.0021\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 459\n",
            "  Client 0: total grad norm = 99.033908 (253 params)\n",
            "  Client 1: total grad norm = 56.935128 (253 params)\n",
            "  Client 2: total grad norm = 91.869686 (253 params)\n",
            "  Client 3: total grad norm = 1.645514 (253 params)\n",
            "\n",
            "=== Optimizer Step 459 ===\n",
            "\n",
            "=== Updating Client Weights (Step 459) ===\n",
            "Gradient norms: [10.841571807861328, 6.8642964363098145, 9.43890380859375, 0.12290284037590027]\n",
            "Target weights: [0.010876593180000782, 0.017178652808070183, 0.012492909096181393, 0.9594518542289734]\n",
            "Updated weights: [0.017758524045348167, 0.020347170531749725, 0.019290326163172722, 0.9426040053367615]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 459\n",
            "  Loss for client 0: 2.7356\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 459\n",
            "  Loss for client 1: 2.0895\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 459\n",
            "  Loss for client 2: 2.0423\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 459\n",
            "  Loss for client 3: 0.0007\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 460\n",
            "  Client 0: total grad norm = 108.683563 (253 params)\n",
            "  Client 1: total grad norm = 103.321490 (253 params)\n",
            "  Client 2: total grad norm = 82.332810 (253 params)\n",
            "  Client 3: total grad norm = 0.591401 (253 params)\n",
            "\n",
            "=== Optimizer Step 460 ===\n",
            "\n",
            "=== Updating Client Weights (Step 460) ===\n",
            "Gradient norms: [10.192248344421387, 10.19382381439209, 10.014463424682617, 0.04003763943910599]\n",
            "Target weights: [0.003882225602865219, 0.0038816258311271667, 0.003951146267354488, 0.9882850050926208]\n",
            "Updated weights: [0.013595635071396828, 0.015407506376504898, 0.01468857191503048, 0.9563083648681641]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 460\n",
            "  Loss for client 0: 1.8969\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 460\n",
            "  Loss for client 1: 1.5822\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 460\n",
            "  Loss for client 2: 1.8010\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 460\n",
            "  Loss for client 3: 0.0157\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 461\n",
            "  Client 0: total grad norm = 95.216716 (253 params)\n",
            "  Client 1: total grad norm = 78.578486 (253 params)\n",
            "  Client 2: total grad norm = 89.147196 (253 params)\n",
            "  Client 3: total grad norm = 83.722566 (253 params)\n",
            "\n",
            "=== Optimizer Step 461 ===\n",
            "\n",
            "=== Updating Client Weights (Step 461) ===\n",
            "Gradient norms: [9.637096405029297, 8.212311744689941, 9.739912033081055, 5.321131706237793]\n",
            "Target weights: [0.20104394853115082, 0.23592381179332733, 0.19892171025276184, 0.3641104996204376]\n",
            "Updated weights: [0.06983013451099396, 0.08156239986419678, 0.06995851546525955, 0.7786489725112915]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 461\n",
            "  Loss for client 0: 2.0244\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 461\n",
            "  Loss for client 1: 1.9990\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 461\n",
            "  Loss for client 2: 2.1664\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 461\n",
            "  Loss for client 3: 0.0007\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 462\n",
            "  Client 0: total grad norm = 100.019757 (253 params)\n",
            "  Client 1: total grad norm = 90.982642 (253 params)\n",
            "  Client 2: total grad norm = 94.325984 (253 params)\n",
            "  Client 3: total grad norm = 0.296390 (253 params)\n",
            "\n",
            "=== Optimizer Step 462 ===\n",
            "\n",
            "=== Updating Client Weights (Step 462) ===\n",
            "Gradient norms: [10.896018028259277, 9.207357406616211, 9.075032234191895, 0.039849039167165756]\n",
            "Target weights: [0.00361250271089375, 0.004275048151612282, 0.004337383434176445, 0.9877750277519226]\n",
            "Updated weights: [0.049964845180511475, 0.058376193046569824, 0.050272174179553986, 0.8413867950439453]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 462\n",
            "  Loss for client 0: 1.7138\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 462\n",
            "  Loss for client 1: 1.9163\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 462\n",
            "  Loss for client 2: 1.7490\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 462\n",
            "  Loss for client 3: 0.0007\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 463\n",
            "  Client 0: total grad norm = 94.411761 (253 params)\n",
            "  Client 1: total grad norm = 91.497664 (253 params)\n",
            "  Client 2: total grad norm = 85.888265 (253 params)\n",
            "  Client 3: total grad norm = 0.304898 (253 params)\n",
            "\n",
            "=== Optimizer Step 463 ===\n",
            "\n",
            "=== Updating Client Weights (Step 463) ===\n",
            "Gradient norms: [9.365537643432617, 9.524160385131836, 8.890307426452637, 0.03098851442337036]\n",
            "Target weights: [0.0032758661545813084, 0.0032213071826845407, 0.003450977150350809, 0.9900519251823425]\n",
            "Updated weights: [0.03595815226435661, 0.041829727590084076, 0.0362258143723011, 0.885986328125]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 463\n",
            "  Loss for client 0: 1.4893\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 463\n",
            "  Loss for client 1: 2.1578\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 463\n",
            "  Loss for client 2: 2.6644\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 463\n",
            "  Loss for client 3: 0.1576\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 464\n",
            "  Client 0: total grad norm = 85.427991 (253 params)\n",
            "  Client 1: total grad norm = 104.220495 (253 params)\n",
            "  Client 2: total grad norm = 116.040510 (253 params)\n",
            "  Client 3: total grad norm = 6.006380 (253 params)\n",
            "\n",
            "=== Optimizer Step 464 ===\n",
            "\n",
            "=== Updating Client Weights (Step 464) ===\n",
            "Gradient norms: [9.021689414978027, 11.03940486907959, 13.635991096496582, 2.216348171234131]\n",
            "Target weights: [0.15268680453300476, 0.12477962672710419, 0.10101890563964844, 0.6215146780014038]\n",
            "Updated weights: [0.07097674906253815, 0.06671469658613205, 0.05566374212503433, 0.806644856929779]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 464\n",
            "  Loss for client 0: 1.3332\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 464\n",
            "  Loss for client 1: 1.6191\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 464\n",
            "  Loss for client 2: 2.0293\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 464\n",
            "  Loss for client 3: 0.0010\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 465\n",
            "  Client 0: total grad norm = 71.923538 (253 params)\n",
            "  Client 1: total grad norm = 76.496372 (253 params)\n",
            "  Client 2: total grad norm = 87.984162 (253 params)\n",
            "  Client 3: total grad norm = 0.653602 (253 params)\n",
            "\n",
            "=== Optimizer Step 465 ===\n",
            "\n",
            "=== Updating Client Weights (Step 465) ===\n",
            "Gradient norms: [6.3275299072265625, 7.504016876220703, 8.450615882873535, 0.052787408232688904]\n",
            "Target weights: [0.00816592387855053, 0.00688566267490387, 0.006114362273365259, 0.9788340926170349]\n",
            "Updated weights: [0.05213350057601929, 0.048765987157821655, 0.04079892486333847, 0.8583016395568848]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 465\n",
            "  Loss for client 0: 1.8393\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 465\n",
            "  Loss for client 1: 2.1372\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 465\n",
            "  Loss for client 2: 2.1433\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 465\n",
            "  Loss for client 3: 0.0008\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 466\n",
            "  Client 0: total grad norm = 95.751786 (253 params)\n",
            "  Client 1: total grad norm = 74.561676 (253 params)\n",
            "  Client 2: total grad norm = 83.299711 (253 params)\n",
            "  Client 3: total grad norm = 0.556100 (253 params)\n",
            "\n",
            "=== Optimizer Step 466 ===\n",
            "\n",
            "=== Updating Client Weights (Step 466) ===\n",
            "Gradient norms: [8.615313529968262, 7.421468257904053, 8.23882007598877, 0.03817245364189148]\n",
            "Target weights: [0.004368700552731752, 0.005071466322988272, 0.004568339325487614, 0.9859915375709534]\n",
            "Updated weights: [0.03780405968427658, 0.03565762937068939, 0.0299297496676445, 0.8966085910797119]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 466\n",
            "  Loss for client 0: 1.4340\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 466\n",
            "  Loss for client 1: 2.0033\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 466\n",
            "  Loss for client 2: 2.4145\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 466\n",
            "  Loss for client 3: 0.2381\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 467\n",
            "  Client 0: total grad norm = 77.704981 (253 params)\n",
            "  Client 1: total grad norm = 84.477127 (253 params)\n",
            "  Client 2: total grad norm = 98.297074 (253 params)\n",
            "  Client 3: total grad norm = 29.116833 (253 params)\n",
            "\n",
            "=== Optimizer Step 467 ===\n",
            "\n",
            "=== Updating Client Weights (Step 467) ===\n",
            "Gradient norms: [8.895140647888184, 8.389480590820312, 9.866907119750977, 3.352435827255249]\n",
            "Target weights: [0.17809051275253296, 0.1888245791196823, 0.16055083274841309, 0.47253406047821045]\n",
            "Updated weights: [0.07988999783992767, 0.08160771429538727, 0.06911607086658478, 0.7693862318992615]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 467\n",
            "  Loss for client 0: 1.6283\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 467\n",
            "  Loss for client 1: 1.7951\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 467\n",
            "  Loss for client 2: 2.2633\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 467\n",
            "  Loss for client 3: 0.0008\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 468\n",
            "  Client 0: total grad norm = 83.787301 (253 params)\n",
            "  Client 1: total grad norm = 80.437048 (253 params)\n",
            "  Client 2: total grad norm = 89.568423 (253 params)\n",
            "  Client 3: total grad norm = 0.354478 (253 params)\n",
            "\n",
            "=== Optimizer Step 468 ===\n",
            "\n",
            "=== Updating Client Weights (Step 468) ===\n",
            "Gradient norms: [9.34673023223877, 8.227046966552734, 9.80688190460205, 0.030475379899144173]\n",
            "Target weights: [0.003228026209399104, 0.0036673536524176598, 0.003076563123613596, 0.9900280237197876]\n",
            "Updated weights: [0.05689140409231186, 0.0582256056368351, 0.04930422082543373, 0.8355787992477417]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 468\n",
            "  Loss for client 0: 0.9873\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 468\n",
            "  Loss for client 1: 1.3574\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 468\n",
            "  Loss for client 2: 1.6494\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 468\n",
            "  Loss for client 3: 0.0015\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 469\n",
            "  Client 0: total grad norm = 77.805345 (253 params)\n",
            "  Client 1: total grad norm = 83.748659 (253 params)\n",
            "  Client 2: total grad norm = 79.170509 (253 params)\n",
            "  Client 3: total grad norm = 1.757760 (253 params)\n",
            "\n",
            "=== Optimizer Step 469 ===\n",
            "\n",
            "=== Updating Client Weights (Step 469) ===\n",
            "Gradient norms: [8.923723220825195, 9.116703987121582, 8.665297508239746, 0.11207073926925659]\n",
            "Target weights: [0.012101489119231701, 0.011845326982438564, 0.012462392449378967, 0.9635908007621765]\n",
            "Updated weights: [0.043454427272081375, 0.0443115197122097, 0.038251668214797974, 0.873982310295105]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 469\n",
            "  Loss for client 0: 2.2279\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 469\n",
            "  Loss for client 1: 1.7209\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 469\n",
            "  Loss for client 2: 1.8207\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 469\n",
            "  Loss for client 3: 0.0016\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 470\n",
            "  Client 0: total grad norm = 104.318922 (253 params)\n",
            "  Client 1: total grad norm = 82.859365 (253 params)\n",
            "  Client 2: total grad norm = 86.391867 (253 params)\n",
            "  Client 3: total grad norm = 1.390087 (253 params)\n",
            "\n",
            "=== Optimizer Step 470 ===\n",
            "\n",
            "=== Updating Client Weights (Step 470) ===\n",
            "Gradient norms: [9.586925506591797, 9.230074882507324, 9.594581604003906, 0.08264323323965073]\n",
            "Target weights: [0.00840042345225811, 0.008725198917090893, 0.008393720723688602, 0.9744806885719299]\n",
            "Updated weights: [0.03293822705745697, 0.03363562375307083, 0.02929428219795227, 0.9041318297386169]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 470\n",
            "  Loss for client 0: 1.8192\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 470\n",
            "  Loss for client 1: 1.8986\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 470\n",
            "  Loss for client 2: 1.1190\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 470\n",
            "  Loss for client 3: 0.4149\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 471\n",
            "  Client 0: total grad norm = 103.724512 (253 params)\n",
            "  Client 1: total grad norm = 88.852062 (253 params)\n",
            "  Client 2: total grad norm = 83.423974 (253 params)\n",
            "  Client 3: total grad norm = 15.715380 (253 params)\n",
            "\n",
            "=== Optimizer Step 471 ===\n",
            "\n",
            "=== Updating Client Weights (Step 471) ===\n",
            "Gradient norms: [10.45483684539795, 9.338000297546387, 7.43461799621582, 3.703627347946167]\n",
            "Target weights: [0.15751250088214874, 0.17635118961334229, 0.22149993479251862, 0.44463637471199036]\n",
            "Updated weights: [0.07031051814556122, 0.07645030319690704, 0.08695598691701889, 0.7662832140922546]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 471\n",
            "  Loss for client 0: 3.1944\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 471\n",
            "  Loss for client 1: 2.1166\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 471\n",
            "  Loss for client 2: 1.6068\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 471\n",
            "  Loss for client 3: 0.0011\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 472\n",
            "  Client 0: total grad norm = 116.957826 (253 params)\n",
            "  Client 1: total grad norm = 99.838614 (253 params)\n",
            "  Client 2: total grad norm = 85.223211 (253 params)\n",
            "  Client 3: total grad norm = 0.633202 (253 params)\n",
            "\n",
            "=== Optimizer Step 472 ===\n",
            "\n",
            "=== Updating Client Weights (Step 472) ===\n",
            "Gradient norms: [11.099259376525879, 11.015134811401367, 10.06972885131836, 0.041994404047727585]\n",
            "Target weights: [0.0037395325489342213, 0.0037680920213460922, 0.004121862817555666, 0.9883705377578735]\n",
            "Updated weights: [0.0503392219543457, 0.054645638912916183, 0.06210574880242348, 0.8329094052314758]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 472\n",
            "  Loss for client 0: 2.0955\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 472\n",
            "  Loss for client 1: 2.0244\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 472\n",
            "  Loss for client 2: 1.4157\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 472\n",
            "  Loss for client 3: 0.0013\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 473\n",
            "  Client 0: total grad norm = 78.709025 (253 params)\n",
            "  Client 1: total grad norm = 91.239217 (253 params)\n",
            "  Client 2: total grad norm = 69.344464 (253 params)\n",
            "  Client 3: total grad norm = 0.720507 (253 params)\n",
            "\n",
            "=== Optimizer Step 473 ===\n",
            "\n",
            "=== Updating Client Weights (Step 473) ===\n",
            "Gradient norms: [10.810931205749512, 9.425033569335938, 7.5324602127075195, 0.06431104987859726]\n",
            "Target weights: [0.0058245849795639515, 0.006681056693196297, 0.00835970975458622, 0.9791346192359924]\n",
            "Updated weights: [0.03698483109474182, 0.04025626555085182, 0.045981936156749725, 0.8767769932746887]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 473\n",
            "  Loss for client 0: 0.9481\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 473\n",
            "  Loss for client 1: 1.6538\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 473\n",
            "  Loss for client 2: 2.1330\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 473\n",
            "  Loss for client 3: 0.1790\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 474\n",
            "  Client 0: total grad norm = 52.134255 (253 params)\n",
            "  Client 1: total grad norm = 73.344983 (253 params)\n",
            "  Client 2: total grad norm = 79.744420 (253 params)\n",
            "  Client 3: total grad norm = 5.522788 (253 params)\n",
            "\n",
            "=== Optimizer Step 474 ===\n",
            "\n",
            "=== Updating Client Weights (Step 474) ===\n",
            "Gradient norms: [6.054894924163818, 6.999763011932373, 9.013850212097168, 2.2373316287994385]\n",
            "Target weights: [0.19072873890399933, 0.16498307883739471, 0.12811866402626038, 0.516169548034668]\n",
            "Updated weights: [0.08310800790786743, 0.07767431437969208, 0.07062295079231262, 0.7685947418212891]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 474\n",
            "  Loss for client 0: 1.6607\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 474\n",
            "  Loss for client 1: 1.8775\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 474\n",
            "  Loss for client 2: 1.5593\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 474\n",
            "  Loss for client 3: 0.0601\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 475\n",
            "  Client 0: total grad norm = 76.796532 (253 params)\n",
            "  Client 1: total grad norm = 81.848397 (253 params)\n",
            "  Client 2: total grad norm = 69.003045 (253 params)\n",
            "  Client 3: total grad norm = 146.282317 (253 params)\n",
            "\n",
            "=== Optimizer Step 475 ===\n",
            "\n",
            "=== Updating Client Weights (Step 475) ===\n",
            "Gradient norms: [7.696484088897705, 8.67102336883545, 7.229909896850586, 9.346311569213867]\n",
            "Target weights: [0.26485705375671387, 0.23508967459201813, 0.28194931149482727, 0.21810400485992432]\n",
            "Updated weights: [0.13763272762298584, 0.12489892542362213, 0.1340208649635315, 0.6034475564956665]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 475\n",
            "  Loss for client 0: 1.4236\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 475\n",
            "  Loss for client 1: 2.0832\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 475\n",
            "  Loss for client 2: 1.0537\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 475\n",
            "  Loss for client 3: 0.0015\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 476\n",
            "  Client 0: total grad norm = 77.880954 (253 params)\n",
            "  Client 1: total grad norm = 81.686706 (253 params)\n",
            "  Client 2: total grad norm = 55.561672 (253 params)\n",
            "  Client 3: total grad norm = 0.785764 (253 params)\n",
            "\n",
            "=== Optimizer Step 476 ===\n",
            "\n",
            "=== Updating Client Weights (Step 476) ===\n",
            "Gradient norms: [9.30234146118164, 8.813985824584961, 6.524481296539307, 0.07563911378383636]\n",
            "Target weights: [0.007907365448772907, 0.008345488458871841, 0.011274001561105251, 0.97247314453125]\n",
            "Updated weights: [0.09871510416269302, 0.0899328887462616, 0.0971967875957489, 0.7141551971435547]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 476\n",
            "  Loss for client 0: 1.8076\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 476\n",
            "  Loss for client 1: 1.8358\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 476\n",
            "  Loss for client 2: 1.6269\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 476\n",
            "  Loss for client 3: 0.2159\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 477\n",
            "  Client 0: total grad norm = 80.060152 (253 params)\n",
            "  Client 1: total grad norm = 86.150491 (253 params)\n",
            "  Client 2: total grad norm = 76.404248 (253 params)\n",
            "  Client 3: total grad norm = 5.190809 (253 params)\n",
            "\n",
            "=== Optimizer Step 477 ===\n",
            "\n",
            "=== Updating Client Weights (Step 477) ===\n",
            "Gradient norms: [9.53273868560791, 9.531678199768066, 8.437983512878418, 2.2075042724609375]\n",
            "Target weights: [0.134260892868042, 0.13427582383155823, 0.15168006718158722, 0.5797832012176514]\n",
            "Updated weights: [0.10937884449958801, 0.10323576629161835, 0.11354176700115204, 0.6738436222076416]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 477\n",
            "  Loss for client 0: 1.5534\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 477\n",
            "  Loss for client 1: 2.4254\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 477\n",
            "  Loss for client 2: 2.0243\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 477\n",
            "  Loss for client 3: 0.0011\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 478\n",
            "  Client 0: total grad norm = 69.152050 (253 params)\n",
            "  Client 1: total grad norm = 84.007151 (253 params)\n",
            "  Client 2: total grad norm = 78.397649 (253 params)\n",
            "  Client 3: total grad norm = 0.893223 (253 params)\n",
            "\n",
            "=== Optimizer Step 478 ===\n",
            "\n",
            "=== Updating Client Weights (Step 478) ===\n",
            "Gradient norms: [8.307345390319824, 8.77062702178955, 9.345932006835938, 0.0621732734143734]\n",
            "Target weights: [0.0073285819962620735, 0.006941472180187702, 0.006514177657663822, 0.9792158007621765]\n",
            "Updated weights: [0.07876376807689667, 0.07434748113155365, 0.0814334824681282, 0.7654552459716797]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 478\n",
            "  Loss for client 0: 0.9387\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 478\n",
            "  Loss for client 1: 2.8030\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 478\n",
            "  Loss for client 2: 1.8764\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 478\n",
            "  Loss for client 3: 0.0020\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 479\n",
            "  Client 0: total grad norm = 46.146594 (253 params)\n",
            "  Client 1: total grad norm = 106.669358 (253 params)\n",
            "  Client 2: total grad norm = 77.796576 (253 params)\n",
            "  Client 3: total grad norm = 1.118762 (253 params)\n",
            "\n",
            "=== Optimizer Step 479 ===\n",
            "\n",
            "=== Updating Client Weights (Step 479) ===\n",
            "Gradient norms: [5.189867973327637, 11.591476440429688, 10.257552146911621, 0.09548775851726532]\n",
            "Target weights: [0.017760468646883965, 0.007951919920742512, 0.008986013010144234, 0.9653015732765198]\n",
            "Updated weights: [0.06046278029680252, 0.054428815841674805, 0.05969924479722977, 0.8254091739654541]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 479\n",
            "  Loss for client 0: 1.6504\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 479\n",
            "  Loss for client 1: 1.5278\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 479\n",
            "  Loss for client 2: 2.4537\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 479\n",
            "  Loss for client 3: 0.0011\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 480\n",
            "  Client 0: total grad norm = 101.675463 (253 params)\n",
            "  Client 1: total grad norm = 73.556554 (253 params)\n",
            "  Client 2: total grad norm = 89.334457 (253 params)\n",
            "  Client 3: total grad norm = 0.587686 (253 params)\n",
            "\n",
            "=== Optimizer Step 480 ===\n",
            "\n",
            "=== Updating Client Weights (Step 480) ===\n",
            "Gradient norms: [12.12839412689209, 8.451796531677246, 10.074402809143066, 0.045654237270355225]\n",
            "Target weights: [0.0037133803125470877, 0.005328729748725891, 0.004470472689718008, 0.9864873886108398]\n",
            "Updated weights: [0.04343796148896217, 0.03969879075884819, 0.04313061386346817, 0.8737326264381409]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 480\n",
            "  Loss for client 0: 2.1302\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 480\n",
            "  Loss for client 1: 1.2304\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 480\n",
            "  Loss for client 2: 1.6449\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 480\n",
            "  Loss for client 3: 0.0007\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 481\n",
            "  Client 0: total grad norm = 93.889338 (253 params)\n",
            "  Client 1: total grad norm = 58.356591 (253 params)\n",
            "  Client 2: total grad norm = 78.051768 (253 params)\n",
            "  Client 3: total grad norm = 0.309474 (253 params)\n",
            "\n",
            "=== Optimizer Step 481 ===\n",
            "\n",
            "=== Updating Client Weights (Step 481) ===\n",
            "Gradient norms: [9.39322566986084, 6.7073235511779785, 9.116121292114258, 0.024563143029808998]\n",
            "Target weights: [0.0025917338207364082, 0.0036295761819928885, 0.002670515328645706, 0.9911081790924072]\n",
            "Updated weights: [0.031184092164039612, 0.028878025710582733, 0.03099258430302143, 0.9089453220367432]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 481\n",
            "  Loss for client 0: 2.2595\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 481\n",
            "  Loss for client 1: 1.9353\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 481\n",
            "  Loss for client 2: 1.5209\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 481\n",
            "  Loss for client 3: 0.0018\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 482\n",
            "  Client 0: total grad norm = 101.387598 (253 params)\n",
            "  Client 1: total grad norm = 86.044224 (253 params)\n",
            "  Client 2: total grad norm = 64.567372 (253 params)\n",
            "  Client 3: total grad norm = 1.283509 (253 params)\n",
            "\n",
            "=== Optimizer Step 482 ===\n",
            "\n",
            "=== Updating Client Weights (Step 482) ===\n",
            "Gradient norms: [10.767659187316895, 9.314329147338867, 7.3431220054626465, 0.09589935839176178]\n",
            "Target weights: [0.00862788874655962, 0.009974111802875996, 0.012651588767766953, 0.9687463641166687]\n",
            "Updated weights: [0.024417230859398842, 0.02320685051381588, 0.025490285828709602, 0.9268856048583984]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 482\n",
            "  Loss for client 0: 1.2271\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 482\n",
            "  Loss for client 1: 2.1282\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 482\n",
            "  Loss for client 2: 2.2297\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 482\n",
            "  Loss for client 3: 0.0011\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 483\n",
            "  Client 0: total grad norm = 89.722212 (253 params)\n",
            "  Client 1: total grad norm = 81.260252 (253 params)\n",
            "  Client 2: total grad norm = 84.667825 (253 params)\n",
            "  Client 3: total grad norm = 0.652289 (253 params)\n",
            "\n",
            "=== Optimizer Step 483 ===\n",
            "\n",
            "=== Updating Client Weights (Step 483) ===\n",
            "Gradient norms: [9.779191017150879, 8.598681449890137, 10.275484085083008, 0.05079447105526924]\n",
            "Target weights: [0.005112117156386375, 0.005813958123326302, 0.004865208175033331, 0.9842087030410767]\n",
            "Updated weights: [0.018625697121024132, 0.017988983541727066, 0.019302763044834137, 0.9440825581550598]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 483\n",
            "  Loss for client 0: 1.5995\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 483\n",
            "  Loss for client 1: 1.4012\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 483\n",
            "  Loss for client 2: 2.2517\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 483\n",
            "  Loss for client 3: 0.0015\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 484\n",
            "  Client 0: total grad norm = 73.343538 (253 params)\n",
            "  Client 1: total grad norm = 64.868743 (253 params)\n",
            "  Client 2: total grad norm = 84.004769 (253 params)\n",
            "  Client 3: total grad norm = 1.089739 (253 params)\n",
            "\n",
            "=== Optimizer Step 484 ===\n",
            "\n",
            "=== Updating Client Weights (Step 484) ===\n",
            "Gradient norms: [7.080647945404053, 7.139616966247559, 9.330124855041504, 0.08834384381771088]\n",
            "Target weights: [0.012062816880643368, 0.011963184922933578, 0.009154492989182472, 0.9668194651603699]\n",
            "Updated weights: [0.01665683276951313, 0.01618124358355999, 0.016258282586932182, 0.9509036540985107]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 484\n",
            "  Loss for client 0: 1.8499\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 484\n",
            "  Loss for client 1: 2.3575\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 484\n",
            "  Loss for client 2: 2.3919\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 484\n",
            "  Loss for client 3: 0.0015\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 485\n",
            "  Client 0: total grad norm = 79.627403 (253 params)\n",
            "  Client 1: total grad norm = 82.629059 (253 params)\n",
            "  Client 2: total grad norm = 88.498334 (253 params)\n",
            "  Client 3: total grad norm = 0.657103 (253 params)\n",
            "\n",
            "=== Optimizer Step 485 ===\n",
            "\n",
            "=== Updating Client Weights (Step 485) ===\n",
            "Gradient norms: [8.169722557067871, 9.355449676513672, 8.724289894104004, 0.05952311307191849]\n",
            "Target weights: [0.007139664143323898, 0.0062347701750695705, 0.006685825530439615, 0.9799398183822632]\n",
            "Updated weights: [0.01380168180912733, 0.01319730095565319, 0.013386544771492481, 0.9596145153045654]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 485\n",
            "  Loss for client 0: 1.5275\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 485\n",
            "  Loss for client 1: 1.1849\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 485\n",
            "  Loss for client 2: 1.9126\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 485\n",
            "  Loss for client 3: 0.0020\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 486\n",
            "  Client 0: total grad norm = 81.508322 (253 params)\n",
            "  Client 1: total grad norm = 72.107316 (253 params)\n",
            "  Client 2: total grad norm = 113.486393 (253 params)\n",
            "  Client 3: total grad norm = 0.977330 (253 params)\n",
            "\n",
            "=== Optimizer Step 486 ===\n",
            "\n",
            "=== Updating Client Weights (Step 486) ===\n",
            "Gradient norms: [7.973369598388672, 8.1181001663208, 13.265140533447266, 0.09952584654092789]\n",
            "Target weights: [0.012092365883290768, 0.011876782402396202, 0.007268442306667566, 0.9687624573707581]\n",
            "Updated weights: [0.013288885354995728, 0.012801144272089005, 0.011551112867891788, 0.962358832359314]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 486\n",
            "  Loss for client 0: 1.6002\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 486\n",
            "  Loss for client 1: 2.2523\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 486\n",
            "  Loss for client 2: 2.1897\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 486\n",
            "  Loss for client 3: 0.0018\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 487\n",
            "  Client 0: total grad norm = 91.178713 (253 params)\n",
            "  Client 1: total grad norm = 124.876309 (253 params)\n",
            "  Client 2: total grad norm = 83.978367 (253 params)\n",
            "  Client 3: total grad norm = 2.772818 (253 params)\n",
            "\n",
            "=== Optimizer Step 487 ===\n",
            "\n",
            "=== Updating Client Weights (Step 487) ===\n",
            "Gradient norms: [10.048904418945312, 11.319328308105469, 9.06003475189209, 0.2250603288412094]\n",
            "Target weights: [0.020987799391150475, 0.018632235005497932, 0.02327854000031948, 0.937101423740387]\n",
            "Updated weights: [0.015598559752106667, 0.014550471678376198, 0.015069341287016869, 0.9547815918922424]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 487\n",
            "  Loss for client 0: 1.4514\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 487\n",
            "  Loss for client 1: 2.0811\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 487\n",
            "  Loss for client 2: 2.0695\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 487\n",
            "  Loss for client 3: 0.1981\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 488\n",
            "  Client 0: total grad norm = 64.469622 (253 params)\n",
            "  Client 1: total grad norm = 78.966100 (253 params)\n",
            "  Client 2: total grad norm = 83.794769 (253 params)\n",
            "  Client 3: total grad norm = 6.433453 (253 params)\n",
            "\n",
            "=== Optimizer Step 488 ===\n",
            "\n",
            "=== Updating Client Weights (Step 488) ===\n",
            "Gradient norms: [6.759853363037109, 8.296677589416504, 9.888354301452637, 2.196938991546631]\n",
            "Target weights: [0.1793617159128189, 0.14613787829875946, 0.12261483073234558, 0.5518855452537537]\n",
            "Updated weights: [0.06472750753164291, 0.05402669683098793, 0.047332990914583206, 0.8339127898216248]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 488\n",
            "  Loss for client 0: 1.7337\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 488\n",
            "  Loss for client 1: 2.0510\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 488\n",
            "  Loss for client 2: 2.4494\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 488\n",
            "  Loss for client 3: 0.0014\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 489\n",
            "  Client 0: total grad norm = 77.435770 (253 params)\n",
            "  Client 1: total grad norm = 126.192825 (253 params)\n",
            "  Client 2: total grad norm = 93.079213 (253 params)\n",
            "  Client 3: total grad norm = 0.518818 (253 params)\n",
            "\n",
            "=== Optimizer Step 489 ===\n",
            "\n",
            "=== Updating Client Weights (Step 489) ===\n",
            "Gradient norms: [9.419897079467773, 10.738129615783691, 10.44417953491211, 0.054294634610414505]\n",
            "Target weights: [0.005672953091561794, 0.004976531025022268, 0.005116594489663839, 0.9842339754104614]\n",
            "Updated weights: [0.04701114073395729, 0.03931164741516113, 0.03466807305812836, 0.8790091276168823]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 489\n",
            "  Loss for client 0: 1.6130\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 489\n",
            "  Loss for client 1: 1.5963\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 489\n",
            "  Loss for client 2: 1.9517\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 489\n",
            "  Loss for client 3: 0.0012\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 490\n",
            "  Client 0: total grad norm = 79.438300 (253 params)\n",
            "  Client 1: total grad norm = 85.109309 (253 params)\n",
            "  Client 2: total grad norm = 85.285613 (253 params)\n",
            "  Client 3: total grad norm = 0.551919 (253 params)\n",
            "\n",
            "=== Optimizer Step 490 ===\n",
            "\n",
            "=== Updating Client Weights (Step 490) ===\n",
            "Gradient norms: [9.80106258392334, 9.832868576049805, 8.87177848815918, 0.04131436347961426]\n",
            "Target weights: [0.004160896874964237, 0.004147437866777182, 0.004596734419465065, 0.9870949387550354]\n",
            "Updated weights: [0.03415606915950775, 0.02876238524913788, 0.025646671652793884, 0.9114348888397217]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 490\n",
            "  Loss for client 0: 1.4669\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 490\n",
            "  Loss for client 1: 1.3212\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 490\n",
            "  Loss for client 2: 2.0901\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 490\n",
            "  Loss for client 3: 0.0020\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 491\n",
            "  Client 0: total grad norm = 71.658000 (253 params)\n",
            "  Client 1: total grad norm = 97.194061 (253 params)\n",
            "  Client 2: total grad norm = 78.832087 (253 params)\n",
            "  Client 3: total grad norm = 1.208014 (253 params)\n",
            "\n",
            "=== Optimizer Step 491 ===\n",
            "\n",
            "=== Updating Client Weights (Step 491) ===\n",
            "Gradient norms: [8.913262367248535, 8.921110153198242, 9.082797050476074, 0.12340448796749115]\n",
            "Target weights: [0.01329637411981821, 0.013284677639603615, 0.013048190623521805, 0.9603707790374756]\n",
            "Updated weights: [0.027898160740733147, 0.024119071662425995, 0.02186712622642517, 0.9261156320571899]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 491\n",
            "  Loss for client 0: 1.1217\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 491\n",
            "  Loss for client 1: 1.3972\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 491\n",
            "  Loss for client 2: 1.5163\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 491\n",
            "  Loss for client 3: 0.0012\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 492\n",
            "  Client 0: total grad norm = 74.590939 (253 params)\n",
            "  Client 1: total grad norm = 69.041632 (253 params)\n",
            "  Client 2: total grad norm = 60.830833 (253 params)\n",
            "  Client 3: total grad norm = 0.575347 (253 params)\n",
            "\n",
            "=== Optimizer Step 492 ===\n",
            "\n",
            "=== Updating Client Weights (Step 492) ===\n",
            "Gradient norms: [7.27174711227417, 7.574573516845703, 7.103141784667969, 0.05318380519747734]\n",
            "Target weights: [0.007157565094530582, 0.006871410179883242, 0.007327462080866098, 0.978643536567688]\n",
            "Updated weights: [0.02167598344385624, 0.018944771960377693, 0.017505226656794548, 0.9418740272521973]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 492\n",
            "  Loss for client 0: 2.0252\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 492\n",
            "  Loss for client 1: 2.0653\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 492\n",
            "  Loss for client 2: 1.8571\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 492\n",
            "  Loss for client 3: 0.0013\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 493\n",
            "  Client 0: total grad norm = 80.291548 (253 params)\n",
            "  Client 1: total grad norm = 91.649129 (253 params)\n",
            "  Client 2: total grad norm = 89.073843 (253 params)\n",
            "  Client 3: total grad norm = 1.116605 (253 params)\n",
            "\n",
            "=== Optimizer Step 493 ===\n",
            "\n",
            "=== Updating Client Weights (Step 493) ===\n",
            "Gradient norms: [10.952780723571777, 8.304356575012207, 9.139551162719727, 0.09536651521921158]\n",
            "Target weights: [0.008448326960206032, 0.011142666451632977, 0.010124421678483486, 0.970284640789032]\n",
            "Updated weights: [0.017707686871290207, 0.016604140400886536, 0.015290985815227032, 0.950397253036499]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 493\n",
            "  Loss for client 0: 1.6809\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 493\n",
            "  Loss for client 1: 2.1632\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 493\n",
            "  Loss for client 2: 2.2874\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 493\n",
            "  Loss for client 3: 0.0011\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 494\n",
            "  Client 0: total grad norm = 97.826696 (253 params)\n",
            "  Client 1: total grad norm = 72.095225 (253 params)\n",
            "  Client 2: total grad norm = 80.004615 (253 params)\n",
            "  Client 3: total grad norm = 0.671581 (253 params)\n",
            "\n",
            "=== Optimizer Step 494 ===\n",
            "\n",
            "=== Updating Client Weights (Step 494) ===\n",
            "Gradient norms: [10.77275562286377, 8.943930625915527, 10.176486015319824, 0.05517812445759773]\n",
            "Target weights: [0.005037808325141668, 0.006067922338843346, 0.005332987755537033, 0.9835612773895264]\n",
            "Updated weights: [0.013906723819673061, 0.013443275354802608, 0.012303587049245834, 0.9603464603424072]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 494\n",
            "  Loss for client 0: 1.9842\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 494\n",
            "  Loss for client 1: 1.9490\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 494\n",
            "  Loss for client 2: 1.9457\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 494\n",
            "  Loss for client 3: 0.0023\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 495\n",
            "  Client 0: total grad norm = 76.518321 (253 params)\n",
            "  Client 1: total grad norm = 75.500204 (253 params)\n",
            "  Client 2: total grad norm = 84.490934 (253 params)\n",
            "  Client 3: total grad norm = 8.082926 (253 params)\n",
            "\n",
            "=== Optimizer Step 495 ===\n",
            "\n",
            "=== Updating Client Weights (Step 495) ===\n",
            "Gradient norms: [8.16392707824707, 7.746186256408691, 9.041165351867676, 0.4365389049053192]\n",
            "Target weights: [0.04617147892713547, 0.04866144433617592, 0.04169159382581711, 0.8634754419326782]\n",
            "Updated weights: [0.023586150258779526, 0.024008726701140404, 0.021119989454746246, 0.9312851428985596]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 495\n",
            "  Loss for client 0: 1.3884\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 495\n",
            "  Loss for client 1: 1.6262\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 495\n",
            "  Loss for client 2: 1.9146\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 495\n",
            "  Loss for client 3: 0.0021\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 496\n",
            "  Client 0: total grad norm = 64.796167 (253 params)\n",
            "  Client 1: total grad norm = 87.999102 (253 params)\n",
            "  Client 2: total grad norm = 68.086198 (253 params)\n",
            "  Client 3: total grad norm = 1.300693 (253 params)\n",
            "\n",
            "=== Optimizer Step 496 ===\n",
            "\n",
            "=== Updating Client Weights (Step 496) ===\n",
            "Gradient norms: [6.378635883331299, 9.19188404083252, 7.616361141204834, 0.1558845043182373]\n",
            "Target weights: [0.023014739155769348, 0.015970898792147636, 0.019274642691016197, 0.9417397379875183]\n",
            "Updated weights: [0.023414725437760353, 0.021597377955913544, 0.020566385239362717, 0.9344215393066406]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 496\n",
            "  Loss for client 0: 2.0125\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 496\n",
            "  Loss for client 1: 1.8621\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 496\n",
            "  Loss for client 2: 2.0378\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 496\n",
            "  Loss for client 3: 0.0014\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 497\n",
            "  Client 0: total grad norm = 93.710751 (253 params)\n",
            "  Client 1: total grad norm = 75.612290 (253 params)\n",
            "  Client 2: total grad norm = 79.517360 (253 params)\n",
            "  Client 3: total grad norm = 0.581119 (253 params)\n",
            "\n",
            "=== Optimizer Step 497 ===\n",
            "\n",
            "=== Updating Client Weights (Step 497) ===\n",
            "Gradient norms: [9.041041374206543, 9.677091598510742, 8.813530921936035, 0.06307940185070038]\n",
            "Target weights: [0.006835829932242632, 0.006386528257280588, 0.007012288551777601, 0.9797653555870056]\n",
            "Updated weights: [0.018441054970026016, 0.017034122720360756, 0.016500156372785568, 0.9480246305465698]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 497\n",
            "  Loss for client 0: 1.6527\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 497\n",
            "  Loss for client 1: 2.1807\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 497\n",
            "  Loss for client 2: 2.3954\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 497\n",
            "  Loss for client 3: 0.1625\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 498\n",
            "  Client 0: total grad norm = 74.334305 (253 params)\n",
            "  Client 1: total grad norm = 76.845631 (253 params)\n",
            "  Client 2: total grad norm = 93.576758 (253 params)\n",
            "  Client 3: total grad norm = 6.576651 (253 params)\n",
            "\n",
            "=== Optimizer Step 498 ===\n",
            "\n",
            "=== Updating Client Weights (Step 498) ===\n",
            "Gradient norms: [8.790874481201172, 9.070332527160645, 9.641237258911133, 2.219911813735962]\n",
            "Target weights: [0.1461774706840515, 0.1416737288236618, 0.1332845240831375, 0.578864336013794]\n",
            "Updated weights: [0.056761980056762695, 0.054426006972789764, 0.0515354685485363, 0.8372765779495239]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 498\n",
            "  Loss for client 0: 1.4581\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 498\n",
            "  Loss for client 1: 1.8510\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 498\n",
            "  Loss for client 2: 2.1497\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 498\n",
            "  Loss for client 3: 0.0008\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 499\n",
            "  Client 0: total grad norm = 82.788987 (253 params)\n",
            "  Client 1: total grad norm = 85.459700 (253 params)\n",
            "  Client 2: total grad norm = 87.561640 (253 params)\n",
            "  Client 3: total grad norm = 0.328900 (253 params)\n",
            "\n",
            "=== Optimizer Step 499 ===\n",
            "\n",
            "=== Updating Client Weights (Step 499) ===\n",
            "Gradient norms: [9.383585929870605, 9.764389038085938, 9.697122573852539, 0.03765673190355301]\n",
            "Target weights: [0.003966426942497492, 0.003811739617958665, 0.0038381805643439293, 0.9883836507797241]\n",
            "Updated weights: [0.040923312306404114, 0.0392417274415493, 0.03722628206014633, 0.8826087117195129]\n",
            "==================================================\n",
            "\n",
            "[Processing] Client 0, batch 499\n",
            "  Loss for client 0: 1.4072\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 1, batch 499\n",
            "  Loss for client 1: 1.9166\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 2, batch 499\n",
            "  Loss for client 2: 2.0204\n",
            "  Collected 253 gradients\n",
            "\n",
            "[Processing] Client 3, batch 499\n",
            "  Loss for client 3: 0.0006\n",
            "  Collected 253 gradients\n",
            "\n",
            "[register_all_gradients] Step 500\n",
            "  Client 0: total grad norm = 76.601311 (253 params)\n",
            "  Client 1: total grad norm = 85.643824 (253 params)\n",
            "  Client 2: total grad norm = 88.960706 (253 params)\n",
            "  Client 3: total grad norm = 0.202780 (253 params)\n",
            "\n",
            "=== Optimizer Step 500 ===\n",
            "\n",
            "=== Updating Client Weights (Step 500) ===\n",
            "Gradient norms: [8.441145896911621, 8.428053855895996, 9.125965118408203, 0.023849064484238625]\n",
            "Target weights: [0.002802166622132063, 0.002806519391015172, 0.0025918900500983, 0.9917994737625122]\n",
            "Updated weights: [0.029486969113349915, 0.028311165049672127, 0.026835963129997253, 0.9153659343719482]\n",
            "==================================================\n",
            "\n",
            "[Validation] Step 500\n",
            "  Val Loss = 1.0182 (4 batches)\n",
            "–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ /content/drive/MyDrive/meritopt_checkpoints/step_500\n",
            "\n",
            "Reached max steps (500), stopping...\n",
            "\n",
            "Epoch 1 completed:\n",
            "   Train Loss = 0.2241\n",
            "   Val Loss = 1.0182 (4 batches)\n",
            "–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ /content/drive/MyDrive/meritopt_checkpoints/epoch_1\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1600x1000 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "try:\n",
        "    model, tokenizer, logger = main_simple()\n",
        "except Exception as e:\n",
        "    print(f\"–û—à–∏–±–∫–∞: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lH3vzRfdwOH"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdPo7iGOpcHQ"
      },
      "source": [
        "–ö–ª–∏–µ–Ω—Ç—ã:\n",
        "* `ENF` -- **—ç–Ω–µ—Ü–∫–∏–µ** -- 51777 –ø—Ä–∏–º–µ—Ä–æ–≤ -- *—Å–∏–Ω–∏–º* —Ü–≤–µ—Ç–æ–º\n",
        "* `NGA` -- **–Ω–≥–∞–Ω–∞—Å–∞–Ω—Å–∫–∏–π** -- 34609 –ø—Ä–∏–º–µ—Ä–æ–≤ -- *—Ä—ã–∂–∏–º* —Ü–≤–µ—Ç–æ–º\n",
        "* `NEN` -- **–Ω–µ–Ω–µ—Ü–∫–∏–µ** -- 10222 –ø—Ä–∏–º–µ—Ä–æ–≤ -- *–∑–µ–ª–µ–Ω—ã–º* —Ü–≤–µ—Ç–æ–º"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "069e12b9d36d463aaa34e01038a21290": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a79fb54490145d2ab2a75181cce4237",
              "IPY_MODEL_8d79cdf07e3c4c84b2de1ca2e0e24b8e",
              "IPY_MODEL_985ff672866d4b99aba8dc964d4e3b9d"
            ],
            "layout": "IPY_MODEL_4b124bfcbee44b52981b35122991ed3f"
          }
        },
        "09f27b49ad4b42ac8157d94e6a7a2e3a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d666bcaad9346f0a16d9ae26e7af600": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b8a7b22ac7d461185ee0647485f4ff2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2582bd71c593480d8a1f11e9b63783cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f5e01e8f2d44a2cb1d70643bfd82dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d5cbb80ba7e042c68f90d515da234918",
              "IPY_MODEL_6f257b5666de4a109bfffe65ac5bd25f",
              "IPY_MODEL_896d22d83f17470d8bf9ef4f69284c3b"
            ],
            "layout": "IPY_MODEL_e365c08cfa714bd4837669a7db4fe476"
          }
        },
        "35b94d9c4ecf4c92b8f3be219b40d65d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "360f59a848314bf1bcc052fa7ee862fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_817320e627fa41f9bcada3f2142890d7",
            "max": 3578,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35b94d9c4ecf4c92b8f3be219b40d65d",
            "value": 499
          }
        },
        "385af288fc034ef2ade84cabf8ad7ad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cbadf8369964980b269abb48d3297d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3deb48617cc64a05b9199f67bbdfeb54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b124bfcbee44b52981b35122991ed3f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a5402a85088406d88314602527aa2dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f257b5666de4a109bfffe65ac5bd25f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e88f702f612643c8b295acc6640e56c6",
            "max": 3578,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca8b50cf17754d4694bfe4bc7ce02bf1",
            "value": 499
          }
        },
        "817320e627fa41f9bcada3f2142890d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85620ffa02304c79b40af9600895dd3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "896d22d83f17470d8bf9ef4f69284c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3deb48617cc64a05b9199f67bbdfeb54",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b575564fbe3348c19b695fcc9b62df01",
            "value": "‚Äá499/3578‚Äá[02:59&lt;14:14,‚Äá‚Äá3.60it/s,‚Äáloss=1.74]"
          }
        },
        "8c14d1a9e1af4d30b8b652f294cbc3f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d79cdf07e3c4c84b2de1ca2e0e24b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cbadf8369964980b269abb48d3297d6",
            "max": 3500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c14d1a9e1af4d30b8b652f294cbc3f5",
            "value": 499
          }
        },
        "985ff672866d4b99aba8dc964d4e3b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a5402a85088406d88314602527aa2dd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e98343bb6a4d400e9ea0c7c2626882ed",
            "value": "‚Äá499/3500‚Äá[05:03&lt;16:40,‚Äá‚Äá3.00it/s,‚Äáloss=1.34]"
          }
        },
        "9a79fb54490145d2ab2a75181cce4237": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f36e2cb34f87421e9deaba7e1aeec022",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_85620ffa02304c79b40af9600895dd3e",
            "value": "FL‚ÄáEpoch‚Äá1:‚Äá‚Äá14%"
          }
        },
        "a62ec6941fa149f5a7283ec4ae03fa98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "add5848224864485a1539e425812b183": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b575564fbe3348c19b695fcc9b62df01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca8b50cf17754d4694bfe4bc7ce02bf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d14c346160914ddf91e072551748a4af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09f27b49ad4b42ac8157d94e6a7a2e3a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a62ec6941fa149f5a7283ec4ae03fa98",
            "value": "FL‚ÄáEpoch‚Äá1:‚Äá‚Äá14%"
          }
        },
        "d5cbb80ba7e042c68f90d515da234918": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2582bd71c593480d8a1f11e9b63783cf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0d666bcaad9346f0a16d9ae26e7af600",
            "value": "FL‚ÄáEpoch‚Äá1:‚Äá‚Äá14%"
          }
        },
        "e365c08cfa714bd4837669a7db4fe476": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e88f702f612643c8b295acc6640e56c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e98343bb6a4d400e9ea0c7c2626882ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb366f9f242e437db16a83dd785b8247": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b8a7b22ac7d461185ee0647485f4ff2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_385af288fc034ef2ade84cabf8ad7ad4",
            "value": "‚Äá499/3578‚Äá[03:02&lt;14:00,‚Äá‚Äá3.66it/s,‚Äáloss=1.34]"
          }
        },
        "f36e2cb34f87421e9deaba7e1aeec022": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa551c132feb44febc20ea803b8c9fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d14c346160914ddf91e072551748a4af",
              "IPY_MODEL_360f59a848314bf1bcc052fa7ee862fb",
              "IPY_MODEL_eb366f9f242e437db16a83dd785b8247"
            ],
            "layout": "IPY_MODEL_add5848224864485a1539e425812b183"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
