# Федеративное обучение: машинный перевод самодийских языков
## Датасет
Основан на корпусах INEL (Corpora of indigenous Northern Eurasian languages (Universität Hamburg)). 
* Ненецкие языки[^1] (один корпус, лесной и тундровой языки ~ диалекты). 61,278 токенов.
* Нганасанский язык[^2]. 221,747 токенов.
* Энецкие языки[^3] (один корпус, лесной и тундренный языки ~ диалекты). 218,710 токенов.

Ручной парсинг xlm-файлов, предобработка.

Объем датасета: 97,434 строк. Датасет доступен на [HuggingFace](https://huggingface.co/datasets/katykool/samoyed.ic).

<img width="1125" height="193" alt="image" src="https://github.com/user-attachments/assets/25ec2957-8e51-46b3-b950-ed02f67cdd86" />

## Вдохновение: предыдущие исследования
Мой эксперимент вдохновлён работой "Low-Resource Machine Translation through the Lens of Personalized Federated Learning"[^4] [[Moskvoretskii et al., 2024]](https://arxiv.org/abs/2406.12564). В работе авторы применяют алгоритм федеративного обучения для работы с моделями машинного перевода. 
### Что такое федеративное обучение? 
Федеративное обучение — это метод машинного обучения, который позволяет обучать модель на децентрализованных данных, хранящихся на разных устройствах. Это удобно для, например, конфиденциальных данных / данных, распределенных по разным серверам, тяжелые к передаче и т.п. Но в случае NLP -- и особенно в случае моего эксперимента -- это интересный экспериментальный сетап. Данные разных устройств (дальше я буду называть их клиентами (clients)) соотносятся с данными разных языков, которые используются при обучении мультилингвальной модели. 

Процесс обучения:
* Сервер (= глобальная модель) отправляет текущую модель клиентам (= локальным моделям, разные языки)
* Каждый клиент обучает модель на своих данных
* Клиенты отправляют градиенты обратно на сервер
* Сервер агрегирует градиенты и обновляет общую модель
* 
```
Единая модель M (параметры θ)
    ↓
Клиент 1 (ENF): M(θ) → градиенты g₁
Клиент 2 (NGA): M(θ) → градиенты g₂  
Клиент 3 (NEN): M(θ) → градиенты g₃
    ↓
Сервер: агрегирует g₁, g₂, g₃ → обновляет θ
    ↓
Новая единая модель M(θ')
```

### В чём суть эксперимента? 
Мы разделили языки по клиентам. Теперь мы можем отслеживать, как модель "поглощает" каждый из языков. Авторы статьи [Moskvoretskii et al., 2024] предлагают алгоритм MeritFed -- динамическое изменение весов разных языков. MeritFed позволяет наглядно отслеживать «вклад» каждого вспомогательного языка в обучение общей модели через динамически обновляемые веса (cf. концепция персонализированного федеративного обучения (PFL), где цель — не просто обучить единую глобальную модель, а эффективно агрегировать знания из разнородных источников данных.

В частности, авторы используют датасеты для саамских языков (< уральские; север Скандинавского и Кольского п-ова). Гипотеза, что при обучении модели для языка Х, больше всего будут "помогать" близкородственные ему языки, подтвердилась не полностью: скорее роль играла ресурсность. Большие и качественные датасеты получают большее влияние. В другом эксперименте в той же работе (уже на языках Индонезии) авторы замечают, что "вливание" неродственных языков (венгерского) приносит чуть ли не более значительный вклад, чем родственных. 

Из работы [Moskvoretskii et al., 2024] я беру идею о федеративности -- разделении датасетов по языкам по разным клиентам, -- и оптимизатор, похожий на MeritOpt, но сильно более упрощенный. 

Меня интересует механизм адаптивного обновления весов, когда клиентами выступают не вспомогательные языки для одного целевого, а несколько самостоятельных языков. 

## Сетап 
### Модель
Seq2Seq модель MarianMT (Helsinki-NLP/opus-mt-en-ru).
### Оптимизатор
Кастомный оптимизатор MeritFedAdam, модифицирующий стандартный Adam и отличающийся от версии в статье. 
* Для каждого параметра модели хранится отдельный массив градиентов от каждого клиента (`state[p]['grads']`).
* На каждом шаге после получения градиентов от всех клиентов происходит их взвешенное агрегирование: `agg_grad = Σ (weights[i] * grads[i]`).
* Функция `_update_client_weights`: вес клиента *i* вычисляется как обратно пропорциональная величина к суммарной L2-норме его градиентов: `weight[i] ∝ 1 / (||grad_i|| + ε)`. Клиент, предоставляющий более уверенные (менее колебательные) и согласованные обновления, получает больший вес. Затем веса нормализуются (сумма = 1).

## Эксперимент 0: свободный.
Три языка (ненецкий, нганасанский, энецкий), нет ограничения по объему датасетов. 

Есть небольшая связь с ресурсностью: средние веса выше у энецкого, потом идёт нганасанский, потом ненецкий (самый маленткий датасет).
<img width="1320" height="682" alt="image" src="https://github.com/user-attachments/assets/55b8a82a-5c9a-4f08-9e3b-ff3079132483" />

## Эксперимент 1: размер датасета?
Ради сравнения зафиксируем размер датасета. 

Средние веса выравниваются. 
<img width="1320" height="682" alt="image" src="https://github.com/user-attachments/assets/2bbb73b2-f052-4bce-8f87-81eacc5c77aa" />

Важно, что веса сильно колеблются по ходу обучения: это логично, потому что у нас довольно мало данных, и из-за этого мы наблюдаем нестабильность. 

## Эксперимент 2: (не)родственность?
Сначала видим, что португальский не вносит большого вклада. Но чем больше шагов, тем выше его веса. Важно, что это не связано напрямую с размером датасета, то есть непосредственной ресурсностью. На примерно 50м шаге португальский взлетает. 
<img width="1589" height="530" alt="image" src="https://github.com/user-attachments/assets/d313c5b8-5045-44bd-9e39-6f96affba9d3" />

## Некоторые интересные идеи
* Формализация лингвистической интуиции о том, как влияют на обучение родственные языки.
* Родственность влияет на первых шагах, потом больше веса дается более ресурсному (в целом в nlp-пространстве, не по размеру обучающего корпуса) языку.
* Даже при +2к предложений в корпусе разницы в ресурсности нет, поэтому энецкий не получает бОльших весов. Ресурсность -- это такой "внеэкспериментальный" параметр, и скорее всего параметр модели (потому что португальские данные, наверное, есть в нашей модели изначально).

[^1]: _Budzisch, Josefina; Wagner-Nagy, Beáta. 2024. INEL Nenets Corpus. Version 1.0. Publication date 2024-12-31. https://hdl.handle.net/11022/0000-0007-FE37-E._
[^2]: _Brykina, Maria; Gusev, Valentin; Szeverényi, Sándor; Wagner-Nagy, Beáta. INEL Nganasan Corpus. Version 1.0. Publication date 2025-05-02. https://hdl.handle.net/11022/0000-0007-FE63-C._
[^3]: _Shluinsky, Andrey; Khanina, Olesya; Wagner-Nagy, Beáta. 2024. INEL Enets Corpus. Version 1.0. Publication date 2024-11-30. https://hdl.handle.net/11022/0000-0007-FE1D-C._ 
[^4]: _Moskvoretskii, Viktor, et al. “Low-Resource Machine Translation through the Lens of Personalized Federated Learning.” Findings of the Association for Computational Linguistics: EMNLP 2024, Association for Computational Linguistics, 2024, pp. 8806–25. Crossref, https://doi.org/10.18653/v1/2024.findings-emnlp.514._
