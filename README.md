# Федеративное обучение: машинный перевод самодийских языков
## Датасет
Основан на корпусах INEL (Corpora of indigenous Northern Eurasian languages (Universität Hamburg)). 
* Ненецкие языки[^1] (один корпус, лесной и тундровой языки ~ диалекты). 61,278 токенов.
* Нганасанский язык[^2]. 221,747 токенов.
* Энецкие языки[^3] (один корпус, лесной и тундренный языки ~ диалекты). 218,710 токенов.

Ручной парсинг xlm-файлов, предобработка.

Объем датасета: 97,434 строк. Датасет доступен на [HuggingFace](https://huggingface.co/datasets/katykool/samoyed.ic).

<img width="1125" height="193" alt="image" src="https://github.com/user-attachments/assets/25ec2957-8e51-46b3-b950-ed02f67cdd86" />

## Вдохновение: предыдущие исследования
Мой эксперимент вдохновлён работой "Low-Resource Machine Translation through the Lens of Personalized Federated Learning"[^4] [[Moskvoretskii et al., 2024]](https://arxiv.org/abs/2406.12564). В работе авторы применяют алгоритм федеративного обучения для работы с моделями машинного перевода. 
### Что такое федеративное обучение? 
Федеративное обучение — это метод машинного обучения, который позволяет обучать модель на децентрализованных данных, хранящихся на разных устройствах. Это удобно для, например, конфиденциальных данных / данных, распределенных по разным серверам, тяжелые к передаче и т.п. Но в случае NLP -- и особенно в случае моего эксперимента -- это интересный экспериментальный сетап. Данные разных устройств (дальше я буду называть их клиентами (clients)) соотносятся с данными разных языков, которые используются при обучении мультилингвальной модели. 

<img width="906" height="435" alt="image" src="https://github.com/user-attachments/assets/15e0a81d-db07-4aa0-8290-c3d0ec5e2ddc" />

Процесс обучения:
* Сервер (= глобальная модель) отправляет текущую модель клиентам (= локальным моделям, разные языки)
* Каждый клиент обучает модель на своих данных
* Клиенты отправляют градиенты обратно на сервер
* Сервер агрегирует градиенты и обновляет общую модель

```
Единая модель M (параметры θ)
    ↓
Клиент 1 (ENF): M(θ) → градиенты g₁
Клиент 2 (NGA): M(θ) → градиенты g₂  
Клиент 3 (NEN): M(θ) → градиенты g₃
    ↓
Сервер: агрегирует g₁, g₂, g₃ → обновляет θ
    ↓
Новая единая модель M(θ')
```
```python
for client_id in range(config.npeers):
    # обучаем глобальную модель model на данных каждого языка
    outputs = model(input_ids=model_inputs['input_ids'], ...)
    loss = outputs.light
    loss.backward()  # считаем градиенты
    
    # сохраняем градиенты этой модели для этого клиента
    client_gradients = []
    for param in model.parameters():
        client_gradients.append(param.grad.clone().detach())
    
    optimizer.save_client_gradients(client_id, client_gradients)

# после обработки всех клиентов агрегируем градиенты от этих трёх обучений и обновляем глобальную модель
optimizer.register_all_gradients() 
optimizer.step()  # обновляем параметры модели
# (о том, как происходит агрегация -- ниже)
```

### В чём суть эксперимента? 
Мы разделили языки по клиентам. Теперь мы можем отслеживать, как модель "поглощает" каждый из языков. Авторы статьи [Moskvoretskii et al., 2024] предлагают алгоритм MeritFed -- динамическое изменение весов разных языков. MeritFed позволяет наглядно отслеживать «вклад» каждого вспомогательного языка в обучение общей модели через динамически обновляемые веса (cf. концепция персонализированного федеративного обучения (PFL), где цель — не просто обучить единую глобальную модель, а эффективно агрегировать знания из разнородных источников данных.

В частности, авторы используют датасеты для саамских языков (< уральские; север Скандинавского и Кольского п-ова). Гипотеза, что при обучении модели для языка Х, больше всего будут "помогать" близкородственные ему языки, подтвердилась не полностью: скорее роль играла ресурсность. Большие и качественные датасеты получают большее влияние. В другом эксперименте в той же работе (уже на языках Индонезии) авторы замечают, что "вливание" неродственных языков (венгерского) приносит чуть ли не более значительный вклад, чем родственных. 

Из работы [Moskvoretskii et al., 2024] я беру идею о федеративности -- разделении датасетов по языкам по разным клиентам, -- и оптимизатор, похожий на MeritOpt, но сильно более упрощенный. 

Меня интересует механизм адаптивного обновления весов, когда клиентами выступают не вспомогательные языки для одного целевого, а **несколько самостоятельных языков**. 

## Сетап 
### Модель
Seq2Seq модель MarianMT (Helsinki-NLP/opus-mt-en-ru).
### Federated learning: подробнее про то, как агрегируются градиенты от разных клиентов
```python
def step(self, closure=None):
    for group in self.param_groups:
        for p in group['params']:
            state = self.state[p]
            
            # агрегация с учетом весов клиентов
            agg_grad = torch.zeros_like(p, device=self.device)
            for i in range(self.npeers):
                agg_grad += state['grads'][i] * self.weights[i]  # веса. как вычисляются -- в след. буллете
            
            # ...обычный adam update с агрегированным градиентом
```

### Оптимизатор: как **динамически изменяются** веса клиентов?
Кастомный оптимизатор MeritFedAdam, модифицирующий стандартный Adam и отличающийся от версии в статье. 
* В самом начале инициализируются одинаковые веса [0.33, 0.33, 0.33].
* Для каждого параметра модели хранится отдельный массив градиентов от каждого клиента (`state[p]['grads']`).
* На каждом шаге после получения градиентов от всех клиентов происходит их взвешенное агрегирование: `agg_grad = Σ (weights[i] * grads[i]`).
* Функция `_update_client_weights`: вес клиента *i* вычисляется как обратно пропорциональная величина к суммарной L2-норме его градиентов: `weight[i] ∝ 1 / (||grad_i|| + ε)`. Клиент, предоставляющий более уверенные (менее колебательные) и согласованные обновления, получает больший вес. Затем веса нормализуются (сумма = 1).
То есть:
* Большая L2-норма градиента, т.е. очень крутой градиент → модель сильно меняется на данных клиента → клиент далекий от оптимума → даем меньший вес
* Малая норма градиента → модель почти не меняется → клиент близкий к оптимуму → даем больший вес

Посмотрим на логи: 
```
Step 1:
Gradient norms: [21.52, 18.08, 27.50]
Target weights: [0.336, 0.400, 0.263]  # NGA (самый маленький градиент) получает самый большой вес

Step 2:  
Gradient norms: [16.63, 16.51, 33.57]
Target weights: [0.400, 0.402, 0.198]  # NEN (самый большой градиент) получает самый маленький вес
```

## Эксперимент 0: свободный.
Три языка (ненецкий, нганасанский, энецкий), нет ограничения по объему датасетов. 

Есть небольшая связь с ресурсностью: средние веса выше у энецкого, потом идёт нганасанский, потом ненецкий (самый маленткий датасет).
<img width="1320" height="682" alt="image" src="https://github.com/user-attachments/assets/55b8a82a-5c9a-4f08-9e3b-ff3079132483" />

## Эксперимент 1: размер датасета?
Ради сравнения зафиксируем размер датасета. 

Средние веса выравниваются. Интересно, что энецкий на первых шагах получает более маленькие веса: как будто при фиксированном размере модель сначала смотрит на более малые языки. 
<img width="1320" height="682" alt="image" src="https://github.com/user-attachments/assets/2bbb73b2-f052-4bce-8f87-81eacc5c77aa" />

Важно, что веса сильно колеблются по ходу обучения: это логично, потому что у нас довольно мало данных, и из-за этого мы наблюдаем нестабильность. 

## Эксперимент 2: (не)родственность?
Сначала видим, что португальский не вносит большого вклада. Но чем больше шагов, тем выше его веса. Важно, что это не связано напрямую с размером датасета, то есть непосредственной ресурсностью. На примерно 50м шаге португальский взлетает. 
<img width="1589" height="530" alt="image" src="https://github.com/user-attachments/assets/d313c5b8-5045-44bd-9e39-6f96affba9d3" />

## Некоторые интересные идеи
* Родственность влияет на первых шагах, потом больше веса дается более ресурсному (в целом в nlp-пространстве, не по размеру обучающего корпуса) языку.
* Даже при +2к предложений в корпусе разницы в ресурсности нет, поэтому энецкий не получает бОльших весов. Ресурсность -- это такой "внеэкспериментальный" параметр, это не = размер корпуса в нашем эксперименте: языки, представленные **в предобучении модели**, имеют преимущество.

[^1]: _Budzisch, Josefina; Wagner-Nagy, Beáta. 2024. INEL Nenets Corpus. Version 1.0. Publication date 2024-12-31. https://hdl.handle.net/11022/0000-0007-FE37-E._
[^2]: _Brykina, Maria; Gusev, Valentin; Szeverényi, Sándor; Wagner-Nagy, Beáta. INEL Nganasan Corpus. Version 1.0. Publication date 2025-05-02. https://hdl.handle.net/11022/0000-0007-FE63-C._
[^3]: _Shluinsky, Andrey; Khanina, Olesya; Wagner-Nagy, Beáta. 2024. INEL Enets Corpus. Version 1.0. Publication date 2024-11-30. https://hdl.handle.net/11022/0000-0007-FE1D-C._ 
[^4]: _Moskvoretskii, Viktor, et al. “Low-Resource Machine Translation through the Lens of Personalized Federated Learning.” Findings of the Association for Computational Linguistics: EMNLP 2024, Association for Computational Linguistics, 2024, pp. 8806–25. Crossref, https://doi.org/10.18653/v1/2024.findings-emnlp.514._
